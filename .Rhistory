names(kor_census) <- c("행정구역", "X2013인구수","X2023인구수","성장률","행정구역_코드")
kor_census$시도 = sapply(kor_census$행정구역,
function(x) strsplit(x, " ")[[1]][1])
kor_census$시도 = factor(kor_census$시도,
levels = c("서울특별시","대전광역시","대구광역시","부산광역시"))
region_colors <- c("#E69F00","#56B4E9","#009E73","#F0E442")
ggplot(kor_census,aes(x = reorder(행정구역,성장률),y= 성장률, fill = 시도))+
geom_col()+
scale_y_continuous(name = "인구성장률",
expand = c(0,0),
labels = scales::percent_format(scale = 100))+
scale_fill_manual(values = region_colors)+
coord_flip()+
theme_light()+
theme(panel.border = element_blank(),
panel.grid.major.y = element_blank())+
theme(axis.title.y = element_blank(),
axis.line.y = element_blank(),
axis.ticks.length = unit(0,"pt"),
axis.text.y = element_text(size = 8),legend.position = c(.78,.28),legend.background = element_rect(fill = "#FFFFFFB0"))
kor_map <- kor_sigu
kor_map$행정구역_코드 <- paste(kor_map$SIG_CD,"00000",sep="")
kor_census_2 <- data_2013 %>%
merge(data_2023,by = "행정구역_코드", all.x=T) %>%
mutate(성장률 = (X2023년03월_총인구수 - X2013년03월_총인구수) / X2013년03월_총인구수) %>%
select(행정구역.x,X2013년03월_총인구수,X2023년03월_총인구수, 성장률, 행정구역_코드) %>%
arrange(desc(성장률))
names(kor_census_2) <- c("행정구역", "X2013인구수","X2023인구수","성장률","행정구역_코드")
kor_map <- kor_map %>% left_join(kor_census_2, by="행정구역_코드")
kor_map %>% ggplot(aes(fill=성장률))+
geom_sf()+
scale_fill_continuous_diverging(
name = "인구성장률",
palette = "BLue-Red",
limits = c(-0.4,2.4))+
theme_minimal()+
theme(legend.title.align = 0.5,
legend.text.align = 1.0,
legend.position = c(0.85,0.2))
data <- read.csv("https://raw.githubusercontent.com/Sungileo/trainsets/main/kor_census_2013_2023.csv",encoding = "utf-8")
data <- data %>% mutate(인구증감 = 총인구수_2023-총인구수_2013)
use_map <- kor_sigu
use_map$행정구역_코드 <- paste(use_map$SIG_CD,"00000",sep = "") %>% as.numeric()
use_map <- use_map %>% merge(data,by = "행정구역_코드")
use_map %>% ggplot(aes(fill = 인구증감))+
geom_sf()+
coord_sf(datum = NA)+
scale_fill_continuous_diverging(
name = "인구증감",
palette = "BLue-Red",
na.value = "grey40",
mid=0,
rev = T,
limits = c(-4,4)*100000,
labels = format(seq(-4,4,2) * 1e+5, big.mark = ",",scientific = FALSE))+
theme_minimal()+
theme(legend.position = c(0.85,0.2))
reticulate::repl_python()
import geopandas as gpd
# Read in the data
full_data = gpd.read_file("C:/archive/DEC_lands/DEC_lands/DEC_lands.shp")
# View the first five rows of the data
full_data.head()
type(full_data)
data = full_data.loc[:,["CLASS","COUNTY","geometry"]].copy()
data["CLASS"].value_counts()
# Select lands that fall under the "WILD FOREST" or "WILDERNESS" category
wild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()
wild_lands.head()
wild_lands.plot()
# Campsites in New York state (Point)
POI_data = gpd.read_file("C:/archive/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp")
campsites = POI_data.loc[POI_data.ASSET=='PRIMITIVE CAMPSITE'].copy()
# Foot trails in New York state (LineString)
roads_trails = gpd.read_file("C:/archive/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp")
trails = roads_trails.loc[roads_trails.ASSET=='FOOT TRAIL'].copy()
# County boundaries in New York state (Polygon)
counties = gpd.read_file("C:/archive/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp")
# Define a base map with county boundaries
ax = counties.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)
# Add wild lands, campsites, and foot trails to the base map
wild_lands.plot(color='lightgreen', ax=ax)
campsites.plot(color='maroon', markersize=2, ax=ax)
trails.plot(color='black', markersize=1, ax=ax)
world_loans = gpd.read_file("C:/archive/kiva_loans/kiva_loans/kiva_loans.shp")
world_loans.head()
View(world_loans)
world_loans.plot()
world_filepath = gpd.datasets.get_path("naturalearth_lowers")
world = gpd.read_file(world_filepath)
world.head()
`
world_filepath = gpd.datasets.get_path("naturalearth_lowres")
world_filepath = gpd.datasets.get_path("naturalearth_lowres")
world = gpd.read_file(world_filepath)
world.head()
ax = world.plot(figsize = (20,20),color = "whitesmoke",linestyle=":",edgecolor = "black")
world_loans.plot(ax=ax, markersize=2)
PHL_loans = world_loans.loc[world_loans["country"]=="Philippines"].copy()
View(PHL_loans)
PHL_loans.head()
gpd.io.file.fiona.drvsupport.supported_drivers["KML"] = "rw"
PHL = gpd.read_file("C:/archive/Philippines_AL258.kml",driver = "KML")
PHL.head()
ax = PHL.plot(figsize = (20,20),color = "whitesmoke",linestyle=":",edgecolor = "lightgray")
PHL_loans.plot(ax=ax, markersize = 2)
reticulate::repl_python()
import folium
from folium import Choropleth, Circle, Marker
from folium.plugins import HeatMap, MarkerCluster
# Create a map
m_1 = folium.Map(location=[42.32,-71.0589], tiles='openstreetmap', zoom_start=10)
# Display the map
m_1
import pandas as pd
crimes = pd.read_csv("C:/archive/crimes-in-boston/crimes-in-boston/crime.csv", encoding='latin-1')
crimes.dropna(subset=['Lat', 'Long', 'DISTRICT'], inplace=True)
crimes = crimes[crimes.OFFENSE_CODE_GROUP.isin([
crimes = crimes[crimes.OFFENSE_CODE_GROUP.isin([
'Larceny', 'Auto Theft', 'Robbery', 'Larceny From Motor Vehicle', 'Residential Burglary',
'Simple Assault', 'Harassment', 'Ballistics', 'Aggravated Assault', 'Other Burglary',
'Arson', 'Commercial Burglary', 'HOME INVASION', 'Homicide', 'Criminal Harassment',
'Manslaughter'])]
crimes = crimes[crimes.YEAR>=2018]
crimes = crimes[crimes.OFFENSE_CODE_GROUP.isin([
'Larceny', 'Auto Theft', 'Robbery', 'Larceny From Motor Vehicle', 'Residential Burglary',
'Simple Assault', 'Harassment', 'Ballistics', 'Aggravated Assault', 'Other Burglary',
'Arson', 'Commercial Burglary', 'HOME INVASION', 'Homicide', 'Criminal Harassment',
'Manslaughter'])]
crimes = crimes[crimes.YEAR>=2018]
crimes = crimes[crimes.YEAR>=2018]
crimes.head()
(crimes.HOUR.isin(range(9,18))))]
daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \
# Create a map
m_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
# Add points to the map
for idx, row in daytime_robberies.iterrows():
Marker([row['Lat'], row['Long']]).add_to(m_2)
# Display the map
m_2
# Create a map
m_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
# Add points to the map
for idx, row in daytime_robberies.iterrows():
Marker([row['Lat'], row['Long']]).add_to(m_2)
# Display the map
m_2
daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \
daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & (crimes.HOUR.isin(range(9,18))))]
daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & (crimes.HOUR.isin(range(9,18))))]
m_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
for idx, row in daytime_robberies.iterrows():
m_2
Marker([row['Lat'], row['Long']]).add_to(m_2)
for idx, row in daytime_robberies.iterrows():
for idx, row in daytime_robberies.iterrows():
Marker([row['Lat'], row['Long']]).add_to(m_2)
for idx, row in daytime_robberies.iterrows():
Marker([row['Lat'], row['Long']]).add_to(m_2)
m_2
# Create a map
m_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
# Add points to the map
for idx, row in daytime_robberies.iterrows():
Marker([row['Lat'], row['Long']]).add_to(m_2)
# Display the map
m_2
# Create the map
m_3 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
# Add points to the map
mc = MarkerCluster()
for idx, row in daytime_robberies.iterrows():
if not math.isnan(row['Long']) and not math.isnan(row['Lat']):
mc.add_child(Marker([row['Lat'], row['Long']]))
m_3.add_child(mc)
# Display the map
m_3
import math
# Create the map
m_3 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
# Add points to the map
mc = MarkerCluster()
for idx, row in daytime_robberies.iterrows():
if not math.isnan(row['Long']) and not math.isnan(row['Lat']):
mc.add_child(Marker([row['Lat'], row['Long']]))
m_3.add_child(mc)
# Display the map
m_3
# Create a base map
m_4 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)
def color_producer(val):
if val <= 12:
return 'forestgreen'
else:
return 'darkred'
# Add a bubble map to the base map
for i in range(0,len(daytime_robberies)):
Circle(
location=[daytime_robberies.iloc[i]['Lat'], daytime_robberies.iloc[i]['Long']],
radius=20,
color=color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)
# Display the map
m_4
# Create a base map
m_5 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)
# Add a heatmap to the base map
HeatMap(data=crimes[['Lat', 'Long']], radius=10).add_to(m_5)
# Display the map
m_5
import geopandas as gpd
districts_full = gpd.read_file("C:/archive/Police_Districts/Police_Districts/Police_Districts.shp")
districts = districts_full[["DISTRICT", "geometry"]].set_index("DISTRICT")
districts.head()
# Number of crimes in each police district
plot_dict = crimes.DISTRICT.value_counts()
plot_dict.head()
# Create a base map
m_6 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)
# Add a choropleth map to the base map
Choropleth(geo_data=districts.__geo_interface__,
data=plot_dict,
key_on="feature.id",
fill_color='YlGnBu',
legend_name='Major criminal incidents (Jan-Aug 2018)'
).add_to(m_6)
# Display the map
m_6
plate_boundaries = gpd.read_file("C:/archive/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp")
plate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')
plate_boundaries.drop('geometry', axis=1, inplace=True)
plate_boundaries.head()
earthquakes = pd.read_csv("C:/archive/earthquakes1970-2014.csv", parse_dates=["DateTime"])
earthquakes.head()
m_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)
for i in range(len(plate_boundaries)):
folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)
HeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=15).add_to(m_1)
m_1
m_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)
for i in range(len(plate_boundaries)):
folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)
# Custom function to assign a color to each circle
def color_producer(val):
if val < 50:
return 'forestgreen'
elif val < 100:
return 'darkorange'
else:
return 'darkred'
# Add a map to visualize earthquake depth
for i in range(0,len(earthquakes)):
folium.Circle(
location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],
radius=2000,
color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)
m_2
# GeoDataFrame with prefecture boundaries
prefectures = gpd.read_file("C:/archive/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp")
prefectures.set_index('prefecture', inplace=True)
prefectures.head()
# DataFrame containing population of each prefecture
population = pd.read_csv("C:/archive/japan-prefecture-population.csv")
population.set_index('prefecture', inplace=True)
# Calculate area (in square kilometers) of each prefecture
area_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')
stats = population.join(area_sqkm)
# Add density (per square kilometer) of each prefecture
stats['density'] = stats["population"] / stats["area_sqkm"]
stats.head()
m_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)
# Create a choropleth map to visualize population density
Choropleth(geo_data=prefectures['geometry'].__geo_interface__,
data=stats['density'],
key_on="feature.id",
fill_color='YlGnBu',
legend_name='Population density (per square kilometer)'
).add_to(m_3)
m_3
# Create a base map
m_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)
# Create a map
def color_producer(magnitude):
if magnitude > 6.5:
return 'red'
else:
return 'green'
Choropleth(
geo_data=prefectures['geometry'].__geo_interface__,
data=stats['density'],
key_on="feature.id",
fill_color='BuPu',
legend_name='Population density (per square kilometer)').add_to(m_4)
for i in range(0,len(earthquakes)):
folium.Circle(
location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],
popup=("{} ({})").format(
earthquakes.iloc[i]['Magnitude'],
earthquakes.iloc[i]['DateTime'].year),
radius=earthquakes.iloc[i]['Magnitude']**5.5,
color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)
m_4
reticulate::repl_python()
from geopy.geocoders import Nominatim
import geopy
from geopy.geocoders import Nominatim
import geopy.geocoders
import pandas as pd
import geopandas as gpd
import numpy as np
import folium
from folium import Marker
import warnings
warnings.filterwarnings('ignore')
from geopy.geocoders import Nominatim
!pip install geopy
from geopy.geocoders import Nominatim
geolocator = Nominatim(user_agent="kaggle_learn")
location = geolocator.geocode("Pyramid of Khufu")
print(location.point)
print(location.address)
point = location.point
print("Latitude:", point.latitude)
print("Longitude:", point.longitude)
universities = pd.read_csv("C:/archive/top_universities.csv")
universities.head()
def my_geocoder(row):
try:
point = geolocator.geocode(row).point
return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})
except:
return None
universities[['Latitude', 'Longitude']] = universities.apply(lambda x: my_geocoder(x['Name']), axis=1)
print("{}% of addresses were geocoded!".format(
(1 - sum(np.isnan(universities["Latitude"])) / len(universities)) * 100))
# Drop universities that were not successfully geocoded
universities = universities.loc[~np.isnan(universities["Latitude"])]
universities = gpd.GeoDataFrame(
universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude))
universities.crs = {'init': 'epsg:4326'}
universities.head()
# Create a map
m = folium.Map(location=[54, 15], tiles='openstreetmap', zoom_start=2)
# Add points to the map
for idx, row in universities.iterrows():
Marker([row['Latitude'], row['Longitude']], popup=row['Name']).add_to(m)
# Display the map
m
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
europe = world.loc[world.continent == 'Europe'].reset_index(drop=True)
europe_stats = europe[["name", "pop_est", "gdp_md_est"]]
europe_boundaries = europe[["name", "geometry"]]
europe_boundaries.head()
europe_stats.head()
# Use an attribute join to merge data about countries in Europe
europe = europe_boundaries.merge(europe_stats, on="name")
europe.head()
# Use spatial join to match universities to countries in Europe
european_universities = gpd.sjoin(universities, europe)
# Investigate the result
print("We located {} universities.".format(len(universities)))
print("Only {} of the universities were located in Europe (in {} different countries).".format(
len(european_universities), len(european_universities.name.unique())))
european_universities.head()
european_universities = gpd.sjoin(universities, europe)
!pip install rtree
!pip install pygeos
european_universities = gpd.sjoin(universities, europe)
import pygeos
european_universities = gpd.sjoin(universities, europe)
import rtree
european_universities = gpd.sjoin(universities, europe)
import pygeos
import rtree
european_universities = gpd.sjoin(universities, europe)
european_universities = gpd.sjoin(universities, europe)
import geopandas as gpd
european_universities = gpd.sjoin(universities, europe)
european_universities = gpd.sjoin(universities, europe)
import pygeos
import rtree
european_universities = gpd.sjoin(universities, europe)
# Use spatial join to match universities to countries in Europe
import pygeos
import rtree
import geopandas as gpd
european_universities = gpd.sjoin(universities, europe)
# Investigate the result
print("We located {} universities.".format(len(universities)))
print("Only {} of the universities were located in Europe (in {} different countries).".format(
len(european_universities), len(european_universities.name.unique())))
european_universities.head()
european_universities = gpd.sjoin(universities, europe)
import rtree
import pygeos
import rtree
import geopandas as gpd
european_universities = gpd.sjoin(universities, europe)
!pip install geopandas
european_universities = gpd.sjoin(universities, europe)
import geopandas as gpd
european_universities = gpd.sjoin(universities, europe)
# Use spatial join to match universities to countries in Europe
#european_universities = gpd.sjoin(universities, europe)
# Investigate the result
#print("We located {} universities.".format(len(universities)))
#print("Only {} of the universities were located in Europe (in {} different countries).".format(
#len(european_universities), len(european_universities.name.unique())))
#european_universities.head()
starbucks = pd.read_csv("C:\archive\starbucks_locations.csv")
starbucks.head()
starbucks = pd.read_csv("C:/archive/starbucks_locations.csv")
starbucks.head()
# How many rows in each column have missing values?
print(starbucks.isnull().sum())
# View rows with missing locations
rows_with_missing = starbucks[starbucks["City"]=="Berkeley"]
rows_with_missing
geolocator = Nominatim(user_agent="kaggle_learn")
def my_geocoder(row):
point = geolocator.geocode(row).point
return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})
berkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)
starbucks.update(berkeley_locations)
m_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)
# Add a marker for each Berkeley location
for idx, row in starbucks[starbucks["City"]=='Berkeley'].iterrows():
Marker([row['Latitude'], row['Longitude']]).add_to(m_2)
CA_counties = gpd.read_file("C:/archive/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp")
CA_counties.crs = {'init': 'epsg:4326'}
CA_counties.head()
CA_pop = pd.read_csv("C:/archive/CA_county_population.csv", index_col="GEOID")
CA_high_earners = pd.read_csv("C:/archive/CA_county_high_earners.csv", index_col="GEOID")
CA_median_age = pd.read_csv("C:/archive/CA_county_median_age.csv", index_col="GEOID")
cols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()
CA_stats = CA_counties.merge(cols_to_add, on="GEOID")
CA_stats["density"] = CA_stats["population"] / CA_stats["area_sqkm"]
sel_counties = CA_stats[((CA_stats.high_earners > 100000) &
(CA_stats.median_age < 38.5) &
(CA_stats.density > 285) &
((CA_stats.median_age < 35.5) |
(CA_stats.density > 1400) |
(CA_stats.high_earners > 500000)))]
starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))
starbucks_gdf.crs = {'init': 'epsg:4326'}
starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))
starbucks_gdf.crs = {'init': 'epsg:4326'}
locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)
num_stores = len(locations_of_interest)
starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))
starbucks_gdf.crs = {'init': 'epsg:4326'}
#locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)
#num_stores = len(locations_of_interest)
m_6 = folium.Map(location=[37,-120], zoom_start=6)
mc = MarkerCluster()
locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)
for idx, row in locations_of_interest.iterrows():
if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):
mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))
m_6.add_child(mc)
#m_6.add_child(mc)
#m_6 = folium.Map(location=[37,-120], zoom_start=6)
#mc = MarkerCluster()
#locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)
#for idx, row in locations_of_interest.iterrows():
#    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):
#        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))
#m_6.add_child(mc)
quit
source("~/.active-rstudio-document", echo=TRUE)
reticulate::repl_python()
import folium
from folium import Marker, GeoJson
from folium.plugins import HeatMap
import pandas as pd
import geopandas as gpd
releases = gpd.read_file("C:/archive/toxic_release_pennsylvania/toxic_release_pennsylvania/toxic_release_pennsylvania.shp")
releases.head()
stations = gpd.read_file("C:/archive/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations.shp")
stations.head()
print(stations.crs)
print(releases.crs)
# Select one release incident in particular
recent_release = releases.iloc[360]
# Measure distance from release to each station
distances = stations.geometry.distance(recent_release.geometry)
distances
print('Mean distance to monitoring stations: {} feet'.format(distances.mean()))
print('Closest monitoring station ({} feet):'.format(distances.min()))
print(stations.iloc[distances.idxmin()][["ADDRESS", "LATITUDE", "LONGITUDE"]])
two_mile_buffer = stations.geometry.buffer(2*5280)
two_mile_buffer.head()
# Create map with release incidents and monitoring stations
m = folium.Map(location=[39.9526,-75.1652], zoom_start=11)
HeatMap(data=releases[['LATITUDE', 'LONGITUDE']], radius=15).add_to(m)
for idx, row in stations.iterrows():
Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)
# Plot each polygon on the map
GeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m)
# Show the map
m
# Turn group of polygons into single multipolygon
my_union = two_mile_buffer.geometry.unary_union
print('Type:', type(my_union))
# Show the MultiPolygon object
my_union
# The closest station is less than two miles away
my_union.contains(releases.iloc[360].geometry)
# The closest station is more than two miles away
my_union.contains(releases.iloc[358].geometry)
collisions = gpd.read_file("C:/archive/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp")
collisions.head()
