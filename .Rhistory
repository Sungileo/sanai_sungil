library(tidyverse)
rm(list = ls())
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
sigmoid <- function(x){
1/(1+exp(-x))
}
sigmoid_derivative <- function(x){
sigmoid(x)*(1-sigmoid(x))
}
mse_loss <- function(y_true,y_pred){
mean((y_true - y_pred)**2)
}
x <-seq(-10,10,0.001)
ggplot() +
geom_line(data = data.frame(x, sigmoid(x)),
aes(x, sigmoid(x), color = "Sigmoid"),
linewidth = 1.5)+
geom_line(data = data.frame(x, sigmoid_derivative(x)),
aes(x, sigmoid_derivative(x), color = "Sigmoid Derivative"),
linetype = 5,linewidth = 1.5)+
scale_color_manual(values = c("Sigmoid" = "#fdb863", "Sigmoid Derivative" = "#0571b0")) +
scale_y_continuous(limits = c(0, 1),
name = NULL)+
theme_light()+
theme(legend.position = c(.25,.7),
legend.background = element_rect(fill="#FFFFFFb0"),
panel.border = element_blank())+
labs(colour=NULL)
input_size <- 2
output_size <- 1
W1 <- matrix(rnorm(input_size * output_size), nrow = input_size, ncol = output_size)
b1 <- matrix(0, nrow = 4, ncol = output_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
# 손실 계산
loss <- mse_loss(y, a1)
# Backward Pass
da1 <- a1 - y
dz1 <- da1 * sigmoid_derivative(a1)  # Sigmoid 함수의 미분
dW1 <- t(X) %*% dz1
db1 <- sum(dW1)
# 가중치 업데이트
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a1)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line(aes(color = "red")) +
labs(x = "epoch", y = "loss") +
theme_minimal()
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
hidden_2_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
W3 <- matrix(rnorm(hidden_2_size * output_size), nrow = hidden_2_size, ncol = output_size)
b3 <- matrix(0, nrow = 4, ncol = output_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
z2 <- a1 %*% W2 + b2
a2 <- sigmoid(z2)
z3 <- a2 %*% W3 + b3
a3 <- sigmoid(z3)
# 손실 계산
loss <- mse_loss(y, a3)
# Backward Pass
da3 <- a3 - y
dz3 <- da3 * sigmoid_derivative(a3)
dW3 <- t(a2) %*% dz3
db3 <- sum(dz3)
da2 <- a2 - cbind(y,y,y)
dz2 <- da2 * sigmoid_derivative(a2)
dW2 <- t(a1) %*% dz2
db2 <- colSums(dz2)
da1 <- dz2 - cbind(y,y,y)
dz1 <- da1 * sigmoid_derivative(a1)
dW1 <- t(X) %*% dz1
db1 <- colSums(dz1)
# 가중치 업데이트
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a3)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line(aes(color = "red")) +
labs(x = "epoch", y = "loss") +
theme_minimal()+
theme(legend.position = NULL)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line(aes(color = "red")) +
labs(x = "epoch", y = "loss") +
theme_minimal()+
theme(legend.position = NULL)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line(aes(color = "red")) +
labs(x = "epoch", y = "loss") +
theme_minimal()
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
library(tidyverse)
rm(list = ls())
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
sigmoid <- function(x){
1/(1+exp(-x))
}
sigmoid_derivative <- function(x){
sigmoid(x)*(1-sigmoid(x))
}
mse_loss <- function(y_true,y_pred){
mean((y_true - y_pred)**2)
}
x <-seq(-10,10,0.001)
ggplot() +
geom_line(data = data.frame(x, sigmoid(x)),
aes(x, sigmoid(x), color = "Sigmoid"),
linewidth = 1.5)+
geom_line(data = data.frame(x, sigmoid_derivative(x)),
aes(x, sigmoid_derivative(x), color = "Sigmoid Derivative"),
linetype = 5,linewidth = 1.5)+
scale_color_manual(values = c("Sigmoid" = "#fdb863", "Sigmoid Derivative" = "#0571b0")) +
scale_y_continuous(limits = c(0, 1),
name = NULL)+
theme_light()+
theme(legend.position = c(.25,.7),
legend.background = element_rect(fill="#FFFFFFb0"),
panel.border = element_blank())+
labs(colour=NULL)
input_size <- 2
output_size <- 1
W1 <- matrix(rnorm(input_size * output_size), nrow = input_size, ncol = output_size)
b1 <- matrix(0, nrow = 4, ncol = output_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
# 손실 계산
loss <- mse_loss(y, a1)
# Backward Pass
da1 <- a1 - y
dz1 <- da1 * sigmoid_derivative(a1)  # Sigmoid 함수의 미분
dW1 <- t(X) %*% dz1
db1 <- sum(dW1)
# 가중치 업데이트
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a1)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
hidden_2_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
W3 <- matrix(rnorm(hidden_2_size * output_size), nrow = hidden_2_size, ncol = output_size)
b3 <- matrix(0, nrow = 4, ncol = output_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
z2 <- a1 %*% W2 + b2
a2 <- sigmoid(z2)
z3 <- a2 %*% W3 + b3
a3 <- sigmoid(z3)
# 손실 계산
loss <- mse_loss(y, a3)
# Backward Pass
da3 <- a3 - y
dz3 <- da3 * sigmoid_derivative(a3)
dW3 <- t(a2) %*% dz3
db3 <- sum(dz3)
da2 <- a2 - cbind(y,y,y)
dz2 <- da2 * sigmoid_derivative(a2)
dW2 <- t(a1) %*% dz2
db2 <- colSums(dz2)
da1 <- dz2 - cbind(y,y,y)
dz1 <- da1 * sigmoid_derivative(a1)
dW1 <- t(X) %*% dz1
db1 <- colSums(dz1)
# 가중치 업데이트
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a3)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
output_size = 1
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
z2 <- a1 %*% W2 + b2
a2 <- sigmoid(z2)
# 손실 계산
loss <- mse_loss(y, a3)
da2 <- a2 - cbind(y,y,y)
dz2 <- da2 * sigmoid_derivative(a2)
dW2 <- t(a1) %*% dz2
db2 <- colSums(dz2)
da1 <- dz2 - cbind(y,y,y)
dz1 <- da1 * sigmoid_derivative(a1)
dW1 <- t(X) %*% dz1
db1 <- colSums(dz1)
# 가중치 업데이트
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a2)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
library(tidyverse)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
hidden_2_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
W3 <- matrix(rnorm(hidden_2_size * output_size), nrow = hidden_2_size, ncol = output_size)
b3 <- matrix(0, nrow = 4, ncol = output_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
z2 <- a1 %*% W2 + b2
a2 <- sigmoid(z2)
z3 <- a2 %*% W3 + b3
a3 <- sigmoid(z3)
# 손실 계산
loss <- mse_loss(y, a3)
# Backward Pass
da3 <- a3 - y
dz3 <- da3 * sigmoid_derivative(a3)
dW3 <- t(a2) %*% dz3
db3 <- sum(dz3)
da2 <- a2 - cbind(y,y,y)
dz2 <- da2 * sigmoid_derivative(a2)
dW2 <- t(a1) %*% dz2
db2 <- colSums(dz2)
da1 <- dz2 - cbind(y,y,y)
dz1 <- da1 * sigmoid_derivative(a1)
dW1 <- t(X) %*% dz1
db1 <- colSums(dz1)
# 가중치 업데이트
W3 <- W3 - learning_rate * dW3
b3 <- b3 - learning_rate * db3
W2 <- W2 - learning_rate * dW2
b2 <- b2 - learning_rate * db2
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a3)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
hidden_2_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
W3 <- matrix(rnorm(hidden_2_size * output_size), nrow = hidden_2_size, ncol = output_size)
b3 <- matrix(0, nrow = 4, ncol = output_size)
learning_rate = 0.1
epochs = 2000
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
z2 <- a1 %*% W2 + b2
a2 <- sigmoid(z2)
z3 <- a2 %*% W3 + b3
a3 <- sigmoid(z3)
# 손실 계산
loss <- mse_loss(y, a3)
# Backward Pass
da3 <- a3 - y
dz3 <- da3 * sigmoid_derivative(a3)
dW3 <- t(a2) %*% dz3
db3 <- sum(dz3)
da2 <- a2 - cbind(y,y,y)
dz2 <- da2 * sigmoid_derivative(a2)
dW2 <- t(a1) %*% dz2
db2 <- colSums(dz2)
da1 <- dz2 - cbind(y,y,y)
dz1 <- da1 * sigmoid_derivative(a1)
dW1 <- t(X) %*% dz1
db1 <- colSums(dz1)
# 가중치 업데이트
W3 <- W3 - learning_rate * dW3
b3 <- b3 - learning_rate * db3
W2 <- W2 - learning_rate * dW2
b2 <- b2 - learning_rate * db2
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a3)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
set.seed(2023)
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
# 학습 시작
for (epoch in 1:epochs) {
# Forward Pass
z1 <- X %*% W1 + b1
a1 <- sigmoid(z1)
z2 <- a1 %*% W2 + b2
a2 <- sigmoid(z2)
# 손실 계산
loss <- mse_loss(y, a3)
da2 <- a2 - cbind(y,y,y)
dz2 <- da2 * sigmoid_derivative(a2)
dW2 <- t(a1) %*% dz2
db2 <- colSums(dz2)
da1 <- dz2 - cbind(y,y,y)
dz1 <- da1 * sigmoid_derivative(a1)
dW1 <- t(X) %*% dz1
db1 <- colSums(dz1)
# 가중치 업데이트
W2 <- W2 - learning_rate * dW2
b2 <- b2 - learning_rate * db2
W1 <- W1 - learning_rate * dW1
b1 <- b1 - learning_rate * db1
loss_2 <- c(loss_2, loss)
}
print(a2)
loss_data <- data.frame(epoch = 1:length(loss_2), loss = loss_2)
ggplot(data = loss_data, aes(x = epoch, y = loss)) +
geom_line() +
labs(x = "epoch", y = "loss") +
theme_minimal()
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값
y <- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값
input_size = 2
hidden_size = 3
output_size = 1
W1 <- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)
b1 <- matrix(0, nrow = 4, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)
b2 <- matrix(0, nrow = 4, ncol = hidden_2_size)
W1
b1
W2
b2
learning_rate = 0.1
epochs = 500
# 초기화
loss_2 <- numeric()  # 빈 벡터로 손실을 저장
X %*% W1 + b1
X %*% W1
X
W1
a1 <- sigmoid(z1)
a1
X %*% W1 + b1
sigmoid(z1)
a2
cbind(y,y)
a2
cbind(y,y)
da2 <- a2 - cbind(y,y,y)
da2
