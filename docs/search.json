[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sungil_Park",
    "section": "",
    "text": "Hello, I am Park Sung-il, majoring in Big-data application and Statistics at Hannam University in South Korea.\n\n\nThis is a quarto blog created with Rstudio.\n\n\nCreated for publishing my projects and records about me.\n\n\nEnvironments\n\nSamsung Galaxybook 2 pro 360 i7\nDesktop\n\nUbuntu 20.04 LTS\nRyzen 5600X\nRadeon RX570\nG.SKILL DDR4 8G 25600 CL16 *2"
  },
  {
    "objectID": "basics_example/R basic.html",
    "href": "basics_example/R basic.html",
    "title": "Statistical Analysis with R",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "basics_example/R basic.html#purpose-and-examples-of-performing-statistical-analysis",
    "href": "basics_example/R basic.html#purpose-and-examples-of-performing-statistical-analysis",
    "title": "Statistical Analysis with R",
    "section": "Purpose and examples of performing statistical analysis",
    "text": "Purpose and examples of performing statistical analysis\n\n\n\n\nflowchart TD\n    A(데이터 수집 및 특성 파악) --&gt; B(데이터 특성 상세화 &lt;br&gt; - 분석 방향 설정)\n    B --&gt; |데이터 두개 이상| C( - 분할표 작성&lt;br&gt; - 데이터 사이 의존 관계 분석&lt;br&gt; :카이제곱, 피셔 검정&lt;br&gt; - 특정 분포를 따르는지 분석&lt;br&gt; :콜모고로프-스미노프, &lt;br&gt;샤피로 윌크 검정)\n    B --&gt;|데이터가 한개| D(통계 분석 작업 수행)\n    C--&gt;D\n    D --&gt; E(차이검정&lt;br&gt; - T-test, ANOVA, 비율, 부호, 맥나마 검정)\n    D --&gt; F(인과관계&lt;br&gt; - 상관계수 및 상관분석)\n\n\n\n\n\n\n통계 분석을 수행하는 목적\n차이 검정은 어떤 그룹, 집단, 형태가 차이가 있는지를 알아보는 것입니다.\n\n샘플로 뽑은 데이터가 전체 모집단을 대표한다고 볼 수 있는지를 겁정합니다. 만약 대표한다면, 샘플에서 얻은 데이터로 모집단을 예측할 수 있습니다.\n약을 먹기 전과 후의 환자 상태를 조사한 후에, 이것이 차이가 있는지를 검정합니다. 이것을 통하여 약이 효과가 있는지 없는지를 알 수 있습니다.\n\n인과 관계는 어떤 그룹 사이 인과관계(상관관계)가 있는지를 알아보는 것입니다.\n\n상관관계가 있다면 한쪽에 변화에 대하여 다른쪽의 변화를 예측할 수 있습니다.\n\n\n\n통계에서 사용하는 데이터 유형\n범주형 데이터(Categorical Data) : 사전에 정해진 특정 유형으로 분류되는 데이터\n\n명목형 데이터\n\n성별, 좌파/우파\n\n순서형 데이터 : 분류된 데이터 사이\n\n대/중/소, A/B/C\n\n\n연속형 데이터\n\n등간 척도\n\n온도, 시간 등\n\n비율 척도\n\n키, 몸무게, 점수, 관찰 빈도 등"
  },
  {
    "objectID": "basics_example/R basic.html#표본-만들기-및-기초통계량",
    "href": "basics_example/R basic.html#표본-만들기-및-기초통계량",
    "title": "Statistical Analysis with R",
    "section": "표본 만들기 및 기초통계량",
    "text": "표본 만들기 및 기초통계량\n\n난수생성 & 분포함수 그리기\n정규 분포를 따르는 난수를 생성하고 확률 밀도 함수를 그려 보겠습니다.\n\n# 평균 0, 표준편차 10인 난수를 1000000개 생성\n\nrnorm(1000000,0,10) %&gt;% \n  density() %&gt;% \n  plot()\n\n\n\n\n\n\n표본추출 방법\n단순 표본 추출\n\n# 1 ~ 10 중에 2개 추출\nsample(1:10, 2, replace = TRUE)\n\n[1] 5 3\n\n\n층화 임의 추출\n\niris 데이터 속 종별로 3개씩 추출\n\n\nlibrary(sampling)\n\n# srswor : 복원 단순 임의 추출\nstrata(c(\"Species\"), size = c(3,3,3), method = \"srswor\", data=iris)\n\n       Species ID_unit Prob Stratum\n20      setosa      20 0.06       1\n26      setosa      26 0.06       1\n41      setosa      41 0.06       1\n59  versicolor      59 0.06       2\n72  versicolor      72 0.06       2\n96  versicolor      96 0.06       2\n105  virginica     105 0.06       3\n122  virginica     122 0.06       3\n126  virginica     126 0.06       3\n\n\n\n\n기초통계량\n\nx &lt;- c(1:5)\n# 평균\nmean(x)\n\n[1] 3\n\n# 분산\nvar(x)\n\n[1] 2.5\n\n# 표준편차\nsd(x)\n\n[1] 1.581139\n\n# 기초통계량\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\n\n분할표\n\nx &lt;- c('A','B','C','D','D','A','A','A','C','B','C') %&gt;% factor()\n\n# 각 데이터별 빈도수수\ntable(x)\n\nx\nA B C D \n4 2 3 2"
  },
  {
    "objectID": "basics_example/R basic.html#독립성-및-적합성-검정",
    "href": "basics_example/R basic.html#독립성-및-적합성-검정",
    "title": "Statistical Analysis with R",
    "section": "독립성 및 적합성 검정",
    "text": "독립성 및 적합성 검정\n데이터에 대한 분할표가 구성되었다면, 이것을 대상으로 독립성 검정과 적합도 검정을 수행할 단계입니다."
  },
  {
    "objectID": "basics_example/R basic.html#독립성검정",
    "href": "basics_example/R basic.html#독립성검정",
    "title": "Statistical Analysis with R",
    "section": "독립성검정",
    "text": "독립성검정\n\n카이제곱 검정(Chi-squared test)\n\n검정하려고 하는 가설은 “child1과 child2가 가지고 있는 장난감 비율 차이가 있는가?” 입니다. 즉, 분할표를 구성한 데이터 두개 사이 상호 연관이 있는지를 검정하는 것입니다.\n\n\ntoy &lt;- data.frame(child1 = c(5,11,1),\n                  child2 = c(4,7,3),\n                  row.names = c(\"car\",\"truck\",\"doll\"))\n\ntoy\n\n      child1 child2\ncar        5      4\ntruck     11      7\ndoll       1      3\n\nchisq.test(toy)\n\nWarning in chisq.test(toy): 카이제곱 approximation은 정확하지 않을수도 있습니다\n\n\n\n    Pearson's Chi-squared test\n\ndata:  toy\nX-squared = 1.7258, df = 2, p-value = 0.4219\n\n\n귀무가설 H0 : child1 = child2 (차이가 없다)\n대립가설 H1 : child1 =/= child2 (차이가 있다)\n\np-value가 0.05보다 크기 때문에 귀무가설을 채택한다. 즉, “아이에 따라 장난감 비율 차이가 없다”고 볼 수 있다.\n\n\n\n피셔 검정(Fisher test)\n피셔 검정은 표본 수가 적거나 분할표가 치우치게 분포된 경우에 적용하는 검정입니다.\n앞에서 카이제곱 검정을 했지만 경고가 떴으므로 정확한 결과를 위하여 피셔 검정을 수행합니다.\n\nfisher.test(toy)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  toy\np-value = 0.5165\nalternative hypothesis: two.sided\n\n\n\np-value가 0.05보다 크기 때문에 귀무가설을 채택한다. 즉, “아이에 따라 장난감 비율 차이가 없다”고 볼 수 있다.\n\nex) 냉장고 제품 A,B,C의 시장 점유율은 A가 55%, B가 15%, C가 30% 이다. 특정 지역 냉장고 제품 보유수를 조사하니, 각 320, 80, 265 였다. 이때, 특정 지역의 냉장고 보유 비율이 시장 점유율과 같다고 볼 수 있는가?\nH0 : 특정 지역 보유비율 = 시장점유율\nH1 : 특정 지역 보유비율 =/= 시장점유율\n\nfridge &lt;- data.frame(\"특정지역보유수\" = c(320,80,265), \n                     \"시장점유율_개수\" = c(665*0.55, 665*0.15, 665*0.3),\n                     row.names = c(\"A\",\"B\",\"C\"))\nfridge\n\n  특정지역보유수 시장점유율_개수\nA            320          365.75\nB             80           99.75\nC            265          199.50\n\nchisq.test(fridge)\n\n\n    Pearson's Chi-squared test\n\ndata:  fridge\nX-squared = 14.459, df = 2, p-value = 0.0007251\n\n\n\np-value 가 0.05보다 작으므로 “특정 지역의 보유율과 시장점유율은 같지 않다”고 볼 수 있다."
  },
  {
    "objectID": "basics_example/R basic.html#적합성검정",
    "href": "basics_example/R basic.html#적합성검정",
    "title": "Statistical Analysis with R",
    "section": "적합성검정",
    "text": "적합성검정\nGoodness od Fit test\n\n콜모고로프-스미노프 검정(kolmogorov-Smirnov test)\n콜모고로프-스미노프 검정은 두 데이터 분포가 같은지 검정하는 것입니다.\n즉, 주어진 두개의 데이터를 근거로 두 모집단의 분포가 같은지 검정합니다.\n\nx &lt;- rnorm(50) # 평균이 0, 표준편차가 1인 정규분포를 따르는 난수 50개\ny &lt;- runif(30) # 0 ~ 1 사이의 난수 30개\n\nks.test(x,y)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  x and y\nD = 0.56, p-value = 6.303e-06\nalternative hypothesis: two-sided\n\n\nH0 : x의 분포 = y의 분포 (분포가 같다)\nH1 : x의 분포 =/= y의 분포 (분포가 다르다)\n\np-value가 0.05보다 작으므로 대립가설 채택, “분포가 다르다”고 볼 수 있다다.\n\n\n\n샤피로 윌크 검정(Shapiro-Wilk test)\n샤피로 윌크 검정(Shapiro-Wilk test)은 데이터가 정규분포를 따르는지를 검정하는 방법입니다.\nH0 : 정규 분포를 한다.\nH1 : 정규 분포를 하지 않는다.\n\nrnorm(100,mean=5,sd=3) %&gt;% shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98781, p-value = 0.494\n\n\n\np-value가 0.05보다 크므로 H0채택, “정규 분포를 따른다”고 볼 수 있다."
  },
  {
    "objectID": "recordings.html",
    "href": "recordings.html",
    "title": "Recordings",
    "section": "",
    "text": "Running\n\n\n\nAthletics\n\n\n\nRunning\n\n\n\nsungil_park\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStress is water soluble\n\n\n\nAthletics\n\n\n\nSwimming\n\n\n\nsungil_park\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSungil Gallery\n\n\n\nPhoto\n\n\n\nSome photo\n\n\n\nsungil_park\n\n\nJul 26, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "records/running.html",
    "href": "records/running.html",
    "title": "Running",
    "section": "",
    "text": "Updates at weekend\n\n\nRecord with Galaxy Watch 4\n\n\nGears\n\nNIKE Air Zoom Tempo NEXT% Flyknit\nNIKE Epic React Flyknit 1\n\n\n\n\nDate\nDistance(km)\nTime\nPace(/1km)\n\n\n\n\n2023/07/25\n5.01\n24:49\n4’57\n\n\n2023/07/21\n6.51\n43:41\n6’42\n\n\n2023/07/16\n5.01\n23:49\n4’45\n\n\n2023/06/29\n5.01\n24:14\n4’50\n\n\n2023/06/27\n5.01\n24:12\n4’49\n\n\n2023/06/23\n5.00\n23:47\n4’44\n\n\n2023/06/21\n7.21\n42:56\n5’56\n\n\n2023/06/13\n5.00\n25:07\n5’00\n\n\n2023/06/06\n5.01\n23:58\n4’46\n\n\n2023/06/01\n9.89\n50:55\n5:08\n\n\n2023/05/30\n5.01\n24:41\n4:55\n\n\n2023/05/23\n5.11\n24:42\n4’49\n\n\n2023/05/21\n10.01\n51:33\n5’08\n\n\n2023/05/18\n5.03\n24:07\n4’47\n\n\n2023/05/14\n8.24\n46:59\n5’42\n\n\n2023/05/11\n10.02\n52:59\n5’17\n\n\n2023/05/09\n3\n15:24\n5’06\n\n\n2023/05/07\n3.9\n19:55\n5’06\n\n\n2023/04/28\n5.22\n27:33\n5’16\n\n\n2023/04/23\n3.56\n18:30\n5’11\n\n\n2023/04/20\n10.06\n52:39\n5’13\n\n\n2023/04/16\n5.88\n35:36\n6’03\n\n\n2023/03/25\n4.2\n30:16\nTreadmil\n\n\n2023/03/24\n3.54\n23:25\n6’36\n\n\n2023/03/19\n3.01\n15:36\n5’10\n\n\n2023/03/17\n3.03\n17:55\n5’54\n\n\n2023/03/15\n3.38\n23:21\n6’54\n\n\n2023/03/05\n4.14\n30:30\n7’20"
  },
  {
    "objectID": "kaggle_geo.html",
    "href": "kaggle_geo.html",
    "title": "Basics examples",
    "section": "",
    "text": "Backpropagtion with R\n\n\n\n\n\n\n\nR\n\n\n\n\nBasics of Deep learning\n\n\n\n\n\n\nOct 21, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nPydeck example\n\n\n\n\n\n\n\npython\n\n\n\n\nInteractive map\n\n\n\n\n\n\nMay 17, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression with R\n\n\n\n\n\n\n\nR\n\n\n\n\nR basics\n\n\n\n\n\n\nApr 14, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Analysis with R\n\n\n\n\n\n\n\nR\n\n\n\n\nExamples of performing Statistical analysis\n\n\n\n\n\n\nApr 11, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nweek_1a_numpy\n\n\n\n\n\n\n\nPython\n\n\nLecture\n\n\n\n\nPython Basics\n\n\n\n\n\n\nApr 6, 2023\n\n\nJiho Yeo\n\n\n\n\n\n\n  \n\n\n\n\nweek_1b_pandas\n\n\n\n\n\n\n\nPython\n\n\nLecture\n\n\n\n\nPython Basics\n\n\n\n\n\n\nApr 6, 2023\n\n\nJiho Yeo\n\n\n\n\n\n\n  \n\n\n\n\nGeocomputation with R\n\n\n\n\n\n\n\nR\n\n\n\n\nCreative & Experimental geography\n\n\n\n\n\n\nMar 22, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nTensorflow Regression example\n\n\n\n\n\n\n\nPython\n\n\n\n\nCreate a DNN model to predict penguin weight\n\n\n\n\n\n\nMar 21, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nggplot example\n\n\n\n\n\n\n\nR\n\n\n\n\nData Visualization with ggplot\n\n\n\n\n\n\nMar 15, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nggplot geospatial example\n\n\n\n\n\n\n\nR\n\n\n\n\nGeospartial Data Visualization with ggplot\n\n\n\n\n\n\nMar 15, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nYour first map\n\n\n\n\n\n\n\nPython\n\n\nKaggle tutorial\n\n\n\n\nKaggle Geospatial Analysis (1/5)\n\n\n\n\n\n\nMar 5, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate Reference Systems\n\n\n\n\n\n\n\nPython\n\n\nKaggle tutorial\n\n\n\n\nKaggle Geospatial Analysis (2/5)\n\n\n\n\n\n\nMar 5, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Maps\n\n\n\n\n\n\n\nPython\n\n\nKaggle tutorial\n\n\n\n\nKaggle Geospatial Analysis (3/5)\n\n\n\n\n\n\nMar 3, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nManipulating Geospatial Data\n\n\n\n\n\n\n\nPython\n\n\nKaggle tutorial\n\n\n\n\nKaggle Geospatial Analysis (4/5)\n\n\n\n\n\n\nMar 2, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nProximity Analysis\n\n\n\n\n\n\n\nPython\n\n\nKaggle tutorial\n\n\n\n\nKaggle Geospatial Analysis (5/5)\n\n\n\n\n\n\nMar 1, 2023\n\n\nSungil Park\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "My Blog",
    "section": "",
    "text": "My Blog"
  },
  {
    "objectID": "posts/pyhton_playground.html",
    "href": "posts/pyhton_playground.html",
    "title": "Python playground",
    "section": "",
    "text": "import pandas as pd\npd.__version__\n\n'2.0.3'\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\ns2['bob']\n\n83\npandas can draw plot\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\ns7.plot()\nplt.show()\n\n\n\n\nplt.clf()\ndates = pd.date_range('2016/10/29 5:30pm', periods = 12, freq=\"H\")\ndates\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()\n리샘플링\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n&lt;pandas.core.resample.DatetimeIndexResampler object at 0x7f9feadf33a0&gt;\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").mean()\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n업생플링& 보간\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()\n\n\n\nplt.clf()"
  },
  {
    "objectID": "posts/pyhton_playground.html#파이썬-심화",
    "href": "posts/pyhton_playground.html#파이썬-심화",
    "title": "Python playground",
    "section": "파이썬 심화",
    "text": "파이썬 심화\n\nfruit = [\"apple\",\"banana\", \"cherry\"]\n\nupper_fruit = []\n\n# for 루프를 사용하여 리스트의 각 요소를 대문자로 변환하여 빈 리스트에 추가\nfor x in fruit:\n  upper_fruit.append(x.upper())\n\n\nprint(upper_fruit)\n\n['APPLE', 'BANANA', 'CHERRY']\n\n\n\ntotal = []\nfor i in range(1, 1000):\n    if i % 5 == 0 or i % 7 == 0:\n        total.append(i)\n\nprint(\"list:\", total)\n\nlist: [5, 7, 10, 14, 15, 20, 21, 25, 28, 30, 35, 40, 42, 45, 49, 50, 55, 56, 60, 63, 65, 70, 75, 77, 80, 84, 85, 90, 91, 95, 98, 100, 105, 110, 112, 115, 119, 120, 125, 126, 130, 133, 135, 140, 145, 147, 150, 154, 155, 160, 161, 165, 168, 170, 175, 180, 182, 185, 189, 190, 195, 196, 200, 203, 205, 210, 215, 217, 220, 224, 225, 230, 231, 235, 238, 240, 245, 250, 252, 255, 259, 260, 265, 266, 270, 273, 275, 280, 285, 287, 290, 294, 295, 300, 301, 305, 308, 310, 315, 320, 322, 325, 329, 330, 335, 336, 340, 343, 345, 350, 355, 357, 360, 364, 365, 370, 371, 375, 378, 380, 385, 390, 392, 395, 399, 400, 405, 406, 410, 413, 415, 420, 425, 427, 430, 434, 435, 440, 441, 445, 448, 450, 455, 460, 462, 465, 469, 470, 475, 476, 480, 483, 485, 490, 495, 497, 500, 504, 505, 510, 511, 515, 518, 520, 525, 530, 532, 535, 539, 540, 545, 546, 550, 553, 555, 560, 565, 567, 570, 574, 575, 580, 581, 585, 588, 590, 595, 600, 602, 605, 609, 610, 615, 616, 620, 623, 625, 630, 635, 637, 640, 644, 645, 650, 651, 655, 658, 660, 665, 670, 672, 675, 679, 680, 685, 686, 690, 693, 695, 700, 705, 707, 710, 714, 715, 720, 721, 725, 728, 730, 735, 740, 742, 745, 749, 750, 755, 756, 760, 763, 765, 770, 775, 777, 780, 784, 785, 790, 791, 795, 798, 800, 805, 810, 812, 815, 819, 820, 825, 826, 830, 833, 835, 840, 845, 847, 850, 854, 855, 860, 861, 865, 868, 870, 875, 880, 882, 885, 889, 890, 895, 896, 900, 903, 905, 910, 915, 917, 920, 924, 925, 930, 931, 935, 938, 940, 945, 950, 952, 955, 959, 960, 965, 966, 970, 973, 975, 980, 985, 987, 990, 994, 995]\n\n\n\n죄수의 딜레마\n나와 공범이 체포되었다.\n서로 협조를 하면 각자 5년의 형량을 받는다.\n한명만 협조를 하게 되면 협조한 자는 석방, 협조하지 않은 자는 10년의 형량을 받는다.\n내가 협조를 하였을떄 상대방이 협조할 확률이 50%라면 내 형량의 기댓값은 얼마인가?\n(1000번 반복한다.)\n\nme_decision=[]\nme_term=[]\nyou_decision=[]\nyou_term=[]\n\nimport random\n\nfor k in range(1,1001):\n  me=['협조']\n  you=random.sample(['묵비권','협조'],1)\n  me_decision.append(me)\n  you_decision.append(you)\n  \n  if you==me:\n    me_term.append(5)\n    you_term.append(5)\n    \n  else:\n    me_term.append(0)\n    you_term.append(10)\n\n    \nprint(me_decision, me_term, you_decision, you_term)\n\n[['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조']] [5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 0, 0, 5, 5, 5, 0, 0, 5, 5, 5, 0, 5, 0, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 5] [['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조']] [5, 10, 10, 5, 10, 5, 10, 5, 5, 10, 10, 10, 5, 10, 10, 5, 5, 10, 5, 5, 5, 10, 5, 5, 10, 5, 5, 5, 10, 10, 10, 10, 10, 10, 5, 5, 5, 5, 10, 10, 5, 10, 10, 10, 10, 5, 10, 5, 5, 5, 5, 10, 5, 10, 5, 5, 5, 5, 5, 10, 10, 5, 5, 10, 10, 10, 5, 10, 5, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 10, 10, 5, 5, 10, 10, 10, 10, 5, 5, 5, 5, 10, 5, 5, 10, 5, 5, 10, 10, 10, 10, 10, 5, 5, 10, 5, 10, 10, 10, 5, 10, 10, 10, 5, 10, 5, 5, 10, 5, 5, 10, 5, 10, 5, 10, 5, 10, 5, 5, 5, 10, 5, 10, 5, 10, 5, 10, 5, 5, 10, 10, 10, 5, 10, 10, 10, 5, 10, 5, 5, 10, 10, 10, 5, 10, 10, 10, 10, 10, 5, 5, 10, 5, 5, 5, 5, 5, 10, 10, 10, 10, 5, 5, 5, 5, 10, 5, 10, 10, 5, 10, 10, 10, 5, 5, 5, 10, 5, 10, 10, 5, 5, 5, 10, 10, 5, 5, 5, 10, 5, 10, 10, 10, 10, 5, 5, 5, 10, 5, 10, 5, 5, 5, 10, 5, 5, 5, 10, 10, 5, 5, 5, 5, 10, 5, 5, 5, 10, 10, 10, 10, 5, 5, 10, 10, 10, 5, 5, 10, 10, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 5, 10, 5, 5, 10, 5, 10, 5, 5, 5, 5, 10, 5, 5, 5, 10, 10, 5, 5, 5, 10, 10, 5, 10, 10, 10, 10, 5, 5, 10, 5, 10, 5, 10, 10, 5, 10, 10, 5, 10, 10, 10, 5, 5, 10, 10, 5, 5, 5, 10, 5, 5, 10, 10, 5, 5, 5, 10, 10, 10, 5, 5, 5, 10, 5, 5, 10, 5, 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 10, 10, 10, 10, 10, 10, 5, 10, 5, 10, 5, 5, 10, 5, 10, 10, 10, 10, 5, 5, 10, 10, 5, 10, 10, 5, 5, 5, 10, 5, 5, 10, 5, 10, 10, 5, 5, 10, 10, 5, 5, 5, 5, 5, 5, 10, 5, 10, 5, 5, 10, 10, 5, 10, 5, 10, 10, 10, 10, 10, 5, 10, 10, 10, 5, 5, 10, 5, 10, 5, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 10, 5, 10, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 5, 5, 5, 5, 10, 5, 10, 10, 5, 5, 10, 5, 5, 5, 10, 10, 10, 10, 10, 5, 5, 10, 5, 10, 10, 10, 5, 10, 10, 5, 10, 5, 10, 5, 5, 10, 5, 10, 5, 5, 5, 5, 5, 10, 5, 10, 5, 5, 10, 5, 5, 5, 5, 10, 5, 5, 5, 10, 10, 10, 10, 5, 10, 10, 10, 10, 5, 10, 5, 5, 5, 5, 10, 5, 10, 5, 10, 10, 5, 10, 5, 10, 10, 5, 5, 10, 10, 10, 5, 10, 10, 5, 10, 5, 10, 10, 5, 10, 10, 5, 10, 5, 10, 5, 5, 10, 5, 10, 5, 5, 5, 5, 10, 5, 5, 5, 10, 5, 10, 5, 5, 5, 5, 10, 5, 5, 10, 5, 5, 5, 10, 10, 10, 10, 5, 5, 10, 10, 5, 5, 10, 10, 5, 10, 10, 10, 10, 5, 5, 10, 5, 10, 5, 10, 10, 5, 5, 10, 5, 5, 5, 5, 10, 5, 5, 5, 10, 10, 10, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 5, 5, 10, 5, 5, 5, 5, 10, 10, 5, 5, 5, 10, 5, 10, 10, 5, 10, 10, 10, 10, 5, 10, 10, 5, 10, 5, 10, 5, 10, 5, 5, 5, 10, 10, 5, 5, 5, 5, 5, 10, 10, 5, 5, 10, 10, 10, 5, 5, 10, 5, 10, 10, 5, 10, 10, 5, 10, 10, 10, 5, 5, 5, 10, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 10, 10, 5, 5, 10, 5, 5, 10, 5, 5, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 5, 5, 5, 10, 10, 5, 10, 10, 10, 5, 5, 5, 10, 10, 10, 10, 5, 5, 10, 10, 10, 10, 10, 10, 5, 10, 10, 10, 10, 5, 10, 5, 10, 10, 5, 10, 10, 10, 10, 5, 5, 10, 10, 5, 5, 5, 10, 10, 5, 5, 5, 5, 10, 10, 10, 10, 5, 5, 10, 10, 10, 10, 5, 10, 10, 5, 10, 10, 5, 10, 10, 10, 5, 5, 10, 5, 5, 10, 10, 10, 10, 5, 10, 5, 10, 10, 5, 10, 5, 5, 5, 5, 5, 10, 5, 5, 5, 10, 5, 10, 10, 10, 5, 10, 10, 5, 10, 10, 5, 10, 5, 10, 5, 10, 5, 10, 10, 5, 5, 10, 5, 5, 5, 10, 10, 10, 5, 10, 10, 10, 5, 10, 5, 5, 5, 5, 10, 5, 5, 5, 5, 5, 5, 10, 10, 5, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 5, 10, 10, 5, 10, 5, 5, 10, 10, 10, 10, 5, 5, 10, 5, 10, 5, 5, 5, 10, 10, 5, 10, 10, 10, 5, 5, 10, 5, 10, 10, 5, 5, 10, 10, 10, 5, 10, 10, 10, 5, 10, 5, 5, 5, 5, 10, 10, 10, 5, 5, 10, 10, 10, 10, 5, 5, 10, 5, 5, 10, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 10, 5, 5, 10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 10, 10, 5, 5, 10, 5, 5, 5, 10, 5, 10, 5, 10, 10, 5, 10, 10, 5, 5, 10, 10, 5, 10, 10, 10, 10, 10, 5, 5, 5, 5, 5, 10, 5, 10, 10, 10, 10, 10, 10, 10, 5, 5, 10, 10, 10, 5, 10, 10, 5, 5, 10, 5, 10, 10, 5]\n\nprint('내형량',sum(me_term)/1000,'공범형량',sum(you_term)/1000)\n\n내형량 2.455 공범형량 7.545\n\n\n\nfruit = [\"apple\",\"banana\", \"cherry\"]\nage = [1,2,3]\n\nfor x in range(3):\n  print(\"i'm\",fruit[x],\", my age\",age[x])\n\ni'm apple , my age 1\ni'm banana , my age 2\ni'm cherry , my age 3\n\n\n\na = ([1,2,3],[4,5,6],[7,8])\nb = []\nfor i in a:\n  for x in i:\n    b.append(x)\n    \nb\n\n[1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\nimport pandas as pd\nmpg = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/mpg.csv\")\n\n\nmpg['hwy+displ'] = mpg.apply(lambda x: x[\"displ\"]+x[\"hwy\"],axis=1)\n\n\nmpg['hwy+displ']\n\n0      30.8\n1      30.8\n2      33.0\n3      32.0\n4      28.8\n       ... \n229    30.0\n230    31.0\n231    28.8\n232    28.8\n233    29.6\nName: hwy+displ, Length: 234, dtype: float64"
  },
  {
    "objectID": "posts/pyhton_playground.html#img",
    "href": "posts/pyhton_playground.html#img",
    "title": "Python playground",
    "section": "img",
    "text": "img\n\n#!pip install opencv-python\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n\nimg_raw = cv2.imread(\"lena_color.png\")\n\n\nimg_gray = cv2.cvtColor(img_raw,cv2.COLOR_BGR2GRAY)\n\n\nplt.imshow(img_gray,\"gray\")\nplt.show()"
  },
  {
    "objectID": "posts/pizza.html",
    "href": "posts/pizza.html",
    "title": "pizza",
    "section": "",
    "text": "Let the radius of the pizza be ‘z’ and the depth be ‘a’.\n\n\nThe volume of the pizza is π * z**2 * a.\npi z z a\ndelicious pizza"
  },
  {
    "objectID": "posts/tashu_2020.html",
    "href": "posts/tashu_2020.html",
    "title": "타슈 데이터 분석",
    "section": "",
    "text": "대전광역시에는 시민들의 이동 편의성을 위하여 “타슈 공영자전거 시스템”을 운영 중에 있습니다.\n주어진 데이터로 해당 문제를 풀어 보시오.\n\n\ntashu_station = 타슈 보관소 데이터\ntashu_2020 = 2020년 사용내역 데이터\ndaejeon_area = 지리데이터\n\ntashu_station = read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/tasu_station.csv\") ## github을 통해 utf8로 변환\ntashu_2020 = read.csv(\"~/dataset/2020.csv\")\ndaejeon_area = readOGR(\"~/dataset/LSMD_ADM_SECT_UMD_30.shp\",encoding = \"euc-kr\")\n\nOGR data source with driver: ESRI Shapefile \nSource: \"/home/sungil/dataset/LSMD_ADM_SECT_UMD_30.shp\", layer: \"LSMD_ADM_SECT_UMD_30\"\nwith 187 features\nIt has 5 fields\n\n\n\n\n\n\ntashu_2020 %&gt;% str()\n\n'data.frame':   604446 obs. of  6 variables:\n $ 대여스테이션: int  174 174 117 167 203 167 92 203 154 110 ...\n $ 대여일시    : num  2.02e+13 2.02e+13 2.02e+13 2.02e+13 2.02e+13 ...\n $ 반납스테이션: int  224 224 115 94 203 94 83 55 169 16 ...\n $ 반납일시    : num  2.02e+13 2.02e+13 2.02e+13 2.02e+13 2.02e+13 ...\n $ 이동거리    : int  640 640 1070 1490 0 1540 1190 750 1110 1490 ...\n $ 회원구분    : int  2 2 0 2 1 2 2 1 2 0 ...\n\ntashu_station %&gt;% str()\n\n'data.frame':   261 obs. of  4 variables:\n $ ID         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ stationName: chr  \"무역전시관입구(택시승강장)\" \"대전컨벤션센터\" \"한밭수목원1\" \"초원아파트(104동 버스정류장)\" ...\n $ holder     : int  14 20 19 12 13 12 13 12 12 12 ...\n $ address    : chr  \"대전광역시 유성구 도룡동 3-8\" \"대전광역시 유성구 도룡동 4-19\" \"대전광역시 서구 만년동 396\" \"대전광역시 서구 만년동 401\" ...\n\nnames(tashu_station) &lt;- c(\"번호\", \"대여소명\", \"거치대\", \"주소\")\nnames(tashu_2020) &lt;- c(\"대여스테이션\", \"대여일시\", \"반납스테이션\", \"반납일시\", \"이동거리\",\"회원구분\")\n\n\n\n\n\nggmap::register_google(key = \"your API\")\n\n\n\n\ntashu_station &lt;- ggmap::mutate_geocode(data = tashu_station, location = `주소`, source = 'google')\n\n\n\n\n\ndj_map &lt;- get_map(\"daejeon\",zoom=12)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=daejeon&zoom=12&size=640x640&scale=2&maptype=terrain&language=en-EN&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=daejeon&key=xxx&gt;\n\nggmap(dj_map) + geom_point(data = tashu_station,\n                           aes(x=lon,y=lat),\n                           size=2,\n                           alpha=0.7)\n\nWarning: Removed 16 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\ntashu_2020 &lt;- tashu_2020 %&gt;% mutate(대여년월일 = substr(tashu_2020$대여일시,1,8))\n\n\n\n\n\ntbl_1 &lt;- data.frame(tashu_2020$대여년월일 %&gt;% table()) %&gt;% arrange(-Freq) #20200913\ntop1_day &lt;- c(20200913)\n\ntashu_top1_day &lt;- tashu_2020 %&gt;% filter(대여년월일==top1_day)\ntashu_top1_day %&gt;% dim()\n\n[1] 4048    7\n\ntashu_top1_day &lt;- tashu_top1_day %&gt;% filter(대여스테이션 != 262 | 반납스테이션 != 262)\ntashu_top1_day %&gt;% dim()\n\n[1] 4046    7\n\nborrow_count &lt;- tashu_top1_day %&gt;% \n  group_by(대여스테이션) %&gt;% \n  summarize(cnt = n()) %&gt;% \n  na.omit()\n\nnames(borrow_count) &lt;- c(\"번호\", \"cnt\")\nborrow_count &lt;- borrow_count %&gt;% full_join(tashu_station,by = \"번호\") %&gt;% filter(번호 != 262)\nborrow_count[is.na(borrow_count)] &lt;- 0\n\ntashu_sf &lt;- st_as_sf(borrow_count,coords = c(\"lon\",\"lat\"))\nst_crs(tashu_sf) &lt;- 4326\n\ntashu_sf &lt;- tashu_sf %&gt;% filter(대여소명 != \"읍내동우편취급국\")\n\n\ntashu_sf %&gt;% head()\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 127.3752 ymin: 36.35531 xmax: 127.3917 ymax: 36.37513\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n   번호   cnt 대여소명                    거치대 주소             geometry\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                        &lt;int&gt; &lt;chr&gt;         &lt;POINT [°]&gt;\n1     1    54 무역전시관입구(택시승강장)      14 대전… (127.3894 36.37513)\n2     2    61 대전컨벤션센터                  20 대전… (127.3917 36.37502)\n3     3   285 한밭수목원1                     19 대전… (127.3878 36.36793)\n4     4    24 초원아파트(104동 버스정류…      12 대전… (127.3795 36.36799)\n5     5    55 둔산대공원 입구(버스정류장)     13 대전… (127.3846 36.35531)\n6     6    25 백합네거리(농협)                12 대전…  (127.3752 36.3617)\n\n\n\n\n\n\nggmap(dj_map) + \n  coord_sf(crs = st_crs(4326))+\n  geom_sf(data = tashu_sf,aes(size = cnt,alpha = 0.7),inherit.aes = FALSE)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\n\n\n같은 CRS로 맟추기\n\ndaejeon_area %&gt;% st_crs()\n\nCoordinate Reference System:\n  User input: Korea 2000 / Unified CS \n  wkt:\nPROJCRS[\"Korea 2000 / Unified CS\",\n    BASEGEOGCRS[\"Korea 2000\",\n        DATUM[\"Geocentric datum of Korea\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4737]],\n    CONVERSION[\"Korea Unified Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",38,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",127.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",2000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (X)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (Y)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"Korea, Republic of (South Korea)\"],\n        BBOX[28.6,122.71,40.27,134.28]],\n    ID[\"EPSG\",5179]]\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\nd2 &lt;- spTransform(daejeon_area, to_crs)\n\nWarning: PROJ support is provided by the sf and terra packages among others\n\ndaejeon_area_sf &lt;- st_as_sf(d2)\n\n\ndaejeon_area_sf &lt;- st_as_sf(d2)\ndaejeon_area_sf &lt;- daejeon_area_sf %&gt;% st_make_valid()\n\ndaejeon_area_sf %&gt;% plot()\n\n\n\n\n\ndae2 &lt;- daejeon_area_sf[\"EMD_NM\"]\nst_crs(dae2) &lt;- 4326\n\ngeom_joined &lt;- dae2 %&gt;% st_join(tashu_sf,join = st_intersects)\n\ngeom_joined[is.na(geom_joined)] &lt;- 0\n\n\ndj_map2 &lt;- get_map(\"daejeon\", zoom=11)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=daejeon&zoom=11&size=640x640&scale=2&maptype=terrain&language=en-EN&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=daejeon&key=xxx&gt;\n\nggmap(dj_map2) + \n  coord_sf(crs = st_crs(4326))+\n  geom_sf(data=geom_joined,aes(fill=cnt,alpha = 0.9),inherit.aes = FALSE)+\n  scale_fill_continuous(name = \"count\",\n                        labels = c(c(0,5,10,15) %&gt;% paste()),\n                        breaks = c(0,5,10,15),\n                        limits = c(0,20))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one."
  },
  {
    "objectID": "posts/tashu_2020.html#타슈-데이터-분석하기",
    "href": "posts/tashu_2020.html#타슈-데이터-분석하기",
    "title": "타슈 데이터 분석",
    "section": "",
    "text": "대전광역시에는 시민들의 이동 편의성을 위하여 “타슈 공영자전거 시스템”을 운영 중에 있습니다.\n주어진 데이터로 해당 문제를 풀어 보시오.\n\n\ntashu_station = 타슈 보관소 데이터\ntashu_2020 = 2020년 사용내역 데이터\ndaejeon_area = 지리데이터\n\ntashu_station = read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/tasu_station.csv\") ## github을 통해 utf8로 변환\ntashu_2020 = read.csv(\"~/dataset/2020.csv\")\ndaejeon_area = readOGR(\"~/dataset/LSMD_ADM_SECT_UMD_30.shp\",encoding = \"euc-kr\")\n\nOGR data source with driver: ESRI Shapefile \nSource: \"/home/sungil/dataset/LSMD_ADM_SECT_UMD_30.shp\", layer: \"LSMD_ADM_SECT_UMD_30\"\nwith 187 features\nIt has 5 fields\n\n\n\n\n\n\ntashu_2020 %&gt;% str()\n\n'data.frame':   604446 obs. of  6 variables:\n $ 대여스테이션: int  174 174 117 167 203 167 92 203 154 110 ...\n $ 대여일시    : num  2.02e+13 2.02e+13 2.02e+13 2.02e+13 2.02e+13 ...\n $ 반납스테이션: int  224 224 115 94 203 94 83 55 169 16 ...\n $ 반납일시    : num  2.02e+13 2.02e+13 2.02e+13 2.02e+13 2.02e+13 ...\n $ 이동거리    : int  640 640 1070 1490 0 1540 1190 750 1110 1490 ...\n $ 회원구분    : int  2 2 0 2 1 2 2 1 2 0 ...\n\ntashu_station %&gt;% str()\n\n'data.frame':   261 obs. of  4 variables:\n $ ID         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ stationName: chr  \"무역전시관입구(택시승강장)\" \"대전컨벤션센터\" \"한밭수목원1\" \"초원아파트(104동 버스정류장)\" ...\n $ holder     : int  14 20 19 12 13 12 13 12 12 12 ...\n $ address    : chr  \"대전광역시 유성구 도룡동 3-8\" \"대전광역시 유성구 도룡동 4-19\" \"대전광역시 서구 만년동 396\" \"대전광역시 서구 만년동 401\" ...\n\nnames(tashu_station) &lt;- c(\"번호\", \"대여소명\", \"거치대\", \"주소\")\nnames(tashu_2020) &lt;- c(\"대여스테이션\", \"대여일시\", \"반납스테이션\", \"반납일시\", \"이동거리\",\"회원구분\")\n\n\n\n\n\nggmap::register_google(key = \"your API\")\n\n\n\n\ntashu_station &lt;- ggmap::mutate_geocode(data = tashu_station, location = `주소`, source = 'google')\n\n\n\n\n\ndj_map &lt;- get_map(\"daejeon\",zoom=12)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=daejeon&zoom=12&size=640x640&scale=2&maptype=terrain&language=en-EN&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=daejeon&key=xxx&gt;\n\nggmap(dj_map) + geom_point(data = tashu_station,\n                           aes(x=lon,y=lat),\n                           size=2,\n                           alpha=0.7)\n\nWarning: Removed 16 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\ntashu_2020 &lt;- tashu_2020 %&gt;% mutate(대여년월일 = substr(tashu_2020$대여일시,1,8))\n\n\n\n\n\ntbl_1 &lt;- data.frame(tashu_2020$대여년월일 %&gt;% table()) %&gt;% arrange(-Freq) #20200913\ntop1_day &lt;- c(20200913)\n\ntashu_top1_day &lt;- tashu_2020 %&gt;% filter(대여년월일==top1_day)\ntashu_top1_day %&gt;% dim()\n\n[1] 4048    7\n\ntashu_top1_day &lt;- tashu_top1_day %&gt;% filter(대여스테이션 != 262 | 반납스테이션 != 262)\ntashu_top1_day %&gt;% dim()\n\n[1] 4046    7\n\nborrow_count &lt;- tashu_top1_day %&gt;% \n  group_by(대여스테이션) %&gt;% \n  summarize(cnt = n()) %&gt;% \n  na.omit()\n\nnames(borrow_count) &lt;- c(\"번호\", \"cnt\")\nborrow_count &lt;- borrow_count %&gt;% full_join(tashu_station,by = \"번호\") %&gt;% filter(번호 != 262)\nborrow_count[is.na(borrow_count)] &lt;- 0\n\ntashu_sf &lt;- st_as_sf(borrow_count,coords = c(\"lon\",\"lat\"))\nst_crs(tashu_sf) &lt;- 4326\n\ntashu_sf &lt;- tashu_sf %&gt;% filter(대여소명 != \"읍내동우편취급국\")\n\n\ntashu_sf %&gt;% head()\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 127.3752 ymin: 36.35531 xmax: 127.3917 ymax: 36.37513\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n   번호   cnt 대여소명                    거치대 주소             geometry\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                        &lt;int&gt; &lt;chr&gt;         &lt;POINT [°]&gt;\n1     1    54 무역전시관입구(택시승강장)      14 대전… (127.3894 36.37513)\n2     2    61 대전컨벤션센터                  20 대전… (127.3917 36.37502)\n3     3   285 한밭수목원1                     19 대전… (127.3878 36.36793)\n4     4    24 초원아파트(104동 버스정류…      12 대전… (127.3795 36.36799)\n5     5    55 둔산대공원 입구(버스정류장)     13 대전… (127.3846 36.35531)\n6     6    25 백합네거리(농협)                12 대전…  (127.3752 36.3617)\n\n\n\n\n\n\nggmap(dj_map) + \n  coord_sf(crs = st_crs(4326))+\n  geom_sf(data = tashu_sf,aes(size = cnt,alpha = 0.7),inherit.aes = FALSE)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\n\n\n같은 CRS로 맟추기\n\ndaejeon_area %&gt;% st_crs()\n\nCoordinate Reference System:\n  User input: Korea 2000 / Unified CS \n  wkt:\nPROJCRS[\"Korea 2000 / Unified CS\",\n    BASEGEOGCRS[\"Korea 2000\",\n        DATUM[\"Geocentric datum of Korea\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4737]],\n    CONVERSION[\"Korea Unified Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",38,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",127.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",2000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (X)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (Y)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"Korea, Republic of (South Korea)\"],\n        BBOX[28.6,122.71,40.27,134.28]],\n    ID[\"EPSG\",5179]]\n\nto_crs = CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\nd2 &lt;- spTransform(daejeon_area, to_crs)\n\nWarning: PROJ support is provided by the sf and terra packages among others\n\ndaejeon_area_sf &lt;- st_as_sf(d2)\n\n\ndaejeon_area_sf &lt;- st_as_sf(d2)\ndaejeon_area_sf &lt;- daejeon_area_sf %&gt;% st_make_valid()\n\ndaejeon_area_sf %&gt;% plot()\n\n\n\n\n\ndae2 &lt;- daejeon_area_sf[\"EMD_NM\"]\nst_crs(dae2) &lt;- 4326\n\ngeom_joined &lt;- dae2 %&gt;% st_join(tashu_sf,join = st_intersects)\n\ngeom_joined[is.na(geom_joined)] &lt;- 0\n\n\ndj_map2 &lt;- get_map(\"daejeon\", zoom=11)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=daejeon&zoom=11&size=640x640&scale=2&maptype=terrain&language=en-EN&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=daejeon&key=xxx&gt;\n\nggmap(dj_map2) + \n  coord_sf(crs = st_crs(4326))+\n  geom_sf(data=geom_joined,aes(fill=cnt,alpha = 0.9),inherit.aes = FALSE)+\n  scale_fill_continuous(name = \"count\",\n                        labels = c(c(0,5,10,15) %&gt;% paste()),\n                        breaks = c(0,5,10,15),\n                        limits = c(0,20))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one."
  },
  {
    "objectID": "posts/Linear_model.html",
    "href": "posts/Linear_model.html",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "",
    "text": "#update.packages(ask = FALSE, checkBuilt = TRUE)\n#install.packages(\"xfun\")"
  },
  {
    "objectID": "posts/Linear_model.html#패키지-로드",
    "href": "posts/Linear_model.html#패키지-로드",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "패키지 로드",
    "text": "패키지 로드"
  },
  {
    "objectID": "posts/Linear_model.html#목적",
    "href": "posts/Linear_model.html#목적",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "목적",
    "text": "목적\n\n아파트 매매, 가격결정에 관한 유용한 인사이트 획득"
  },
  {
    "objectID": "posts/Linear_model.html#데이터",
    "href": "posts/Linear_model.html#데이터",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "데이터",
    "text": "데이터\n활용 데이터는 2022-12월부터, 2023-01 까지의 천안시 아파트 매매내역 1059건이다.\n아파트명, 전용면적, 거래금액, 건축년도, 층, 근처 역, 인구수, 크기, 외국인 비율, 초등학교 거리로 구성되어 있다."
  },
  {
    "objectID": "posts/Linear_model.html#전처리",
    "href": "posts/Linear_model.html#전처리",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "전처리",
    "text": "전처리\n\n위도, 경도 데이터 제거\n컬럼명 영문 변환\n\n\ndata &lt;- read_excel(\"~/Sungil_LAB/linear_model_dataset.xlsx\")\ndata &lt;- data %&gt;% select(-c(Latitude,Longitude))\nnames(data) &lt;- c(\"year_built\",\"floor\",\"station_m\",\"pop\",\"dong_area\",\"foreign_ratio\",\"school_m\",\"market_m\",\"price_per_pyeong\")\n\n\n\n\n한\n영\n\n\n\n\n건축년도\nyear_built\n\n\n층\nfloor\n\n\n근처 역까지의 거리\nstation_m\n\n\n법정동 인구수\npop\n\n\n법정동크기\ndong_area\n\n\n외국인 비율\nforeign_ratio\n\n\n근처 초등학교까지의 거리\nschool_m\n\n\n근처 마트까지의 거리\nmarket_m\n\n\n평당 금액\nprice_per_pyeong\n\n\n\n\ndata %&gt;% head()\n\n# A tibble: 6 × 9\n  year_built floor station_m   pop dong_area foreign_ratio school_m market_m\n       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       2015     9      324.  8191      0.61        0.0603     478.     1198\n2       2015     9      324.  8191      0.61        0.0603     478.     1198\n3       2015     9      324.  8191      0.61        0.0603     478.     1198\n4       2022    20      451.  8191      0.61        0.0603     460.     1149\n5       2022    12      451.  8191      0.61        0.0603     460.     1149\n6       2022    11      451.  8191      0.61        0.0603     460.     1149\n# ℹ 1 more variable: price_per_pyeong &lt;dbl&gt;"
  },
  {
    "objectID": "posts/Linear_model.html#이상치-확인",
    "href": "posts/Linear_model.html#이상치-확인",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "이상치 확인",
    "text": "이상치 확인\n\n박스플롯\n\n\n\n\n\n\n층, 전철역 거리, 법정동크기, 초등학교 거리, 대형마트의 거리 변수의 이상치가 많고 뭉쳐있는걸 확인할 수있음\n이상치가 있는 독립변수들을 로그변환한 결과 이상치가 줄어듬\nOutlier 70건 제거\n\n\n\n\n\n\n\n\n상관행렬\n\ndata_cor &lt;- cor(data)\ndata_cor\n\n                   year_built       floor    station_m         pop   dong_area\nyear_built        1.000000000  0.36861613  0.008798964  0.48743016 -0.12096269\nfloor             0.368616127  1.00000000 -0.042935596  0.22219497 -0.11654329\nstation_m         0.008798964 -0.04293560  1.000000000  0.09491796  0.55525138\npop               0.487430164  0.22219497  0.094917964  1.00000000 -0.01967819\ndong_area        -0.120962689 -0.11654329  0.555251378 -0.01967819  1.00000000\nforeign_ratio    -0.064676692 -0.05008170  0.001685205 -0.38236764  0.38510073\nschool_m         -0.010043271 -0.07108195  0.271737273 -0.11386670  0.35683852\nmarket_m         -0.444229582 -0.28400512  0.177691898 -0.50896731  0.40061845\nprice_per_pyeong  0.787452386  0.38437866 -0.019798398  0.63206041 -0.25067183\n                 foreign_ratio    school_m   market_m price_per_pyeong\nyear_built        -0.064676692 -0.01004327 -0.4442296       0.78745239\nfloor             -0.050081700 -0.07108195 -0.2840051       0.38437866\nstation_m          0.001685205  0.27173727  0.1776919      -0.01979840\npop               -0.382367642 -0.11386670 -0.5089673       0.63206041\ndong_area          0.385100726  0.35683852  0.4006185      -0.25067183\nforeign_ratio      1.000000000  0.29844999  0.2365821      -0.19486466\nschool_m           0.298449991  1.00000000  0.3200577      -0.07806446\nmarket_m           0.236582096  0.32005775  1.0000000      -0.42468714\nprice_per_pyeong  -0.194864664 -0.07806446 -0.4246871       1.00000000\n\ncorrplot(data_cor,\n         method = \"shade\",\n         addCoef.col=\"black\",\n         tl.col = \"black\")"
  },
  {
    "objectID": "posts/Linear_model.html#선형회귀-모델",
    "href": "posts/Linear_model.html#선형회귀-모델",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "선형회귀 모델",
    "text": "선형회귀 모델\n\n다중선형회귀 모델\n\nmodel &lt;- lm(price_per_pyeong ~ ., data = data)\n\nsummary(model)\n\n\nCall:\nlm(formula = price_per_pyeong ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-979.18 -156.49  -10.74  148.06 1374.58 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.814e+04  2.287e+03 -29.792  &lt; 2e-16 ***\nyear_built     3.369e+01  1.145e+00  29.428  &lt; 2e-16 ***\nfloor          6.146e+01  1.141e+01   5.386 9.03e-08 ***\nstation_m      5.885e+01  1.471e+01   4.000 6.82e-05 ***\npop            1.356e-02  7.012e-04  19.340  &lt; 2e-16 ***\ndong_area     -1.608e+02  1.250e+01 -12.870  &lt; 2e-16 ***\nforeign_ratio  1.608e+03  3.473e+02   4.630 4.16e-06 ***\nschool_m      -1.137e+01  1.654e+01  -0.687    0.492    \nmarket_m       1.199e+02  1.515e+01   7.916 6.63e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 277.9 on 980 degrees of freedom\nMultiple R-squared:  0.7565,    Adjusted R-squared:  0.7545 \nF-statistic: 380.6 on 8 and 980 DF,  p-value: &lt; 2.2e-16\n\n\n\n초등학교까지의 거리의 p-value가 0.05 보다 크므로 유의하지 않다고 볼 수 있다.\nR-squred 값이 0.7565으로 독립변수들이 종속변수(평당 금액)을 76% 정도 설명할 수 있음을 의미함"
  },
  {
    "objectID": "posts/Linear_model.html#vif-분산팽창계수",
    "href": "posts/Linear_model.html#vif-분산팽창계수",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "VIF (분산팽창계수)",
    "text": "VIF (분산팽창계수)\n\nmodel %&gt;% vif()\n\n   year_built         floor     station_m           pop     dong_area \n     1.599917      1.183459      1.619763      2.037188      2.283779 \nforeign_ratio      school_m      market_m \n     1.679231      1.291166      1.939407 \n\n\n\nVIF값이 10 이상인 변수가 없는걸로 보아 다중공선성이 없는 것으로 판단됨"
  },
  {
    "objectID": "posts/Linear_model.html#변수선택",
    "href": "posts/Linear_model.html#변수선택",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "변수선택",
    "text": "변수선택\n\nStepwise selection (단계적 선택법)\n\nmodel_2 &lt;- step(model,direction = \"both\")\n\nStart:  AIC=11139.64\nprice_per_pyeong ~ year_built + floor + station_m + pop + dong_area + \n    foreign_ratio + school_m + market_m\n\n                Df Sum of Sq       RSS   AIC\n- school_m       1     36469  75717165 11138\n&lt;none&gt;                        75680696 11140\n- station_m      1   1235310  76916006 11154\n- foreign_ratio  1   1655169  77335865 11159\n- floor          1   2240128  77920824 11166\n- market_m       1   4838695  80519391 11199\n- dong_area      1  12790706  88471402 11292\n- pop            1  28884594 104565290 11457\n- year_built     1  66879023 142559719 11764\n\nStep:  AIC=11138.12\nprice_per_pyeong ~ year_built + floor + station_m + pop + dong_area + \n    foreign_ratio + market_m\n\n                Df Sum of Sq       RSS   AIC\n&lt;none&gt;                        75717165 11138\n+ school_m       1     36469  75680696 11140\n- station_m      1   1199157  76916322 11152\n- foreign_ratio  1   1623235  77340400 11157\n- floor          1   2252594  77969759 11165\n- market_m       1   4899147  80616312 11198\n- dong_area      1  12908715  88625880 11292\n- pop            1  28848133 104565298 11455\n- year_built     1  67327710 143044875 11765\n\n\nAIC 값으로 초등학교까지의 거리(school_m) 변수가 제거되었다.\n\nmodel_2 %&gt;% summary()\n\n\nCall:\nlm(formula = price_per_pyeong ~ year_built + floor + station_m + \n    pop + dong_area + foreign_ratio + market_m, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-982.6 -160.5  -14.2  148.1 1371.4 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.800e+04  2.278e+03 -29.849  &lt; 2e-16 ***\nyear_built     3.360e+01  1.138e+00  29.535  &lt; 2e-16 ***\nfloor          6.162e+01  1.141e+01   5.402 8.26e-08 ***\nstation_m      5.727e+01  1.453e+01   3.942 8.67e-05 ***\npop            1.354e-02  7.006e-04  19.333  &lt; 2e-16 ***\ndong_area     -1.613e+02  1.247e+01 -12.932  &lt; 2e-16 ***\nforeign_ratio  1.560e+03  3.402e+02   4.586 5.10e-06 ***\nmarket_m       1.176e+02  1.476e+01   7.967 4.49e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 277.8 on 981 degrees of freedom\nMultiple R-squared:  0.7564,    Adjusted R-squared:  0.7547 \nF-statistic: 435.2 on 7 and 981 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Linear_model.html#회귀진단",
    "href": "posts/Linear_model.html#회귀진단",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "회귀진단",
    "text": "회귀진단\n\nplot(model_2)\n\n\n\n\n\n\n\n\n\n\n\n\nols_plot_cooksd_bar(model_2)\n\n\n\n\n\nCook의거리 기준 Outlier 관측치 37건을 제거한 모델\n\ncooked_dist &lt;- cooks.distance(model_2)\n\ndata_2 &lt;- data %&gt;% \n  cbind(cooked_dist) %&gt;% \n  filter(cooked_dist&lt;0.005)\n\nmodel_3 &lt;- lm(price_per_pyeong ~ year_built+floor+pop+dong_area+station_m+foreign_ratio+school_m+market_m , data=data_2)\n\nmodel_3 &lt;- step(model_3,direction = \"both\")\n\nStart:  AIC=10482.23\nprice_per_pyeong ~ year_built + floor + pop + dong_area + station_m + \n    foreign_ratio + school_m + market_m\n\n                Df Sum of Sq       RSS   AIC\n- school_m       1     29283  56566077 10481\n&lt;none&gt;                        56536794 10482\n- station_m      1   1222622  57759416 10501\n- floor          1   2230901  58767696 10517\n- foreign_ratio  1   2448685  58985479 10521\n- market_m       1   6013763  62550557 10576\n- dong_area      1  12614591  69151385 10672\n- pop            1  27421249  83958043 10857\n- year_built     1  69040014 125576808 11240\n\nStep:  AIC=10480.72\nprice_per_pyeong ~ year_built + floor + pop + dong_area + station_m + \n    foreign_ratio + market_m\n\n                Df Sum of Sq       RSS   AIC\n&lt;none&gt;                        56566077 10481\n+ school_m       1     29283  56536794 10482\n- station_m      1   1193424  57759501 10499\n- floor          1   2242828  58808905 10516\n- foreign_ratio  1   2434488  59000565 10519\n- market_m       1   6082380  62648457 10576\n- dong_area      1  12765427  69331504 10672\n- pop            1  27394726  83960804 10855\n- year_built     1  69484688 126050766 11242\n\nmodel_3 %&gt;% summary()\n\n\nCall:\nlm(formula = price_per_pyeong ~ year_built + floor + pop + dong_area + \n    station_m + foreign_ratio + market_m, data = data_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-772.07 -145.82   -8.12  147.76  828.28 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -7.092e+04  2.058e+03 -34.468  &lt; 2e-16 ***\nyear_built     3.494e+01  1.026e+00  34.053  &lt; 2e-16 ***\nfloor          6.388e+01  1.044e+01   6.118 1.39e-09 ***\npop            1.398e-02  6.539e-04  21.382  &lt; 2e-16 ***\ndong_area     -1.680e+02  1.151e+01 -14.596  &lt; 2e-16 ***\nstation_m      5.857e+01  1.313e+01   4.463 9.07e-06 ***\nforeign_ratio  1.971e+03  3.093e+02   6.374 2.88e-10 ***\nmarket_m       1.453e+02  1.442e+01  10.075  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 244.8 on 944 degrees of freedom\nMultiple R-squared:  0.7991,    Adjusted R-squared:  0.7976 \nF-statistic: 536.3 on 7 and 944 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n변수\nEstimate\nStd. Error\nt value\nPr(&gt;\n\n\n\n\nIntercept\n-7.092e+04\n2.058e+03\n-34.468\n&lt; 2e-16 ***\n\n\nyear_built\n3.494e+01\n1.026e+00\n34.053\n&lt; 2e-16 ***\n\n\nfloor\n6.388e+01\n1.044e+01\n6.118\n1.39e-09 ***\n\n\npop\n1.398e-02\n6.539e-04\n21.382\n&lt; 2e-16 ***\n\n\ndong_area\n-1.680e+02\n1.151e+01\n-14.596\n&lt; 2e-16 ***\n\n\nstation_m\n5.857e+01\n1.313e+01\n4.463\n9.07e-06 ***\n\n\nforeign_ratio\n1.971e+03\n3.093e+02\n6.374\n2.88e-10 ***\n\n\nmarket_m\n1.453e+02\n1.442e+01\n10.075\n&lt; 2e-16 ***\n\n\n\n\nR-squared값이 0.7564 -&gt; 0.7991로 향상됨을 볼 수 있다.\n따라서 회귀식은 평당거래액 = 건축년도 x1 + 층 x2 + 인구수 x3+ 법정동크기 x4 + 외국인비율 x5 + 대형마트 거리 x6 + e 로 결정"
  },
  {
    "objectID": "posts/Linear_model.html#표준화-회귀계수",
    "href": "posts/Linear_model.html#표준화-회귀계수",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "표준화 회귀계수",
    "text": "표준화 회귀계수\n\n\n\nCall:\nlm(formula = price_per_pyeong ~ year_built + floor + pop + dong_area + \n    station_m + foreign_ratio + market_m, data = data_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-772.07 -145.82   -8.12  147.76  828.28 \n\nCoefficients:\n                Estimate Standardized Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -7.092e+04           NA  2.058e+03 -34.468  &lt; 2e-16 ***\nyear_built     3.494e+01    6.241e-01  1.026e+00  34.053  &lt; 2e-16 ***\nfloor          6.388e+01    9.752e-02  1.044e+01   6.118 1.39e-09 ***\npop            1.398e-02    4.566e-01  6.539e-04  21.382  &lt; 2e-16 ***\ndong_area     -1.680e+02   -3.290e-01  1.151e+01 -14.596  &lt; 2e-16 ***\nstation_m      5.857e+01    8.220e-02  1.313e+01   4.463 9.07e-06 ***\nforeign_ratio  1.971e+03    1.194e-01  3.093e+02   6.374 2.88e-10 ***\nmarket_m       1.453e+02    2.126e-01  1.442e+01  10.075  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 244.8 on 944 degrees of freedom\nMultiple R-squared:  0.7991,    Adjusted R-squared:  0.7976 \nF-statistic: 536.3 on 7 and 944 DF,  p-value: &lt; 2.2e-16\n\n\n\n표준화 회귀계수표\n\n\n       독립변수 표준화 회귀계수\n1   (Intercept)              NA\n2    year_built          0.6241\n3         floor          0.0975\n4           pop          0.4566\n5     dong_area         -0.3290\n6     station_m          0.0822\n7 foreign_ratio          0.1194\n8      market_m          0.2126"
  },
  {
    "objectID": "posts/Linear_model.html#결론",
    "href": "posts/Linear_model.html#결론",
    "title": "천안시 아파트 매매가 회귀분석",
    "section": "결론",
    "text": "결론\n\n법정동 크기가 커질수록 평균거래액은 낮아짐\n\n법정동은 인구수가 많아질수록 분할되기 때문에 법정동 크기가 큰 관측치는 교외지역이라서 집값이 떨어진다고 볼 수 있다.\n\n건축년도가 예상보다 영향이 많음\n층은 너무 저층이거나 고층일때 비선호되기에, 변수변환이 필요하다고 봄\n\n\n회귀식\n\n-70920 + 34.94 * 2009 + 63.88 * log(11) + 0.01398 * 40187  - 168.0 * log(4.49) + 58.57 * log(2300)+ 1971 * 0.01 + 145.3 * log(699) \n\n[1] 1161.886"
  },
  {
    "objectID": "basics_example/vis_geo.html",
    "href": "basics_example/vis_geo.html",
    "title": "ggplot geospatial example",
    "section": "",
    "text": "rm(list = ls())\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(colorspace)\nlibrary(stringr)\nlibrary(geojsonsf)\nlibrary(sf)\n\nLinking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "basics_example/vis_geo.html#library-packages",
    "href": "basics_example/vis_geo.html#library-packages",
    "title": "ggplot geospatial example",
    "section": "",
    "text": "rm(list = ls())\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(colorspace)\nlibrary(stringr)\nlibrary(geojsonsf)\nlibrary(sf)\n\nLinking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "basics_example/vis_geo.html#map-korea",
    "href": "basics_example/vis_geo.html#map-korea",
    "title": "ggplot geospatial example",
    "section": "Map Korea",
    "text": "Map Korea\n\nload .json\n\nkor_sigu &lt;- geojson_sf(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json\")\n\nWarning in readLines(con):\n'https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json'에서\n불완전한 마지막 행이 발견되었습니다\n\nkor_202202 &lt;- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/202202_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9.csv\")\n\nkor_202202 %&gt;% head()\n\n           행정구역 행정구역_코드 총인구수  세대수 세대당_인구 남자_인구수\n1       서울특별시     1100000000  9508451 4442586        2.14     4615823\n2 서울특별시 종로구    1111000000   144575   73763        1.96       70092\n3   서울특별시 중구    1114000000   122167   63644        1.92       59446\n4 서울특별시 용산구    1117000000   222413  111134        2.00      106881\n5 서울특별시 성동구    1120000000   285137  134286        2.12      138866\n6 서울특별시 광진구    1121500000   340494  168975        2.02      164226\n  여자_인구수 남여_비율\n1     4892628      0.94\n2       74483      0.94\n3       62721      0.95\n4      115532      0.93\n5      146271      0.95\n6      176268      0.93\n\nkor_202202 %&gt;% sapply(class)\n\n     행정구역 행정구역_코드      총인구수        세대수   세대당_인구 \n  \"character\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\" \n  남자_인구수   여자_인구수     남여_비율 \n    \"numeric\"     \"numeric\"     \"numeric\" \n\nkor_202202$행정구역_코드 &lt;- kor_202202$행정구역_코드 %&gt;% format()\n\n\n\nMerge data by 행정구역_코드\n\nuse_map &lt;- kor_sigu\nuse_map$행정구역_코드 &lt;- paste(use_map$SIG_CD,\"00000\",sep = \"\")\nuse_map &lt;- use_map %&gt;% merge(kor_202202,by = \"행정구역_코드\", all.x=T)\n\n\n\nPlot\n\nuse_map %&gt;% ggplot(aes(fill=총인구수))+\n  geom_sf(color = \"grey90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"인구수\",\n    palette = \"Blues\", type = \"seq\", na.value = \"grey60\",\n    direction = 1,\n    breaks = seq(0,10,2) * 1e+5,\n    labels = format(seq(0,10,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2)\n  )\n\n\n\n\n\n\nPlot Daejeon\n\ndaejeon_map &lt;-  use_map %&gt;% filter(행정구역 %&gt;% substr(1,5) == \"대전광역시\")\n\ndaejeon_map %&gt;% ggplot(aes(fill=총인구수))+\n  geom_sf(color = \"grey90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"인구수\",\n    palette = \"Blues\", type = \"seq\", na.value = \"grey60\",\n    direction = 1,\n    breaks = seq(0,10,2) * 1e+5,\n    labels = format(seq(0,10,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.95,0.2)\n  )\n\n\n\n\n\n\nPlot Gender_ratio\n\nuse_map %&gt;% ggplot(aes(fill = 남여_비율))+\n  geom_sf()+\n  scale_fill_continuous_diverging(\n    name = \"남자/여자\",\n    palette = \"BLue-Red\",\n    mid=1,\n  limits = 1 + c(-1,+1)*0.35,\n  rev = T)+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2)\n  )"
  },
  {
    "objectID": "basics_example/vis_geo.html#년-3월-시군구-인구수",
    "href": "basics_example/vis_geo.html#년-3월-시군구-인구수",
    "title": "ggplot geospatial example",
    "section": "2023년 3월 시군구 인구수",
    "text": "2023년 3월 시군구 인구수\n시군구 지도 데이터, 행정구역 코드 10자리로 만들기\n\nuse_map &lt;- kor_sigu\nuse_map$행정구역_코드 &lt;- paste(use_map$SIG_CD,\"00000\",sep = \"\")\n\n\n데이터 로드\n2023년 3월 주민등록 인구통계 데이터, 행정안전부\n\ndata_pop &lt;- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/202303_202303_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9_%EC%9B%94%EA%B0%84.csv\",encoding = \"utf-8\")\ndata_pop %&gt;% head() \n\n                        행정구역 X2023년03월_총인구수 X2023년03월_세대수\n1       서울특별시  (1100000000)            9,426,404          4,463,385\n2 서울특별시 종로구 (1111000000)              141,060             72,679\n3   서울특별시 중구 (1114000000)              120,963             63,862\n4 서울특별시 용산구 (1117000000)              217,756            109,735\n5 서울특별시 성동구 (1120000000)              280,240            133,513\n6 서울특별시 광진구 (1121500000)              336,801            169,787\n  X2023년03월_세대당.인구 X2023년03월_남자.인구수 X2023년03월_여자.인구수\n1                    2.11               4,566,299               4,860,105\n2                    1.94                  68,170                  72,890\n3                    1.89                  58,699                  62,264\n4                    1.98                 104,640                 113,116\n5                    2.10                 136,233                 144,007\n6                    1.98                 162,209                 174,592\n  X2023년03월_남여.비율\n1                  0.94\n2                  0.94\n3                  0.94\n4                  0.93\n5                  0.95\n6                  0.93\n\n\n\n\n전처리\n\n인구수0명 출장소 제외\n행정구역 코드 10자리 추출\n인구수 숫자형으로 변환\n시 단위 제외, 정렬\n\n\ndata_202303 &lt;- data_pop %&gt;% \n  filter(X2023년03월_총인구수&gt;0) %&gt;%  \n  select(행정구역,X2023년03월_총인구수) %&gt;% \n  mutate(행정구역_코드 = str_sub(행정구역,-11,-2),\n         X2023년03월_총인구수 = gsub(\",\",\"\",X2023년03월_총인구수) %&gt;% as.numeric()) %&gt;% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %&gt;% \n  arrange(desc(X2023년03월_총인구수))\n\ndata_202303 %&gt;% head()\n\n                      행정구역 X2023년03월_총인구수 행정구역_코드\n1   경기도 수원시 (4111000000)              1192225    4111000000\n2   경기도 고양시 (4128000000)              1077909    4128000000\n3   경기도 용인시 (4146000000)              1073513    4146000000\n4 경상남도 창원시 (4812000000)              1017273    4812000000\n5   경기도 성남시 (4113000000)               923416    4113000000\n6   경기도 화성시 (4159000000)               922231    4159000000\n\n\n\n\n지도 데이터와 병합\n\nuse_map &lt;- use_map %&gt;% merge(data_202303,by = \"행정구역_코드\", all.x=T)\n\n\n\nPlot\n\nuse_map %&gt;% ggplot(aes(fill=X2023년03월_총인구수))+\n  geom_sf(color = \"grey90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"2023년 3월 인구수\",\n    palette = \"Blues\", type = \"seq\", na.value = \"grey60\",\n    direction = 1,\n    breaks = seq(0,10,2) * 1e+5,\n    labels = format(seq(0,10,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2))+\n  labs(title = \"2023년 3월\")"
  },
  {
    "objectID": "basics_example/vis_geo.html#인구수-증가율",
    "href": "basics_example/vis_geo.html#인구수-증가율",
    "title": "ggplot geospatial example",
    "section": "인구수 증가율",
    "text": "인구수 증가율\n\nfile_2023 &lt;- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/202303_202303_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9_%EC%9B%94%EA%B0%84.csv\",encoding = \"utf-8\")\n\nfile_2013 &lt;- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/201303_201303_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9_%EC%9B%94%EA%B0%84.csv\",encoding = \"UTF-8\")\n\n\n전처리\n\n인구수 0명 이상 필터\n행정구역(지역코드), 총인구수 열만 선택\n행정구역(지역코드)에서 지역코드와 행정구역 분리\n시 단위 제외, 인구수 기준 정렬\n\n\ndata_2023 &lt;- file_2023 %&gt;%\n  filter(X2023년03월_총인구수&gt;0) %&gt;%  \n  select(행정구역,X2023년03월_총인구수) %&gt;% \n  mutate(행정구역_코드 = str_sub(행정구역,-11,-2),\n         X2023년03월_총인구수 = gsub(\",\",\"\",X2023년03월_총인구수) %&gt;% as.numeric(),\n         행정구역 =  sapply(행정구역, function(x) strsplit(x, \"(\", fixed=T)[[1]][1]),\n         행정구역 = sapply(행정구역, function(x) gsub(\"( *)$\", \"\", x) %&gt;% paste())) %&gt;% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %&gt;% \n  arrange(desc(X2023년03월_총인구수))\n\ndata_2013 &lt;- file_2013 %&gt;% \n  filter(X2013년03월_총인구수&gt;0) %&gt;%  \n  select(행정구역,X2013년03월_총인구수) %&gt;% \n  mutate(행정구역_코드 = str_sub(행정구역,-11,-2),\n         X2013년03월_총인구수 = gsub(\",\",\"\",X2013년03월_총인구수) %&gt;% as.numeric(),\n         행정구역 =  sapply(행정구역, function(x) strsplit(x, \"(\", fixed=T)[[1]][1]),\n         행정구역 = sapply(행정구역, function(x) gsub(\"( *)$\", \"\", x) %&gt;% paste())) %&gt;% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %&gt;% \n  arrange(desc(X2013년03월_총인구수))\n\n\n\n병합\n\n지역코드 기준 병합\n인구성장률 열 추가\n중복 열 제거, 인구성장률 기준 정렬\n서울, 대전, 대구, 부산지역만 필터\n시도 추출, factor 변환\n\n\nkor_census &lt;- data_2013 %&gt;% \n  merge(data_2023,by = \"행정구역_코드\", all.x=T) %&gt;%  \n  mutate(성장률 = (X2023년03월_총인구수 - X2013년03월_총인구수) / X2013년03월_총인구수) %&gt;% \n  select(행정구역.x,X2013년03월_총인구수,X2023년03월_총인구수, 성장률, 행정구역_코드) %&gt;% \n  filter(substr(행정구역.x,1,2) %in% c(\"서울\",\"대전\",\"대구\",\"부산\")) %&gt;%\n  arrange(desc(성장률))\n\nnames(kor_census) &lt;- c(\"행정구역\", \"X2013인구수\",\"X2023인구수\",\"성장률\",\"행정구역_코드\")\n\nkor_census$시도 = sapply(kor_census$행정구역,\n                           function(x) strsplit(x, \" \")[[1]][1])\nkor_census$시도 = factor(kor_census$시도,\n                           levels = c(\"서울특별시\",\"대전광역시\",\"대구광역시\",\"부산광역시\"))\n\n\n\nPlot\n\nregion_colors &lt;- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\")\n\n\nggplot(kor_census,aes(x = reorder(행정구역,성장률),y= 성장률, fill = 시도))+\n  geom_col()+\n  scale_y_continuous(name = \"인구성장률\",\n                     expand = c(0,0),\n                     labels = scales::percent_format(scale = 100))+\n  scale_fill_manual(values = region_colors)+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank())+\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),legend.position = c(.78,.28),legend.background = element_rect(fill = \"#FFFFFFB0\"))\n\n\n\n\n\n\nMap plot\n\nkor_map &lt;- kor_sigu\nkor_map$행정구역_코드 &lt;- paste(kor_map$SIG_CD,\"00000\",sep=\"\")\n\n\nkor_census_2 &lt;- data_2013 %&gt;% \n  merge(data_2023,by = \"행정구역_코드\", all.x=T) %&gt;%  \n  mutate(성장률 = (X2023년03월_총인구수 - X2013년03월_총인구수) / X2013년03월_총인구수) %&gt;% \n  select(행정구역.x,X2013년03월_총인구수,X2023년03월_총인구수, 성장률, 행정구역_코드) %&gt;% \n  arrange(desc(성장률))\n\nnames(kor_census_2) &lt;- c(\"행정구역\", \"X2013인구수\",\"X2023인구수\",\"성장률\",\"행정구역_코드\")\n\n\nkor_map &lt;- kor_map %&gt;% left_join(kor_census_2, by=\"행정구역_코드\")\n\n\nkor_map %&gt;% ggplot(aes(fill=성장률))+\n  geom_sf()+\n  scale_fill_continuous_diverging(\n    name = \"인구성장률\",\n    palette = \"BLue-Red\",\n    limits = c(-0.4,2.4))+\n  theme_minimal()+\n  theme(legend.title.align = 0.5,\n        legend.text.align = 1.0,\n        legend.position = c(0.85,0.2))"
  },
  {
    "objectID": "basics_example/vis_geo.html#인구증감",
    "href": "basics_example/vis_geo.html#인구증감",
    "title": "ggplot geospatial example",
    "section": "인구증감",
    "text": "인구증감\n\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor_census_2013_2023.csv\",encoding = \"utf-8\")\n\ndata &lt;- data %&gt;% mutate(인구증감 = 총인구수_2023-총인구수_2013)\n\nuse_map &lt;- kor_sigu\nuse_map$행정구역_코드 &lt;- paste(use_map$SIG_CD,\"00000\",sep = \"\") %&gt;% as.numeric()\nuse_map &lt;- use_map %&gt;% merge(data,by = \"행정구역_코드\")\n\nuse_map %&gt;% ggplot(aes(fill = 인구증감))+\n  geom_sf()+\n  coord_sf(datum = NA)+\n  scale_fill_continuous_diverging(\n    name = \"인구증감\",\n    palette = \"BLue-Red\",\n    na.value = \"grey40\",\n    mid=0,\n    rev = T,\n    limits = c(-4,4)*100000,\n    labels = format(seq(-4,4,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(legend.position = c(0.85,0.2))"
  },
  {
    "objectID": "basics_example/vis_geo.html#mapdeck",
    "href": "basics_example/vis_geo.html#mapdeck",
    "title": "ggplot geospatial example",
    "section": "Mapdeck",
    "text": "Mapdeck\n\nlibrary(dplyr)\nlibrary(mapdeck)\nset_token(\"pk.eyJ1Ijoic3VuZ2lsZW8iLCJhIjoiY2xoYTRwbXEzMGR6eTNkbXBoZnluNXdyYSJ9.Id1fKIbhtvA9Mrnyo_1JQA\")\n\ncrash_data = read.csv(\"https://git.io/geocompr-mapdeck\")\ncrash_data = na.omit(crash_data)\n\nms = mapdeck_style(\"dark\")\n\n# mapdeck(style = \"mapbox://styles/sungileo/clhtwpzyw00rl01rhfoiafw6q\")\n\n\ngrid_map\n\nmapdeck()함수의 주요 Arguments\n\nstyle : the style of the map (see mapdeck_style) : one of streets, outdoors, light, dark, satellite, satellite-streets 생성할 맵의 스타일\npitch : the pitch angle of the map 맵의 기울기를 설정합니다. 0은 수평을 의미하고, 60은 수직\nzoom : zoom level of the map 초기 맵의 줌 레벨을 설정\nlocation : unnamed vector of lon and lat coordinates (in that order) 초기 맵의 중심 위치를 지정\n\nadd_grid()함수의 주요 Argurments\n\ndata: 격자를 생성하는 데 사용할 데이터 프레임\nlon : column containing longitude values\nlat : column containing latitude values\ncell_size : size of each cell in meters. Default 1000 격자 셀의 크기\nelevation : the height the polygon extrudes from the map. Only available if neither stroke_colour or stroke_width are supplied. Default 0 격자의 높이\nlayer_id : single value specifying an id for the layer 격자 레이어의 고유 식별자를 지정하는 데 사용. 이 식별자는 나중에 해당 레이어를 참조하거나 다른 함수를 통해 조작할 때 사용\ncolour_range : vector of 6 hex colours\n\n\n\nmapdeck(style = ms, \n        pitch = 45, \n        location = c(0, 52), \n        zoom = 4) %&gt;%\n  add_grid(data = crash_data, \n           lat = \"lat\", \n           lon = \"lng\", \n           cell_size = 1000,\n           elevation_scale = 50, \n           layer_id = \"grid_layer\",\n           colour_range = viridisLite::plasma(6))\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n\n\n\narc map\n\nadd_arc()함수의 주요 Argurments\nadd_arc function - RDocumentation\n\nlayer_id : single value specifying an id for the layer. Use this value to distinguish between shape layers of the same type. Layers with the same id are likely to conflict and not plot correctly\norigin : vector of longitude and latitude columns, and optionally an elevation column, or an sfc column\ndestination : vector of longitude and latitude columns, and optionally an elevatino column, or an sfc column\nstroke_from : column of data or hex colour to use as the staring stroke colour. IIf using a hex colour, use either a single value, or a column of hex colours on data\nstroke_to : column of data or hex colour to use as the ending stroke colour. If using a hex colour, use either a single value, or a column of hex colours on data\nstroke_width : width of the stroke in pixels\n\n\nVisualize flight data in USA\n\nurl &lt;- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'\nflights &lt;- read.csv(url)\nflights$id &lt;- seq_len(nrow(flights))\nflights$stroke &lt;- sample(1:3, size = nrow(flights), replace = T)\n\nmapdeck(style = ms, \n        pitch = 45 ) %&gt;%\n  add_arc(data = flights, \n          layer_id = \"arc_layer\", \n          origin = c(\"start_lon\", \"start_lat\"), \n          destination = c(\"end_lon\", \"end_lat\"), \n          stroke_from = \"stroke\", \n          stroke_to = \"stroke\", \n          stroke_width = \"stroke\" \n  )\n\n\n\n\n\n\nmapdeck(style = 'mapbox://styles/mapbox/dark-v9', \n        pitch = 45 ) %&gt;%\n  add_animated_arc(data = flights, \n                   layer_id = \"arc_layer\", \n                   origin = c(\"start_lon\", \"start_lat\"), \n                   destination = c(\"end_lon\", \"end_lat\"), \n                   stroke_from = \"stroke\", \n                   stroke_to = \"stroke\", \n                   stroke_width = \"stroke\")\n\nanimated_arc is an experimental layer and the function may change without warning\n\n\n\n\n\n\n\n\nheatmap\n\nmapdeck(style = ms, \n        pitch = 45, \n        location = c(0, 52), \n        zoom = 4)%&gt;%\n  add_heatmap(data = crash_data[1:30000,],\n              lat = \"lat\",\n              lon = \"lng\", \n              colour_range = colourvalues::colour_values(1:6, palette = \"inferno\"))\n\n\n\n\n\n\nroads %&gt;% View()\n\nmapdeck(style = mapdeck_style(\"dark\"), \n        zoom = 10) %&gt;%\n  add_path(data = roads,\n            stroke_colour = \"FQID\",\n           layer_id = \"path_layer\")\n\n\n\n\n\n\n\nhexagon map\n\nmapdeck(style = mapdeck_style(\"dark\"), \n        pitch = 45, \n        location = c(0, 52), \n        zoom = 4) %&gt;%\n  add_hexagon(data = crash_data, \n           lat = \"lat\", \n           lon = \"lng\", \n           elevation_scale = 100, \n           layer_id = \"hex_layer\",\n           colour_range = viridisLite::plasma(6))"
  },
  {
    "objectID": "basics_example/vis_geo.html#leaflet",
    "href": "basics_example/vis_geo.html#leaflet",
    "title": "ggplot geospatial example",
    "section": "leaflet",
    "text": "leaflet\n\nlibrary(leaflet)\nlibrary(spData)\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\ncycle_hire %&gt;% head()\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1975742 ymin: 51.49313 xmax: -0.08460569 ymax: 51.53006\nGeodetic CRS:  WGS 84\n  id               name             area nbikes nempty\n1  1       River Street      Clerkenwell      4     14\n2  2 Phillimore Gardens       Kensington      2     34\n3  3 Christopher Street Liverpool Street      0     32\n4  4  St. Chad's Street     King's Cross      4     19\n5  5     Sedding Street    Sloane Square     15     12\n6  6 Broadcasting House       Marylebone      0     18\n                      geometry\n1  POINT (-0.1099705 51.52916)\n2  POINT (-0.1975742 51.49961)\n3 POINT (-0.08460569 51.52128)\n4  POINT (-0.1209737 51.53006)\n5   POINT (-0.156876 51.49313)\n6  POINT (-0.1442289 51.51812)\n\nlnd %&gt;% head()\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -0.4615023 ymin: 51.28676 xmax: 0.3340155 ymax: 51.63173\nGeodetic CRS:  WGS 84\n                  NAME  GSS_CODE  HECTARES NONLD_AREA ONS_INNER SUB_2009\n1 Kingston upon Thames E09000021  3726.117      0.000         F     &lt;NA&gt;\n2              Croydon E09000008  8649.441      0.000         F     &lt;NA&gt;\n3              Bromley E09000006 15013.487      0.000         F     &lt;NA&gt;\n4             Hounslow E09000018  5658.541     60.755         F     &lt;NA&gt;\n5               Ealing E09000009  5554.428      0.000         F     &lt;NA&gt;\n6             Havering E09000016 11445.735    210.763         F     &lt;NA&gt;\n  SUB_2006                       geometry\n1     &lt;NA&gt; MULTIPOLYGON (((-0.3306791 ...\n2     &lt;NA&gt; MULTIPOLYGON (((-0.06402124...\n3     &lt;NA&gt; MULTIPOLYGON (((0.01213094 ...\n4     &lt;NA&gt; MULTIPOLYGON (((-0.2445624 ...\n5     &lt;NA&gt; MULTIPOLYGON (((-0.4118327 ...\n6     &lt;NA&gt; MULTIPOLYGON (((0.1586928 5...\n\npal = colorNumeric(\"RdYlBu\", domain = cycle_hire$nbikes)\n\nleaflet(data = cycle_hire) %&gt;% \n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;% \n  addPolygons(data = lnd, fill = FALSE) %&gt;% \n  addLegend(pal = pal, values = ~nbikes) %&gt;% \n  setView(lng = -0.1, 51.5, zoom = 12) %&gt;% \n  addMiniMap()\n\n\n\n\n\n\n# creat a basic map\nleaflet() %&gt;%\n  addTiles() %&gt;% # add default OpenStreetMap map tiles\n  setView( lng=127.063, lat=37.513, zoom = 6) # korea, zoom 6\n\n\n\n\n\n\n지도 스타일 추가하기\n\naddProviderTiles(“Tile Name Here”) 를 이용하여 외부 지도 타일을 추가(아래에서 원하는 스타일 map 이름 선택)\n\n\n\n# map style: NASA\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView( lng=127.063, lat=37.513, zoom = 6) %&gt;%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\")\n\n\n\n\n# map style: Esri.WorldImagery\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView( lng=127.063, lat=37.513, zoom = 16) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\")\n\n\n\n\n\n\n지도 좌표에 표시 풍선 추가하기\n\naddMarkers() 메소드를 사용하면 위도(latitude), 경도(longitude) 좌표 위치에 풍선 모양의 표식과 커서를 클릭했을 때 팝업으로 나타나는 설명을 추가할 수 있음\n\n\n\n# adding Popup\npopup = c(\"한남대학교 빅데이터응용학과\")\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(lng = c(127.4219), # longitude\n             lat = c(36.3548), # latitude\n             popup = popup)"
  },
  {
    "objectID": "basics_example/vis_geo.html#ggmap",
    "href": "basics_example/vis_geo.html#ggmap",
    "title": "ggplot geospatial example",
    "section": "ggmap",
    "text": "ggmap\n\nlibrary(ggmap)\n\nℹ Google's Terms of Service: &lt;https://mapsplatform.google.com&gt;\nℹ Please cite ggmap if you use it! Use `citation(\"ggmap\")` for details.\n\n\n\ngetmap &lt;- get_googlemap(\"seoul\")\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=seoul&zoom=10&size=640x640&scale=2&maptype=terrain&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=seoul&key=xxx&gt;\n\nggmap(getmap)\n\n\n\n\n\nstr(crime)\n\n'data.frame':   86314 obs. of  17 variables:\n $ time    : POSIXct, format: \"2010-01-01 15:00:00\" \"2010-01-01 15:00:00\" ...\n $ date    : chr  \"1/1/2010\" \"1/1/2010\" \"1/1/2010\" \"1/1/2010\" ...\n $ hour    : int  0 0 0 0 0 0 0 0 0 0 ...\n $ premise : chr  \"18A\" \"13R\" \"20R\" \"20R\" ...\n $ offense : Factor w/ 7 levels \"aggravated assault\",..: 4 6 1 1 1 3 3 3 3 3 ...\n $ beat    : chr  \"15E30\" \"13D10\" \"16E20\" \"2A30\" ...\n $ block   : chr  \"9600-9699\" \"4700-4799\" \"5000-5099\" \"1000-1099\" ...\n $ street  : chr  \"marlive\" \"telephone\" \"wickview\" \"ashland\" ...\n $ type    : chr  \"ln\" \"rd\" \"ln\" \"st\" ...\n $ suffix  : chr  \"-\" \"-\" \"-\" \"-\" ...\n $ number  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ month   : Ord.factor w/ 8 levels \"january\"&lt;\"february\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ day     : Ord.factor w/ 7 levels \"monday\"&lt;\"tuesday\"&lt;..: 5 5 5 5 5 5 5 5 5 5 ...\n $ location: chr  \"apartment parking lot\" \"road / street / sidewalk\" \"residence / house\" \"residence / house\" ...\n $ address : chr  \"9650 marlive ln\" \"4750 telephone rd\" \"5050 wickview ln\" \"1050 ashland st\" ...\n $ lon     : num  -95.4 -95.3 -95.5 -95.4 -95.4 ...\n $ lat     : num  29.7 29.7 29.6 29.8 29.7 ...\n\nHoustonmap &lt;- get_map(\"Houston\")\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=Houston&zoom=10&size=640x640&scale=2&maptype=terrain&language=en-EN&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=Houston&key=xxx&gt;\n\nggmap(Houstonmap)\n\n\n\nggmap(Houstonmap) + geom_point(data=crime, aes(x=lon,y=lat), size=0.1, alpha=0.1) #점의크기, 점의 투명도 조절\n\nWarning: Removed 237 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n#지도 확대 & 특정 지역 데이터만 추출하기\nHoustonmap &lt;- get_map(\"Houston\", zoom=14)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=Houston&zoom=14&size=640x640&scale=2&maptype=terrain&language=en-EN&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=Houston&key=xxx&gt;\n\ncrime1 &lt;- crime[(crime$lon &lt; -95.344 & crime$lon &gt; -95.395) & \n                  (crime$lat &lt; 29.783 & crime$lat &gt; 29.738),]\nggmap(Houstonmap) + geom_point(data=crime1, aes(x=lon,y=lat), alpha=0.3)\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\nggmap(Houstonmap) + geom_point(data=crime1, aes(x=lon, y=lat, colour = offense))\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\ncrime2 &lt;- crime1[!duplicated(crime1[,c(\"lon\",\"lat\")]),] #위,경도에 대해 중복되지 않게 하나의 관측치만 선택\ncrime2$offense &lt;- as.character(crime2$offense) #범죄 종류 문자형으로 변경\n\ncrime2$offense[crime2$offense==\"murder\" | crime2$offense==\"rape\"] &lt;- \"4\"\ncrime2$offense[crime2$offense==\"robbery\" | crime2$offense==\"aggravated assault\"] &lt;- \"3\"\ncrime2$offense[crime2$offense==\"burglary\" | crime2$offense==\"auto theft\"] &lt;- \"2\"\ncrime2$offense[crime2$offense==\"theft\"] &lt;- \"1\"\n\ncrime2$offense &lt;-  as.numeric(crime2$offense) #문자형을 숫자로 변경\n\nggmap(Houstonmap) + geom_point(data=crime2, aes(x=lon, y=lat, size = offense),alpha=0.2)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n#범죄 위험도에 따라 점의 크기 및 색깔로 구별\nggmap(Houstonmap) + geom_point(data=crime2, aes(x=lon, y=lat, size = offense, colour=offense),\n                               alpha=0.5) +\n  scale_colour_gradient(low=\"white\", high=\"red\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\ncrime3 &lt;- crime2[crime2$date==\"1/1/2010\",]\ncrime4 &lt;- crime3[!duplicated(crime3[,c(\"hour\")]),]\nggmap(Houstonmap) + geom_point(data=crime3, aes(x=lon, y=lat)) +\n  geom_text(data=crime4, aes(label=street), vjust=1.2) +\n  geom_path(data=crime4, aes(x=lon, y=lat), color=\"red\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_text()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_path()`).\n\n\n\n\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spDataLarge)\nlibrary(stplanr)      # geographic transport data package\n\n\n다음의 패키지를 부착합니다: 'stplanr'\n\n\nThe following object is masked from 'package:ggmap':\n\n    route\n\nlibrary(tmap)         # visualization package (see Chapter 8)\n\nnames(bristol_zones)\n\n[1] \"geo_code\" \"name\"     \"geometry\"\n\nbristol_zones\n\nSimple feature collection with 102 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2.845847 ymin: 51.28248 xmax: -2.252388 ymax: 51.73982\nGeodetic CRS:  WGS 84\nFirst 10 features:\n      geo_code                             name                       geometry\n2905 E02002985 Bath and North East Somerset 001 MULTIPOLYGON (((-2.510462 5...\n2907 E02002987 Bath and North East Somerset 003 MULTIPOLYGON (((-2.476122 5...\n2925 E02003005 Bath and North East Somerset 021 MULTIPOLYGON (((-2.55073 51...\n2932 E02003012                      Bristol 001 MULTIPOLYGON (((-2.595763 5...\n2933 E02003013                      Bristol 002 MULTIPOLYGON (((-2.593783 5...\n2934 E02003014                      Bristol 003 MULTIPOLYGON (((-2.639581 5...\n2935 E02003015                      Bristol 004 MULTIPOLYGON (((-2.584973 5...\n2936 E02003016                      Bristol 005 MULTIPOLYGON (((-2.565948 5...\n2937 E02003017                      Bristol 006 MULTIPOLYGON (((-2.616485 5...\n2938 E02003018                      Bristol 007 MULTIPOLYGON (((-2.637681 5...\n\nnrow(bristol_od)\n\n[1] 2910\n\nbristol_od\n\n# A tibble: 2,910 × 7\n   o         d           all bicycle  foot car_driver train\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 E02002985 E02002985   209       5   127         59     0\n 2 E02002985 E02002987   121       7    35         62     0\n 3 E02002985 E02003036    32       2     1         10     1\n 4 E02002985 E02003043   141       1     2         56    17\n 5 E02002985 E02003049    56       2     4         36     0\n 6 E02002985 E02003054    42       4     0         21     0\n 7 E02002985 E02003100    22       0     0         19     3\n 8 E02002985 E02003106    48       3     1         33     8\n 9 E02002985 E02003108    31       0     0         29     1\n10 E02002985 E02003121    42       1     2         34     0\n# ℹ 2,900 more rows\n\nnrow(bristol_zones)\n\n[1] 102\n\n#bristol_zones 지역별로 출발지 'o'로 그룹화 하여 이동량 계산\nzones_attr = bristol_od %&gt;% \n  group_by(o) %&gt;% \n  summarize_if(is.numeric, sum) %&gt;% \n  dplyr::rename(geo_code = o)\n\nsummary(zones_attr$geo_code %in% bristol_zones$geo_code)\n\n   Mode    TRUE \nlogical     102 \n\n\n\n#bristol_zones과 zones_attr를  geo_code를 기준으로 합치기\n\nzones_joined = left_join(bristol_zones, zones_attr, by = \"geo_code\")\nsum(zones_joined$all)\n\n[1] 238805\n\nnames(zones_joined)\n\n[1] \"geo_code\"   \"name\"       \"all\"        \"bicycle\"    \"foot\"      \n[6] \"car_driver\" \"train\"      \"geometry\"  \n\nnames(zones_joined)[3] &lt;- c(\"all_orig\")\nnames(zones_joined)\n\n[1] \"geo_code\"   \"name\"       \"all_orig\"   \"bicycle\"    \"foot\"      \n[6] \"car_driver\" \"train\"      \"geometry\"  \n\n\n\n# 목적지가 geo_code인 데이터 group화하여 이동량 계산, 그리고 zones_joined에 추가하기\nzones_od = bristol_od %&gt;% \n  group_by(d) %&gt;% \n  summarize_if(is.numeric, sum) %&gt;% \n  dplyr::select(geo_code = d, all_dest = all) %&gt;% \n  inner_join(zones_joined, ., by = \"geo_code\")\n\n\nqtm(zones_od, c(\"all_orig\", \"all_dest\")) +\n  tm_layout(panel.labels = c(\"Origin\", \"Destination\"))\n\n\n\n\n\nod_top5 = bristol_od %&gt;% \n  arrange(desc(all)) %&gt;% \n  top_n(5, wt = all)\n\n#활동성(전체 이동수단인원에서 자전거or 도보로 이동한 인원)\nbristol_od$Active = (bristol_od$bicycle + bristol_od$foot) /\n  bristol_od$all * 100\n\n\nod_intra = filter(bristol_od, o == d) #o와 d가 같을때\nod_inter = filter(bristol_od, o != d) #o와 d가 다를때\n\n\ndesire_lines = od2line(od_inter, zones_od)\n\nCreating centroids representing desire line start and end points.\n\n#&gt; Creating centroids representing desire line start and end points.\nqtm(desire_lines, lines.lwd = \"all\")\n\nLegend labels were too wide. Therefore, legend.text.size has been set to 0.61. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ndesire_lines_top5 = od2line(od_top5, zones_od)\n\nCreating centroids representing desire line start and end points.\n\n# tmaptools::palette_explorer()\n\n\ntm_shape(desire_lines) +\n  tm_lines(palette = \"plasma\", \n           breaks = c(0, 5, 10, 20, 40, 100),\n           lwd = \"all\",\n           scale = 9,\n           title.lwd = \"Number of trips\",\n           alpha = 0.6,\n           col = \"Active\",\n           title = \"Active travel (%)\") +\n  tm_shape(desire_lines_top5) +\n  tm_lines(lwd = 5, col = \"black\", alpha = 0.7) +\n  tm_scale_bar()\n\nLegend labels were too wide. Therefore, legend.text.size has been set to 0.61. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\n\ndesire_lines$distance = as.numeric(st_length(desire_lines))\n\ndesire_carshort = dplyr::filter(desire_lines, car_driver &gt; 300 & distance &lt; 5000)\n\nroute_carshort = route(l = desire_carshort, route_fun = route_osrm)\n\nMost common output is sf\n\ndesire_carshort$geom_car = st_geometry(route_carshort)\n\n\nplot(st_geometry(desire_carshort))\nplot(desire_carshort$geom_car, col = \"red\", add = TRUE)\nplot(st_geometry(st_centroid(zones_od)), add = TRUE)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\n\n\n\n\ngetmap &lt;- get_googlemap(\"bristol\", zoom = 11)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=bristol&zoom=11&size=640x640&scale=2&maptype=terrain&key=xxx&gt;\n\n\nℹ &lt;https://maps.googleapis.com/maps/api/geocode/json?address=bristol&key=xxx&gt;\n\nbristol_map &lt;- ggmap(getmap)\n#센터 조정\ngetmap &lt;- get_googlemap(center = c(-2.56, 51.53), zoom = 12)\n\nℹ &lt;https://maps.googleapis.com/maps/api/staticmap?center=51.53,-2.56&zoom=12&size=640x640&scale=2&maptype=terrain&key=xxx&gt;\n\nbristol_map &lt;- ggmap(getmap)\n\nbristol_map + \n  geom_sf(data = desire_carshort, inherit.aes = F) + \n  geom_sf(data = desire_carshort$geom_car, inherit.aes = F, col=\"red\") +\n  geom_sf(data = st_geometry(st_centroid(zones_od)), inherit.aes = F)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\n\n\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap(\"OpenStreetMap\")+\n  tm_shape(desire_carshort$geometry)+\n  tm_lines()+\n  tm_shape(desire_carshort$geom_car)+\n  tm_lines(col = \"red\",lwd = 0.5)+\n  tm_shape(st_geometry(st_centroid(zones_od)))+\n  tm_dots()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019\n\nWarning in CPL_transform(x, crs, aoi, pipeline, reverse, desired_accuracy, :\nGDAL Error 1: PROJ: proj_as_wkt: DatumEnsemble can only be exported to\nWKT2:2019"
  },
  {
    "objectID": "posts/SQL.html",
    "href": "posts/SQL.html",
    "title": "SQL",
    "section": "",
    "text": "rm(list=ls())\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DBI)\nlibrary(RSQLite)\nlibrary(dbplyr)\n\n\n다음의 패키지를 부착합니다: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\n\n\nus_census&lt;-read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/drive-download-20230405T011215Z-001/US_census.csv\")\n\nRows: 3143 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): state, name\ndbl (51): FIPS, pop2010, pop2000, age_under_5, age_under_18, age_over_65, fe...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#us_census %&gt;% View()\n\n\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \n                      dbname = \"testsqlite\")\ncopy_to(dest = con, \n        df = us_census, \n        name = \"us_census\")\n\ncopy_to(dest = con,\n        df = mtcars,\n        name = \"mtcars\")\n\n\ndbListTables(con)\n\n[1] \"mtcars\"       \"sqlite_stat1\" \"sqlite_stat4\" \"us_census\"   \n\n\n\nSELECT * FROM us_census\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\nname\nFIPS\npop2010\npop2000\nage_under_5\nage_under_18\nage_over_65\nfemale\nwhite\nblack\nnative\nasian\npac_isl\ntwo_plus_races\nhispanic\nwhite_not_hispanic\nno_move_in_one_plus_year\nforeign_born\nforeign_spoken_at_home\nhs_grad\nbachelors\nveterans\nmean_work_travel\nhousing_units\nhome_ownership\nhousing_multi_unit\nmedian_val_owner_occupied\nhouseholds\npersons_per_household\nper_capita_income\nmedian_household_income\npoverty\nprivate_nonfarm_establishments\nprivate_nonfarm_employment\npercent_change_private_nonfarm_employment\nnonemployment_establishments\nfirms\nblack_owned_firms\nnative_owned_firms\nasian_owned_firms\npac_isl_owned_firms\nhispanic_owned_firms\nwomen_owned_firms\nmanufacturer_shipments_2007\nmercent_whole_sales_2007\nsales\nsales_per_capita\naccommodation_food_service\nbuilding_permits\nfed_spending\narea\ndensity\n\n\n\n\nAlabama\nAutauga County\n1001\n54571\n43671\n6.6\n26.8\n12.0\n51.3\n78.5\n17.7\n0.4\n0.9\nNA\n1.6\n2.4\n77.2\n86.3\n2.0\n3.7\n85.3\n21.7\n5817\n25.1\n22135\n77.5\n7.2\n133900\n19718\n2.70\n24568\n53255\n10.6\n877\n10628\n16.6\n2971\n4067\n15.2\nNA\n1.3\nNA\n0.7\n31.7\nNA\nNA\n598175\n12003\n88157\n191\n331142\n594.44\n91.8\n\n\nAlabama\nBaldwin County\n1003\n182265\n140415\n6.1\n23.0\n16.8\n51.1\n85.7\n9.4\n0.7\n0.7\nNA\n1.5\n4.4\n83.5\n83.0\n3.6\n5.5\n87.6\n26.8\n20396\n25.8\n104061\n76.7\n22.6\n177200\n69476\n2.50\n26469\n50147\n12.2\n4812\n52233\n17.4\n14175\n19035\n2.7\n0.4\n1.0\nNA\n1.3\n27.3\n1410273\nNA\n2966489\n17166\n436955\n696\n1119082\n1589.78\n114.6\n\n\nAlabama\nBarbour County\n1005\n27457\n29038\n6.2\n21.9\n14.2\n46.9\n48.0\n46.9\n0.4\n0.4\nNA\n0.9\n5.1\n46.8\n83.0\n2.8\n4.7\n71.9\n13.5\n2327\n23.8\n11829\n68.0\n11.1\n88200\n9795\n2.52\n15875\n33219\n25.0\n522\n7990\n-27.0\n1527\n1667\nNA\nNA\nNA\nNA\nNA\n27.0\nNA\nNA\n188337\n6334\nNA\n10\n240308\n884.88\n31.0\n\n\nAlabama\nBibb County\n1007\n22915\n20826\n6.0\n22.7\n12.7\n46.3\n75.8\n22.0\n0.3\n0.1\nNA\n0.9\n1.8\n75.0\n90.5\n0.7\n1.5\n74.5\n10.0\n1883\n28.3\n8981\n82.9\n6.6\n81200\n7441\n3.02\n19918\n41770\n12.6\n318\n2927\n-14.0\n1192\n1385\n14.9\nNA\nNA\nNA\nNA\nNA\n0\nNA\n124707\n5804\n10757\n8\n163201\n622.58\n36.8\n\n\nAlabama\nBlount County\n1009\n57322\n51024\n6.3\n24.6\n14.7\n50.5\n92.6\n1.3\n0.5\n0.2\nNA\n1.2\n8.1\n88.9\n87.2\n4.7\n7.2\n74.7\n12.5\n4072\n33.2\n23887\n82.0\n3.7\n113700\n20605\n2.73\n21070\n45549\n13.4\n749\n6968\n-11.4\n3501\n4458\nNA\nNA\nNA\nNA\nNA\n23.2\n341544\nNA\n319700\n5622\n20941\n18\n294114\n644.78\n88.9\n\n\nAlabama\nBullock County\n1011\n10914\n11714\n6.8\n22.3\n13.5\n45.8\n23.0\n70.2\n0.2\n0.2\nNA\n0.8\n7.1\n21.9\n88.5\n1.1\n3.8\n74.7\n12.0\n943\n28.1\n4493\n76.9\n9.9\n66300\n3732\n2.85\n20289\n31602\n25.3\n120\n1919\n-18.5\n390\n417\nNA\nNA\nNA\nNA\nNA\n38.8\nNA\nNA\n43810\n3995\n3670\n1\n108846\n622.81\n17.5\n\n\nAlabama\nButler County\n1013\n20947\n21399\n6.5\n24.1\n16.7\n53.0\n54.4\n43.4\n0.3\n0.8\n0.0\n0.8\n0.9\n54.1\n92.8\n1.1\n1.6\n74.8\n11.0\n1675\n25.1\n9964\n69.0\n13.7\n70200\n8019\n2.58\n16916\n30659\n25.0\n446\n5400\n2.1\n1180\n1769\nNA\nNA\n3.3\nNA\nNA\nNA\n399132\n56712\n229277\n11326\n28427\n3\n195055\n776.83\n27.0\n\n\nAlabama\nCalhoun County\n1015\n118572\n112249\n6.1\n22.9\n14.3\n51.8\n74.9\n20.6\n0.5\n0.7\n0.1\n1.7\n3.3\n73.6\n82.9\n2.5\n4.5\n78.5\n16.1\n11757\n22.1\n53289\n70.7\n14.3\n98200\n46421\n2.46\n20574\n38407\n19.5\n2444\n38324\n-5.6\n6329\n8713\n7.2\nNA\n1.6\nNA\n0.5\n24.7\n2679991\nNA\n1542981\n13678\n186533\n107\n1830659\n605.87\n195.7\n\n\nAlabama\nChambers County\n1017\n34215\n36583\n5.7\n22.5\n16.7\n52.2\n58.8\n38.7\n0.2\n0.5\n0.0\n1.1\n1.6\n58.1\n86.2\n0.9\n1.6\n71.8\n10.8\n2893\n23.6\n17004\n71.4\n8.7\n82200\n13681\n2.51\n16626\n31467\n20.3\n568\n6241\n-45.8\n2074\n1981\nNA\nNA\nNA\nNA\nNA\n29.3\n667283\nNA\n264650\n7620\n23237\n10\n294718\n596.53\n57.4\n\n\nAlabama\nCherokee County\n1019\n25989\n23988\n5.3\n21.4\n17.9\n50.4\n92.7\n4.6\n0.5\n0.2\n0.0\n1.5\n1.2\n92.1\n88.1\n0.5\n1.4\n73.4\n10.5\n2172\n26.2\n16267\n77.5\n4.3\n97100\n11352\n2.22\n21322\n40690\n17.6\n350\n3600\n5.4\n1627\n2180\nNA\nNA\nNA\nNA\nNA\n14.5\n307439\n62293\n186321\n7613\n13948\n6\n184642\n553.70\n46.9\n\n\n\n\n\n\nselect * from mtcars\n\n\nDisplaying records 1 - 10\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4"
  },
  {
    "objectID": "posts/detect_line.html",
    "href": "posts/detect_line.html",
    "title": "Traffic line detection using CV2",
    "section": "",
    "text": "1.주제 선정\n주제는 ’CV2 활용 차선 탐지’로 최근 자율주행 자동차의 눈이 되는 부분을 이미지분석 수업과정으로 배운 CV2를 활용해 구현할 수 있어 선정하였다.\n\n\n2.이미지 로드 전처리\n프로젝트에 사용한 이미지는 인천대교를 건너는 자동차의 블랙박스 영상이다.\n\n2-1. 원근변환\n\n적색 네 점을 위에서 본 시점으로 변환한다.\ngetPerspectiveTransform함수와, warpPerspective함수를 사용해 원근 변환된 이미지와, 다시 원상복귀시키기 위한 getPerspectiveTransform함수를 역으로 적용시킨 값을 출력한다.\n\n\n2-2 색 범위 탐색\n\n이미지에서 흰색 차선이 있는 곳을 찾기 위해 이미지를 HSV(색상,채도,명도) 형식으로 변환 후 흰색 구간에 부합하는 값을 출력한다.\n\n\n2-3. 관심지역 설정\n\n색 구간으로 흰색을 탐지한 결과 좌상단에 노이즈가 있는 것을 볼 수 있다. 노이즈를 없애기 위해 차선이 있는 구간을 관심지역으로 정해 관심지역 안에서만 탐지하게 한다.\n원본 이미지와 해상도가 동일한 0으로 이루어진 1차원 이미지에 fillpoly함수를 사용하여 관심구간 좌표를 흰색으로 칠한다. 후에 bitwise_and 연산을 이용해 겹치는 구간만을 출력한다.\n\n\n2-4. 흑백화, threshold 연산\n185를 기준으로 threshold 연산을 한 이미지를 출력한다.\n\n\n\n3.탐지구간 분할\n\n3-1. 차선 히스토그램\n전처리 된 1차원 이미지를 행 기준으로 더하여 출력된 리스트를 히스토그램으로 그린후, 히스토그램의 좌측과 우측에서의 최댓값을 가지는 점을 출력한다.\n\n예시 사진에서는 (270,1082) 점에서 최댓값을 가졌다.\n\n\n3-2. 구간 분할 및 탐색\n각 차선을 n개의 구간으로 나누어 좌,우로 125의 마진을 갖는 상자를 그린다. 동영상의 각 프레임에서 상자안에 드는 차선들의 평균값을 리턴한다. 리턴된 값에 Polyfit연산을 사용해 차선의 예측선을 출력한다.\n\n\n\n\n4. 결과 도출\n차선의 예측선 사이를 fillpoly함수를 통해 칠한후, 2-1에서 출력한 원근변환을 이용해 원본 크기로 돌린다. 투명도를 위해 원본 동영상에 addWeighted 연산을 통해 결과 동영상을 출력한다.\n\n\n\n5. 응용 방안\n\n차선의 곡률, 이탈률을 계산하여 차선이탈 경고시스템에 적용 가능하다. 더 나아가 고속도로처럼 차선이 명확한 부분에서는 간단한 자율주행기능에도 적용할 수 있다.\n유사한 방식으로 주차선을 탐지한다면 주차보조 시스템에적용할 수 있다.\n\n\n\n6. 보완해야 할 점\n\n커브길을 진입할 경우에 2-3에서 지정한 관심지역 밖으로 차선이 나가는 경우에는 탐지를 하지 못한다. \n원근변환, 관심지역의 값을 변경해 보완해야 함\n\n\n\n\n그림자가 있거나 갑자기 밝아지는 상황에서 탐지율이 떨어진다. 주변의 밝기를 기준으로 threshold연산을 유동적으로 하여 보완할 필요가 있다.\n\n코드:\nhttps://drive.google.com/drive/folders/1Jfc9HOaVIaTQ48MsyXwG79ePtQIfflnl?usp=sharing\n소스 출처: https://github.com/sidroopdaska/SelfDrivingCar/tree/master/AdvancedLaneLinesDetection\n영상 출처:\nhttps://youtu.be/aItuTJYMj28"
  },
  {
    "objectID": "posts/datamining_report.html",
    "href": "posts/datamining_report.html",
    "title": "천안시 아파트 매매가 분석",
    "section": "",
    "text": "인구 감소, 부모님 세대의 은퇴로 인한 주택 가격 변동\n주택 가격 변동에 따른 예측의 중요성\n\n\n\n\n\n아파트 가격에 변동을 주는 요인 분석\n아파트 가격 예측 모델 제작\n데이터 수집의 다양한 방법 학습"
  },
  {
    "objectID": "posts/datamining_report.html#배경-목적",
    "href": "posts/datamining_report.html#배경-목적",
    "title": "천안시 아파트 매매가 분석",
    "section": "",
    "text": "인구 감소, 부모님 세대의 은퇴로 인한 주택 가격 변동\n주택 가격 변동에 따른 예측의 중요성\n\n\n\n\n\n아파트 가격에 변동을 주는 요인 분석\n아파트 가격 예측 모델 제작\n데이터 수집의 다양한 방법 학습"
  },
  {
    "objectID": "posts/datamining_report.html#데이터",
    "href": "posts/datamining_report.html#데이터",
    "title": "천안시 아파트 매매가 분석",
    "section": "데이터",
    "text": "데이터\n\n메인 데이터\n데이터는 국토교통부 아파트 매매 실거래가정보 API를 활용하여 PublicDataReader을 통해 불러온 2018년 3월 부터 2023년 4월 까지의 천안시 아파트 거래내역 62614 건을 사용한다.\n천안시_아파트매매_실거래_2018_2023\n\n!pip install PublicDataReader\n\nimport PublicDataReader as pdr\nimport pandas as pd\npdr.__version__\n\nservice_key = \"-my api-\"\n\nfrom PublicDataReader import TransactionPrice\napi = TransactionPrice(service_key)\n\nimport PublicDataReader as pdr\nsigungu_name = \"천안시\"\ncode = pdr.code_bdong()\ncode.loc[(code['시군구명'].str.contains(sigungu_name)) &\n         (code['읍면동명']=='')]\n\ncheonan_seobuk = api.get_data(\n    property_type=\"아파트\",\n    trade_type=\"매매\",\n    sigungu_code=\"44133\",\n    start_year_month=\"201803\",\n    end_year_month=\"202304\",\n    )\n    \ncheonan_dongnam = api.get_data(\n    property_type=\"아파트\",\n    trade_type=\"매매\",\n    sigungu_code=\"44131\",\n    start_year_month=\"201803\",\n    end_year_month=\"202304\",\n    )\n\ncheonan = pd.concat([cheonan_dongnam,cheonan_seobuk])\n\n\n\n학교 데이터\n학군 정보를 수집하기 위해 천안시청 학교현황을 통해 얻어낸 학교데이터를 구글 스프레드시트 지오코딩을 통해 지리데이터로 변환하였다.\n초등학교\n중학교\n고등학교\n\n\n부동산 정보 데이터\n아파트의 주차장, 용적률, 건폐율등 부동산 정보 수집을 위해 네이버 부동산 웹크롤링 하였다.\n참고자료\n\nimport pandas as pd\nimport requests\nimport json\n\n시도코드의 수집 과정\n\ndef get_sido_info():\n    down_url = 'https://new.land.naver.com/api/regions/list?cortarNo=0000000000'\n    r = requests.get(down_url,data={\"sameAddressGroup\":\"false\"},headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/...\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n    })\n    r.encoding = \"utf-8-sig\"\n    temp=json.loads(r.text)\n    temp=list(pd.DataFrame(temp[\"regionList\"])[\"cortarNo\"])\n    return temp\ndef get_gungu_info(sido_code):\n    down_url = 'https://new.land.naver.com/api/regions/list?cortarNo='+sido_code\n    r = requests.get(down_url,data={\"sameAddressGroup\":\"false\"},headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/...\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n    })\n    r.encoding = \"utf-8-sig\"\n    temp=json.loads(r.text)\n    temp=list(pd.DataFrame(temp['regionList'])[\"cortarNo\"])\n    return temp\ndef get_dong_info(gungu_code):\n    down_url = 'https://new.land.naver.com/api/regions/list?cortarNo='+gungu_code\n    r = requests.get(down_url,data={\"sameAddressGroup\":\"false\"},headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/...\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n    })\n    r.encoding = \"utf-8-sig\"\n    temp=json.loads(r.text)\n    temp=list(pd.DataFrame(temp['regionList'])[\"cortarNo\"])\n    return temp\ndef get_apt_list(dong_code):\n    down_url = 'https://new.land.naver.com/api/regions/complexes?cortarNo='+dong_code+'&realEstateType=APT&order='\n    r = requests.get(down_url,data={\"sameAddressGroup\":\"false\"},headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/...\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n    })\n    r.encoding = \"utf-8-sig\"\n    temp=json.loads(r.text)\n    try:\n        temp=list(pd.DataFrame(temp['complexList'])[\"complexNo\"])\n    except:\n        temp=[]\n    return temp\n\n\nsido_list=get_sido_info() \ngungu_list=get_gungu_info(sido_list[0])\ndong_list=get_dong_info(gungu_list[0])\nget_apt_list(dong_list[0])[0]\n\n\ndef get_apt_info(apt_code):\n    down_url = 'https://new.land.naver.com/api/complexes/'+apt_code+'?sameAddressGroup=false'\n    r = requests.get(down_url,data={\"sameAddressGroup\":\"false\"},headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/complexes/\"+apt_code+\"?ms=37.482968,127.0634,16&a=APT&b=A1&e=RETAIL\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"})\n    r.encoding = \"utf-8-sig\"\n    temp=json.loads(r.text)\n    return temp\n\n\ndef get_school_info(apt_code):\n    down_url = 'https://new.land.naver.com/api/complexes/'+apt_code+'/schools'\n    r = requests.get(down_url,headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/complexes/\"+apt_code+\"?ms=37.482968,127.0634,16&a=APT&b=A1&e=RETAIL\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"})\n    r.encoding = \"utf-8-sig\"\n    temp_school=json.loads(r.text)\n    return temp_school\n\n##################가격정보\ndef apt_price(apt_code,index):\n    p_num=temp[\"complexPyeongDetailList\"][index][\"pyeongNo\"]\n    down_url = 'https://new.land.naver.com/api/complexes/'+apt_code+'/prices?complexNo='+apt_code+'&tradeType=A1&year=5&priceChartChange=true&areaNo='+p_num+'&areaChange=true&type=table'\n\n    r = requests.get(down_url,headers={\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE2NTk5MzcxNTIsImV4cCI6MTY1OTk0Nzk1Mn0.PD7SqZO7z8f97uGQpfSKYMPbrLy6YtRl9XYHWaHiVVE\",\n        \"Host\": \"new.land.naver.com\",\n        \"Referer\": \"https://new.land.naver.com/complexes/\"+apt_code+\"?ms=37.482968,127.0634,16&a=APT&b=A1&e=RETAIL\",\n        \"sec-ch-ua\": \"\\\".Not\\/A)Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"103\\\", \\\"Chromium\\\";v=\\\"103\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"macOS\",\n        \"Sec-Fetch-Dest\": \"empty\",\n        \"Sec-Fetch-Mode\": \"cors\",\n        \"Sec-Fetch-Site\": \"same-origin\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"})\n    r.encoding = \"utf-8-sig\"\n    temp_price=json.loads(r.text)\n    return temp_price\n\n\nsido_list=['4400000000']\ngungu_list=get_gungu_info(sido_list[0])\ngungu_list\n\n# '4413100000'  동남구\n# '4413300000'  서북구\n\n\nsido_list=['4400000000']\nfor m in range(len(sido_list)):\n    gungu_list=['4413300000']\n    gungu_apt_list=[0]*len(gungu_list)\n    for j in range(len(gungu_list)):#구 마다 하나씩 저장\n        dong_list=get_dong_info(gungu_list[j])\n        dong_apt_list=[0]*len(dong_list)\n        for k in range(len(dong_list)):#동마다 하나씩 저장\n            apt_list=get_apt_list(dong_list[k])\n            apt_list_data=[0]*len(apt_list)\n            for n in range(len(apt_list)):#아파트 마다 하나씩 저장\n                temp=get_apt_info(apt_list[n])\n                try:\n                    area_list=temp[\"complexDetail\"][\"pyoengNames\"].split(\", \")\n                    ex_flag=1\n                except KeyError:   \n                    ex_flag=0\n                    temp_data=pd.DataFrame(columns=temp_data.columns)\n                if ex_flag==1:\n                    temp_school=get_school_info(apt_list[n])\n                    temp_data=pd.DataFrame(index=range(len(area_list)))\n                    for i in range(len(area_list)):\n                        print(temp[\"complexDetail\"][\"address\"],temp[\"complexDetail\"][\"complexName\"])\n                        temp_data.loc[i,\"아파트명\"]=temp[\"complexDetail\"][\"complexName\"]\n                        temp_data.loc[i,\"면적\"]=area_list[i]\n                        temp_data.loc[i,\"법정동주소\"]=temp[\"complexDetail\"][\"address\"]+\" \"+temp[\"complexDetail\"][\"detailAddress\"]\n                        try:\n                            temp_data.loc[i,\"도로명주소\"]=temp[\"complexDetail\"][\"roadAddressPrefix\"]+\" \"+temp[\"complexDetail\"][\"roadAddress\"]\n                        except KeyError:\n                            temp_data.loc[i,\"도로명주소\"]=temp[\"complexDetail\"][\"roadAddressPrefix\"]\n                        temp_data.loc[i,\"latitude\"]=temp[\"complexDetail\"][\"latitude\"]\n                        temp_data.loc[i,\"longitude\"]=temp[\"complexDetail\"][\"longitude\"]\n                        temp_data.loc[i,\"세대수\"]=temp[\"complexDetail\"][\"totalHouseholdCount\"]\n                        temp_data.loc[i,\"임대세대수\"]=temp[\"complexDetail\"][\"totalLeaseHouseholdCount\"]\n                        temp_data.loc[i,\"최고층\"]=temp[\"complexDetail\"][\"highFloor\"]\n                        temp_data.loc[i,\"최저층\"]=temp[\"complexDetail\"][\"lowFloor\"]\n                        try:\n                            temp_data.loc[i,\"용적률\"]=temp[\"complexDetail\"][\"batlRatio\"]\n                        except KeyError:\n                            temp_data.loc[i,\"용적률\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"건폐율\"]=temp[\"complexDetail\"][\"btlRatio\"]\n                        except KeyError:\n                            temp_data.loc[i,\"건폐율\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"주차대수\"]=temp[\"complexDetail\"][\"parkingPossibleCount\"]\n                        except KeyError:\n                            temp_data.loc[i,\"주차대수\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"건설사\"]=temp[\"complexDetail\"][\"constructionCompanyName\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"건설사\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"난방\"]=temp[\"complexDetail\"][\"heatMethodTypeCode\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"난방\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"공급면적\"]=temp[\"complexPyeongDetailList\"][i][\"supplyArea\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"공급면적\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"전용면적\"]=temp[\"complexPyeongDetailList\"][i][\"exclusiveArea\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"전용면적\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"전용율\"]=temp[\"complexPyeongDetailList\"][i][\"exclusiveRate\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"전용율\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"방수\"]=temp[\"complexPyeongDetailList\"][i][\"roomCnt\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"방수\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"욕실수\"]=temp[\"complexPyeongDetailList\"][i][\"bathroomCnt\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"욕실수\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"해당면적_세대수\"]=temp[\"complexPyeongDetailList\"][i][\"householdCountByPyeong\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"해당면적_세대수\"]=\"\"\n                        try:\n                            temp_data.loc[i,\"현관구조\"]=temp[\"complexPyeongDetailList\"][i][\"entranceType\"]\n                        except KeyError:   \n                            temp_data.loc[i,\"현관구조\"]=\"\"\n                        \n\n                    #time.sleep(1)\n                apt_list_data[n]=temp_data\n            if apt_list_data==[]:\n                dong_apt_list[k]=pd.DataFrame(columns=temp_data.columns)\n            else:\n                dong_apt_list[k]=pd.concat(apt_list_data)\n        gungu_apt_list[j]=pd.concat(dong_apt_list)\n        gungu_apt_list[j].to_csv(temp[\"complexDetail\"][\"roadAddressPrefix\"]+\".csv\",encoding=\"CP949\")\n    final_data=pd.concat(gungu_apt_list)\n    final_data.to_csv(temp[\"complexDetail\"][\"roadAddressPrefix\"].split()[0]+\".csv\",encoding=\"CP949\")"
  },
  {
    "objectID": "posts/datamining_report.html#데이터-병합",
    "href": "posts/datamining_report.html#데이터-병합",
    "title": "천안시 아파트 매매가 분석",
    "section": "데이터 병합",
    "text": "데이터 병합\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport folium\nfrom folium import Marker,GeoJson,Choropleth,Circle\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom geopy.geocoders import Nominatim\n\n\n데이터 단위 변경\n\ndata_path = \"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%B2%9C%EC%95%88%EC%8B%9C_%EC%95%84%ED%8C%8C%ED%8A%B8_%EB%A7%A4%EB%A7%A4_2018_2023.csv\"\ndata_raw = pd.read_csv(data_path,index_col=0)\n\n\ndata_use = data_raw.iloc[:,2:12]\ndata_use[\"전용면적_평\"] = data_use[\"전용면적\"]*0.3025\ndata_use[\"평당거래액_만원\"] = data_use[\"거래금액\"]/data_use[\"전용면적_평\"]\n\ndata_use.head()\n\n   법정동    지번      아파트  건축년도   층  ...  월   일   거래금액     전용면적_평     평당거래액_만원\n0  와촌동   173  신동아파밀리에  2005   6  ...  3  28  22000  25.706934   855.800229\n1  성정동   823       선경  1994   4  ...  3   1  10950  18.104625   604.817830\n2  성정동   892  건일(892)  1995   2  ...  3   1   8000  18.083450   442.393459\n3  성정동   785    주공6-1  1989   1  ...  3   5  10800  14.220525   759.465632\n4  성정동  1292    에프엠21  2014  13  ...  3   5  12874   8.411255  1530.568359\n\n[5 rows x 12 columns]\n\n\n\ndata_use.describe()\n\n               건축년도             층  ...        전용면적_평      평당거래액_만원\ncount  62614.000000  62614.000000  ...  62614.000000  62614.000000\nmean    2004.367218      9.122145  ...     21.393032    949.621551\nstd        8.906689      6.227433  ...      8.056254    504.430930\nmin     1981.000000      1.000000  ...      4.062575     92.989736\n25%     1997.000000      4.000000  ...     15.728366    623.430582\n50%     2004.000000      8.000000  ...     21.438175    829.265785\n75%     2013.000000     13.000000  ...     25.697375   1145.525500\nmax     2023.000000     62.000000  ...     80.027282   4550.931812\n\n[8 rows x 9 columns]\n\n\n\n\n아파트별 데이터\n아파트별 특징 추가를 위해 실거래가 데이터에서 아파트명, 주소, 위경도 데이터만 추출 (440개 아파트)\n\napt_path = \"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/apartment_latlon.csv\"\n\napt_raw = pd.read_csv(apt_path)\napt_latlon = apt_raw.iloc[:,2:5]\n\napt_latlon_gdf = gpd.GeoDataFrame(apt_latlon, geometry=gpd.points_from_xy(apt_latlon['lon'], apt_latlon['lat']))\napt_latlon_gdf.head()\n\n          lon        lat      아파트                    geometry\n0  127.144504  36.807438  신동아파밀리에  POINT (127.14450 36.80744)\n1  127.133597  36.822074       선경  POINT (127.13360 36.82207)\n2  127.132594  36.821036  건일(892)  POINT (127.13259 36.82104)\n3  127.134030  36.825454    주공6-1  POINT (127.13403 36.82545)\n4  127.137996  36.828372    에프엠21  POINT (127.13800 36.82837)\n\n\n\ndata_join = pd.merge(data_use,apt_latlon,on=\"아파트\",how=\"left\")\n\n\nm = folium.Map(location=[36.807438,127.144504], zoom_start=13)\n\nfor idx, row in apt_latlon_gdf.iterrows():\n    Marker(location = [row['lat'], row['lon']],\n           popup=row[\"아파트\"]).add_to(m)\n\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n학군 정보\n아파트별로 1000m 내의 초,중,고등학교의 개수를 추가\n\nelm_school = pd.read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%B4%88%EB%93%B1%ED%95%99%EA%B5%90%20%EC%9C%84%EA%B2%BD%EB%8F%84.csv\")\nmid_school = pd.read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%A4%91%ED%95%99%EA%B5%90_%EC%9C%84%EA%B2%BD%EB%8F%84.csv\")\nhi_school = pd.read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EA%B3%A0%EB%93%B1%ED%95%99%EA%B5%90_%EC%9C%84%EA%B2%BD%EB%8F%84.csv\")\n\nelm_school = gpd.GeoDataFrame(elm_school, geometry=gpd.points_from_xy(elm_school['Longitude'], elm_school['Latitude']))\nmid_school = gpd.GeoDataFrame(mid_school, geometry=gpd.points_from_xy(mid_school['Longitude'], mid_school['Latitude']))\nhi_school = gpd.GeoDataFrame(hi_school, geometry=gpd.points_from_xy(hi_school['Longitude'], hi_school['Latitude']))\n\nelm_school.crs = 'EPSG:4326'\nmid_school.crs = 'EPSG:4326'\nhi_school.crs = 'EPSG:4326'\n\nelm_school = elm_school.to_crs('EPSG:3857')\nmid_school = mid_school.to_crs('EPSG:3857')\nhi_school = hi_school.to_crs('EPSG:3857')\n\nhi_school.head()\n\n         고등학교  ...                          geometry\n0    북일여자고등학교  ...  POINT (14155185.117 4415544.888)\n1      북일고등학교  ...  POINT (14155185.117 4415544.888)\n2  천안여자상업고등학교  ...  POINT (14152541.446 4411770.809)\n3    복자여자고등학교  ...  POINT (14154410.433 4413396.960)\n4    천안상업고등학교  ...  POINT (14155084.974 4419483.310)\n\n[5 rows x 5 columns]\n\n\n\nm_1 = folium.Map(location=[36.807438,127.144504], zoom_start=13)\n\nfor idx, row in elm_school.iterrows():\n    Circle(location = [row['Latitude'], row['Longitude']],\n           radius=40,\n           tooltip=row[\"초등학교\"],\n           color = \"red\").add_to(m_1)\n\nfor idx, row in mid_school.iterrows():\n    Circle(location = [row['Latitude'], row['Longitude']],\n           radius=50,\n           tooltip=row[\"중학\"],\n           color = \"blue\").add_to(m_1)\n    \nfor idx, row in hi_school.iterrows():\n    Circle(location = [row['Latitude'], row['Longitude']],\n           radius=60,\n           tooltip=row[\"고등학교\"],\n           color = \"green\").add_to(m_1)\n\n\nm_1\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nred = 초등학교, blue = 중학교, green = 고등학교\n\napt_latlon_gdf.crs = 'EPSG:4326'\napt_latlon_gdf_3857 = apt_latlon_gdf.to_crs('EPSG:3857')\n\n\nbuffer_3 = apt_latlon_gdf_3857.geometry.buffer(1000)\n\n\nm_2 = folium.Map(location=[36.807438,127.144504], zoom_start=13)\n\nfor idx, row in apt_latlon.iterrows():\n    Marker(location = [row['lat'], row['lon']],\n           tooltip=row[\"아파트\"]).add_to(m_2)\n    \n\nfor idx, row in elm_school.iterrows():\n    Circle(location = [row['Latitude'], row['Longitude']],\n           radius=40,\n           tooltip=row[\"초등학교\"],\n           color = \"red\").add_to(m_2)\n\nfor idx, row in mid_school.iterrows():\n    Circle(location = [row['Latitude'], row['Longitude']],\n           radius=50,\n           tooltip=row[\"중학\"],\n           color = \"blue\").add_to(m_2)\n    \nfor idx, row in hi_school.iterrows():\n    Circle(location = [row['Latitude'], row['Longitude']],\n           radius=60,\n           tooltip=row[\"고등학교\"],\n           color = \"green\").add_to(m_2)\n\nGeoJson(buffer_3,style_function=lambda x: {\n    'fillColor': 'blue',\n    'fillOpacity': 0.03, \n    'color': 'blue',\n    'weight': 0.5\n}).add_to(m_2)\n\n\nm_2\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n아파트별 buffer map\n\nbuffer_gdf = gpd.GeoDataFrame(geometry=buffer_3)\n\nelm_sjoin = gpd.sjoin(buffer_gdf, elm_school, how=\"left\", op=\"intersects\")\nelm_count = elm_sjoin.groupby(elm_sjoin.index).count()[\"index_right\"]\n\nmid_sjoin = gpd.sjoin(buffer_gdf, mid_school, how=\"left\", op=\"intersects\")\nmid_count = mid_sjoin.groupby(mid_sjoin.index).count()[\"index_right\"]\n\nhi_sjoin = gpd.sjoin(buffer_gdf, hi_school, how=\"left\", op=\"intersects\")\nhi_count = hi_sjoin.groupby(hi_sjoin.index).count()[\"index_right\"]\n\n\nschool_added = pd.concat([apt_latlon,elm_count,mid_count,hi_count],axis=1)\nschool_added.columns = [\"lon\",\"lat\",\"아파트\",\"geometry\",'1000m_초등학교','1000m_중학교','1000m_고등학교']\nschool_added.head()\n\n\n\n부동산 정보\n아파트별 데이터에 부동산 정보 데이터 병합\n\nseobuk_raw = pd.read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%B6%A9%EC%B2%AD%EB%82%A8%EB%8F%84%20%EC%B2%9C%EC%95%88%EC%8B%9C%20%EC%84%9C%EB%B6%81%EA%B5%AC.csv\",index_col=0)\ndongnam_raw = pd.read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%B6%A9%EC%B2%AD%EB%82%A8%EB%8F%84%20%EC%B2%9C%EC%95%88%EC%8B%9C%20%EB%8F%99%EB%82%A8%EA%B5%AC.csv\")\napt_info_raw = pd.concat([seobuk_raw,dongnam_raw],axis = 0)\n\n사용 컬럼만 추출\n\napt_info = apt_info_raw.iloc[:,0:22]\napt_info = apt_info.drop([\"면적\",\"도로명주소\",\"임대세대수\",\"건설사\",\"전용면적\",\"전용율\",\"공급면적\",\"방수\",\"욕실수\",\"해당면적_세대수\"],axis=1)\napt_info\n\n            아파트명                   법정동주소   latitude  ...    주차대수     난방  현관구조\n0      e편한세상두정2차   충청남도 천안시 서북구 두정동 2055  36.838227  ...  1028.0  HT001   계단식\n1      e편한세상두정2차   충청남도 천안시 서북구 두정동 2055  36.838227  ...  1028.0  HT001   계단식\n2      e편한세상두정2차   충청남도 천안시 서북구 두정동 2055  36.838227  ...  1028.0  HT001   계단식\n3      e편한세상두정2차   충청남도 천안시 서북구 두정동 2055  36.838227  ...  1028.0  HT001   계단식\n0      e편한세상두정3차   충청남도 천안시 서북구 두정동 2063  36.832074  ...  1221.0  HT001   계단식\n..           ...                     ...        ...  ...     ...    ...   ...\n756  청수행정타운금호어울림    충청남도 천안시 동남구 청수동 435  36.792570  ...     0.0  HT001   계단식\n757  청수행정타운금호어울림    충청남도 천안시 동남구 청수동 435  36.792570  ...     0.0  HT001   계단식\n758           현대  충청남도 천안시 동남구 청수동 219-4  36.797601  ...   388.0  HT001   계단식\n759           현대  충청남도 천안시 동남구 청수동 219-4  36.797601  ...   388.0  HT001   계단식\n760           현대  충청남도 천안시 동남구 청수동 219-4  36.797601  ...   388.0  HT001   계단식\n\n[1952 rows x 12 columns]\n\n\n\njoin_test = apt_info.groupby(\"아파트명\").first().reset_index()\njoin_test.columns = ['아파트','법정동주소',\"latitude\",\"longitude\",'세대수','최고층','최저층','용적률','건폐율','주차대수','난방','현관구조']\njoin_test\n\n               아파트                    법정동주소   latitude  ...    주차대수     난방  현관구조\n0        5월의집(도시형)    충청남도 천안시 서북구 성정동 1459  36.826865  ...     4.0  HT001   계단식\n1            88비둘기   충청남도 천안시 동남구 원성동 490-1  36.807701  ...     0.0  HT001   계단식\n2             E클래스   충청남도 천안시 동남구 용곡동 238-1  36.791153  ...     9.0  HT001   계단식\n3      HS에델도프(도시형)     충청남도 천안시 동남구 유량동 164  36.817150  ...    48.0  HT001   계단식\n4        LH천년나무7단지    충청남도 천안시 서북구 불당동 1515  36.817477  ...   494.0  HT005   계단식\n..             ...                      ...        ...  ...     ...    ...   ...\n457        활림크로바3차   충청남도 천안시 동남구 다가동 381-1  36.800456  ...    78.0  HT001   복도식\n458           휴먼시아     충청남도 천안시 동남구 구성동 525  36.797233  ...   483.0  HT001   계단식\n459          희홍밸러뷰  충청남도 천안시 서북구 직산읍 군동리 39  36.896507  ...   123.0  HT001   계단식\n460  힐스테이트천안(주상복합)     충청남도 천안시 동남구 문화동 179  36.807579  ...   525.0  HT001   계단식\n461      힐스테이트천안신부     충청남도 천안시 동남구 신부동 978  36.827729  ...  1149.0  HT001   계단식\n\n[462 rows x 12 columns]\n\n\n\n\n병합시 문제점\n데이터 병합시 아파트별 데이터와, 부동산 정보 데이터의 아파트명 컬럼으로 조인을 진행하였지만 아파트명이 영문으로 쓰여있거나, 단지 표기법의 불일치, 아파트명의 변화로 인해, 440건 중 일부는 지도를 참고하여 수동 조인 진행\n\nm_2 = folium.Map(location=[36.807438,127.144504], zoom_start=13)\nMarker(location=[36.809672,127.161427]).add_to(m_2)\n\nfor idx, row in join_test.iterrows():\n    Marker(location = [row['latitude'], row['longitude']],\n           tooltip=row[\"아파트\"]).add_to(m_2)\n    \nfor idx, row in apt_latlon.iterrows():\n    Circle(location = [row['lat'], row['lon']],\n           tooltip=row[\"아파트\"]).add_to(m_2)\n\n\nm_2\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n아파트별 데이터와 부동산 정보 데이터의 위치정보 비교 지도"
  },
  {
    "objectID": "posts/datamining_report.html#사용-데이터",
    "href": "posts/datamining_report.html#사용-데이터",
    "title": "천안시 아파트 매매가 분석",
    "section": "사용 데이터",
    "text": "사용 데이터\n병합된 데이터중 부동산 정보 데이터가 매칭되지 않은 16개 아파트가 삭제된 데이터\n\ndata_raw = pd.read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/fulldata_test.csv\",index_col=0)\n\n\ndata_raw.head()\n\n     도로명  법정동    지번      아파트  건축년도   층  ...   최저층    용적률   건폐율   주차대수     난방 현관구조\n0  천안천4길  와촌동   173  신동아파밀리에  2005   6  ...  13.0  222.0  20.0  725.0  HT001  계단식\n1   도원2길  성정동   823       선경  1994   4  ...  14.0  273.0  21.0  267.0  HT001  계단식\n2   도원1길  성정동   892  건일(892)  1995   2  ...   5.0  170.0  41.0   10.0  HT001  계단식\n3   서부대로  성정동   785    주공6-1  1989   1  ...   5.0  103.0  19.0  464.0  HT001  계단식\n4   쌍용대로  성정동  1292    에프엠21  2014  13  ...  14.0  793.0  72.0   49.0  HT001  계단식\n\n[5 rows x 27 columns]\n\n\n\ndata_use = data_raw.drop([\"도로명\",\"법정동\",\"지번\",\"아파트\",\"전용면적\",\"거래금액\",\"지번주소\",\"전용면적(평)\"],axis = 1)\ndata_use = data_use.dropna()\n\n\ndata_use.head()\n\n   건축년도   층     년  월   일          평단가  ...   최저층    용적률   건폐율   주차대수     난방  현관구조\n0  2005   6  2018  3  28   855.800229  ...  13.0  222.0  20.0  725.0  HT001   계단식\n1  1994   4  2018  3   1   604.817830  ...  14.0  273.0  21.0  267.0  HT001   계단식\n2  1995   2  2018  3   1   442.393459  ...   5.0  170.0  41.0   10.0  HT001   계단식\n3  1989   1  2018  3   5   759.465632  ...   5.0  103.0  19.0  464.0  HT001   계단식\n4  2014  13  2018  3   5  1530.568359  ...  14.0  793.0  72.0   49.0  HT001   계단식\n\n[5 rows x 19 columns]"
  },
  {
    "objectID": "posts/datamining_report.html#분석",
    "href": "posts/datamining_report.html#분석",
    "title": "천안시 아파트 매매가 분석",
    "section": "분석",
    "text": "분석\n\n독립변수, 종속변수 나누기\n예측할 종속변수는 아파트의 평당 거래액\n독립변수는 건축년도, 층, 거래년월일, 위경도, 1000m 내 초중고 학교 수, 세대수, 최고, 최저층, 용적률, 건폐율, 주차대수, 난방 방식, 현관 구조\n학습, 예측의 성능 비교를 위해 test set 분리 (20%)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(data_use, test_size=0.2, random_state=42)\n\n#X, y 로 나누기\nX_train = train_set.drop('평단가', axis = 1)\ny_train = train_set['평단가']\n\nX_test = test_set.drop('평단가', axis = 1)\ny_test = test_set['평단가']\n\n카테고리형 데이터 변환 & 표준화\n\nlist_cat = [\"난방\",\"현관구조\"]\nlist_num = ['건축년도', '층', '년', '월', '일',  'lon', 'lat', '1000m_초등학교','1000m_중학교', '1000m_고등학교', '세대수', '최고층', '최저층', '용적률', '건폐율', '주차대수']\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([('std_scaler', StandardScaler())])\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, list_num),\n        (\"cat\", OneHotEncoder(), list_cat),\n    ])\n\nX_train_prepared = full_pipeline.fit_transform(X_train)\n\n\nX_train_prepared[0:3]\n\narray([[-0.94811132,  0.73021804,  0.53523285, -1.04732581, -0.08297669,\n        -0.58986061, -0.46848444,  0.83770685, -0.07982424, -0.64833256,\n        -0.07488551,  0.12449917, -0.20334437,  0.00335304, -0.10454875,\n        -0.2614548 ,  1.        ,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ],\n       [-0.83505186,  0.89749658,  0.53523285, -1.04732581,  1.40524442,\n        -0.56539488, -0.33468452,  0.83770685,  0.94180387, -0.64833256,\n        -0.31317101, -0.29473341,  0.3859069 , -0.14222672, -0.26676703,\n        -0.68657743,  1.        ,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ],\n       [-0.83505186, -0.60801024,  2.02031654, -1.63177841,  0.26045895,\n         1.69896745, -1.19178496, -0.71111159, -1.10145235, -0.64833256,\n        -0.84829217, -1.55243114, -1.57826399, -0.51426391,  0.46321524,\n        -0.84028626,  1.        ,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ]])\n\n\n\ncolname =list_num+full_pipeline.named_transformers_['cat'].get_feature_names_out().tolist()\n\nX_train_df = pd.DataFrame(\n    X_train_prepared,\n    columns = colname,\n    index = X_train.index)\n\n\nX_train_df.head()\n\n           건축년도         층         년  ...  현관구조_계단식  현관구조_복도식  현관구조_복합식\n31250 -0.948111  0.730218  0.535233  ...       1.0       0.0       0.0\n31383 -0.835052  0.897497  0.535233  ...       0.0       1.0       0.0\n80636 -0.835052 -0.608010  2.020317  ...       1.0       0.0       0.0\n17321 -1.400349 -0.106175 -0.207309  ...       1.0       0.0       0.0\n12728  1.313078 -0.440732 -0.949851  ...       1.0       0.0       0.0\n\n[5 rows x 22 columns]\n\n\n\nX_test_prepared = full_pipeline.transform(X_test)\n\n\ncolname =list_num+full_pipeline.named_transformers_['cat'].get_feature_names_out().tolist()\n\nX_test_df = pd.DataFrame(\n    X_test_prepared,\n    columns = colname,\n    index = X_test.index)\n\n\nX_test_df.head()\n\n           건축년도         층         년  ...  현관구조_계단식  현관구조_복도식  현관구조_복합식\n80228 -0.495873 -0.106175  1.277775  ...       0.0       1.0       0.0\n6877   1.539197 -0.106175 -0.949851  ...       1.0       0.0       0.0\n55109  1.765316  3.072118 -0.949851  ...       1.0       0.0       0.0\n43599 -1.174230 -1.277124  1.277775  ...       1.0       0.0       0.0\n12506 -1.174230 -0.440732 -0.949851  ...       1.0       0.0       0.0\n\n[5 rows x 22 columns]\n\n\n\n\n머신러닝 적용\n\nlinear regression\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_df,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\nlin_pred = lin_reg.predict(X_test_df)\nlin_mse = mean_squared_error(y_test,lin_pred)\nlin_mse**0.5\n\n252.00958414609863\n\n\n\n\ndecision tree regression\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=2023)\ntree_reg.fit(X_train_df,y_train)\n\nDecisionTreeRegressor(random_state=2023)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(random_state=2023)\n\ntree_pred = tree_reg.predict(X_test_df)\ntree_mse = mean_squared_error(y_test,tree_pred)\ntree_mse**0.5\n\n95.496264449298\n\n\n\n\nrandomforest regression\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=2023)\nforest_reg.fit(X_train_df,y_train)\n\nRandomForestRegressor(random_state=2023)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(random_state=2023)\n\nforest_pred = forest_reg.predict(X_test_df)\nforest_mse = mean_squared_error(y_test,forest_pred)\nforest_mse**0.5\n\n73.72252001575339\n\n\n\n\nxgboost\n\nimport xgboost\n\nxgb_reg = xgboost.XGBRegressor(objective='reg:squarederror',n_estimators=100,random_state = 2023)\nxgb_reg.fit(X_train_df,y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=2023, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=2023, ...)\n\nxgb_pred = xgb_reg.predict(X_test_df)\nxgb_mse = mean_squared_error(y_test,xgb_pred)\nxgb_mse**0.5\n\n79.17591317272414\n\n\n\n\nlightgbm\n\nimport lightgbm as lgb\n\nparams = {}\n\ntrain_ds = lgb.Dataset(X_train_df, label = y_train) \ntest_ds = lgb.Dataset(X_test_df, label = y_test)\n\nlgb_reg = lgb.train(params, train_ds, 1000, test_ds, verbose_eval=100, early_stopping_rounds=100)\n\nlgb_pred = lgb_reg.predict(X_test_prepared)\nlgb_mse = mean_squared_error(y_test,lgb_pred)\n\n\nlgb_mse**0.5\n\n74.42715146903271\n\n\n\n\ncatboost\n\nimport catboost as cb\ncb_reg = cb.CatBoostRegressor()\ncb_reg.fit(X_train_df,y_train)\n\ncb_pred = cb_reg.predict(X_test_df)\ncb_mse = mean_squared_error(y_test,cb_pred)\n\n\ncb_mse**0.5\n\n78.88429958605914\n\n\n\n\n\n딥러닝 적용\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nmodel_1\n\nmodel_1 = keras.Sequential([\n          layers.Dense(20, activation='relu'),\n        layers.Dense(40, activation='relu'),\n        layers.Dense(20, activation='relu'),\n        layers.Dense(10, activation='relu'),\n        layers.Dense(5, activation='relu'),\n        layers.Dense(1)\n      ])\nmodel_1.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n\n\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('')\n        print('.', end='')\n  \nEPOCHS = 300\n\nmodel_1.fit(\n    X_train_df,\n    y_train,\n    validation_split=0.2,\n    verbose=0,epochs=EPOCHS,callbacks=[PrintDot()])\n\n\nmod_1_pred = model_1.predict(X_test_prepared)\nmod_1_mse = mean_squared_error(y_test,mod_1_pred)\nmod_1_mse**0.5\n\n\n\nmodel_2\n\nmodel_2 = keras.Sequential([\n          layers.Dense(20, activation='relu'),\n        layers.Dense(40, activation='relu'),\n        layers.Dense(20, activation='relu'),\n        layers.Dense(10, activation='relu'),\n        layers.Dense(1)\n      ])\nmodel_2.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n\n\nEPOCHS = 300\n\nmodel_2.fit(\n    X_train_df,\n    y_train,\n    validation_split=0.2,\n    verbose=0,epochs=EPOCHS)\n\n\nmod_2_pred = model_2.predict(X_test_prepared)\nmod_2_mse = mean_squared_error(y_test,mod_2_pred)\nmod_2_mse**0.5\n\n\n\nmodel_3\n\nmodel_3 = keras.Sequential([\n          layers.Dense(20, activation='relu'),\n        layers.Dense(10, activation='relu'),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(4, activation='relu'),\n        layers.Dense(2, activation='relu'),\n        layers.Dense(1)\n      ])\nmodel_3.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n\n\nEPOCHS = 300\n\nmodel_3.fit(\n    X_train_df,\n    y_train,\n    validation_split=0.2,\n    verbose=0,epochs=EPOCHS)\n\n\nmod_3_pred = model_3.predict(X_test_prepared)\nmod_3_mse = mean_squared_error(y_test,mod_2_pred)\nmod_3_mse**0.5\n\n\n\nmodel_4\n\nmodel_4 = keras.Sequential([\n          layers.Dense(20, activation='relu'),\n        layers.Dense(40, activation='relu'),\n        layers.Dense(20, activation='relu'),\n        layers.Dense(10, activation='relu'),\n        layers.Dense(5, activation='relu'),\n        layers.Dense(1)\n      ])\nmodel_4.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n\n\nEPOCHS = 1000\n\nmodel_4.fit(\n    X_train_df,\n    y_train,\n    validation_split=0.2,\n    verbose=0,epochs=EPOCHS)\n\n\nmod_4_pred = model_4.predict(X_test_prepared)\nmod_4_mse = mean_squared_error(y_test,mod_2_pred)\nmod_4_mse**0.5\n\n\n\nmodel_5\n\nmodel_5 = keras.Sequential([\n          layers.Dense(20, activation='relu'),\n        layers.Dense(40, activation='relu'),\n        layers.Dense(20, activation='relu'),\n        layers.Dense(10, activation='relu'),\n        layers.Dense(5, activation='relu'),\n        layers.Dense(1)\n      ])\nmodel_5.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('')\n        print('.', end='')\n  \n\nmodel_5.fit(\n    X_train_df,\n    y_train,epochs=EPOCHS,\n    validation_split=0.2,\n     verbose=0, callbacks=[early_stop, PrintDot()])\n\n\nmod_5_pred = model_5.predict(X_test_prepared)\nmod_5_mse = mean_squared_error(y_test,mod_2_pred)\nmod_5_mse**0.5\n\n\n\nmodel_6\n\nmodel_6 = keras.Sequential([\n          layers.Dense(20, activation='relu'),\n        layers.Dense(40, activation='relu'),\n        layers.Dense(20, activation='relu'),\n        layers.Dense(10, activation='relu'),\n        layers.Dense(5, activation='relu'),\n        layers.Dense(1)\n      ])\nmodel_6.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('')\n        print('.', end='')\n  \n\nmodel_6.fit(\n    X_train_df,\n    y_train,epochs=EPOCHS,\n    validation_split=0.2,\n     verbose=0, callbacks=[early_stop, PrintDot()])\n\n\nmod_6_pred = model_6.predict(X_test_prepared)\nmod_6_mse = mean_squared_error(y_test,mod_2_pred)\nmod_6_mse**0.5\n\n예측 성능 비교\n\n\n\nregressor\nRMSE\n\n\n\n\nlinear reg\n252.00\n\n\ndecision tree\n95.46\n\n\nrandomforest\n73.79\n\n\nxgboost\n79.17\n\n\nlightgbm\n74.42\n\n\ncatboost\n78.88\n\n\n\n\n\n\nmodel\nlayers\nepochs\nRMSE\n\n\n\n\nmodel_1\n20/40/20/10/5/1\n300\n91.26\n\n\nmodel_2\n20/40/20/10/1\n300\n95.23\n\n\nmodel_3\n20/10/8/4/2/1\n300\n101.18\n\n\nmodel_4\n20/40/20/10/5/1\n1000\n89.54\n\n\nmodel_5\n20/40/20/10/5/1\n181\n101.18\n\n\nmodel_6\n20/40/20/10/5/1\n279\n101.22\n\n\n\n\nimport matplotlib.pyplot as plt\n\ndef visualize(pred):\n    plt.scatter(y_test,pred,alpha = 0.02)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.axis('equal')\n    plt.axis('square')\n    plt.plot([-100,5000], [-100,5000])\n    \nprediction = [lin_pred, tree_pred, forest_pred, xgb_pred, lgb_pred, cb_pred] \ntitles = ['linear','tree','forest','xgb','lgb','cb']\n\nfor i in range(len(prediction)):\n    plt.subplot(2,3,i+1)\n    visualize(prediction[i])\n    plt.title(titles[i])\n    plt.tight_layout()\n\nplt.show()\n\n\n\n\n머신러닝 모델별 예측력 시각화\n\n\n\n해석\n랜덤 포레스트 모델의 성능이 좋다는 것을 알 수 있지만, 학습, 예측과정을 알 수 없으므로, 단일 트리를 통해 어떻게 값에 도출하는가를 볼 수 있다.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfig = plt.figure(figsize=(15, 10))\n\nforest_reg_4 = RandomForestRegressor(n_estimators=100,max_depth=6, random_state=2023)\nforest_reg_4.fit(X_train_df,y_train)\n\nRandomForestRegressor(max_depth=6, random_state=2023)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=6, random_state=2023)\n\nplot_tree(forest_reg_4.estimators_[5],filled=True,impurity=True,rounded=True)\nplt.show()\n\n\n\n\n\n\n해석 2\n변수 중요도를 통해 예측값 결정의 영향력을 알 수 있다.\n\n건축년도 변수가 영향을 많이 끼침\n난방_HT005는 지열난방 방식으로 최근에 지어진 아파트에 많이 적용됨\n\n\nimport numpy as np\nplt.rc('font', family='NanumGothic')\nimportance = forest_reg.feature_importances_\nindices = np.argsort(importance)[::-1]\nfeature_names =X_train_df.columns\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X_train_df.shape[1]), importance[indices], align='center')\nplt.xticks(range(X_train_df.shape[1]), [feature_names[i] for i in indices], rotation=45)\n\n([&lt;matplotlib.axis.XTick object at 0x7f27895d2f20&gt;, &lt;matplotlib.axis.XTick object at 0x7f27895d0d00&gt;, &lt;matplotlib.axis.XTick object at 0x7f27895d2770&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789412e90&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789413940&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789444430&gt;, &lt;matplotlib.axis.XTick object at 0x7f27894124a0&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789411b10&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789444fd0&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789445a80&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789446530&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789445420&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789446dd0&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789447880&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789460370&gt;, &lt;matplotlib.axis.XTick object at 0x7f27894471f0&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789460c70&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789461720&gt;, &lt;matplotlib.axis.XTick object at 0x7f27894621d0&gt;, &lt;matplotlib.axis.XTick object at 0x7f2789462c80&gt;, &lt;matplotlib.axis.XTick object at 0x7f27894623e0&gt;, &lt;matplotlib.axis.XTick object at 0x7f27894636a0&gt;], [Text(0, 0, '건축년도'), Text(1, 0, 'lon'), Text(2, 0, '난방_HT005'), Text(3, 0, '년'), Text(4, 0, '최고층'), Text(5, 0, 'lat'), Text(6, 0, '용적률'), Text(7, 0, '건폐율'), Text(8, 0, '월'), Text(9, 0, '주차대수'), Text(10, 0, '층'), Text(11, 0, '일'), Text(12, 0, '세대수'), Text(13, 0, '최저층'), Text(14, 0, '1000m_중학교'), Text(15, 0, '1000m_초등학교'), Text(16, 0, '난방_HT001'), Text(17, 0, '1000m_고등학교'), Text(18, 0, '현관구조_복도식'), Text(19, 0, '현관구조_계단식'), Text(20, 0, '난방_HT002'), Text(21, 0, '현관구조_복합식')])\n\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.title('Feature Importance')\nplt.tight_layout()\n\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\n\nplt.show()\n\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\nfindfont: Font family 'NanumGothic' not found.\n\n\n\n\n\n\n\n보완점\n\n아파트 단위의 거래금액보다 좁은 범위의 평형 별 데이터 활용 필요\n다른 추가 변수(상권, 교통수단 등) 수집 필요\n모델의 파라미터 튜닝 필요\n딥러닝, 결과 해석 역량 부족"
  },
  {
    "objectID": "posts/data_visualizing_final.html",
    "href": "posts/data_visualizing_final.html",
    "title": "한남대학교 주변 버스정류장 이용량 시각화",
    "section": "",
    "text": "link\n\n\n\n한남대 주변 버스정류장의 시간대별 사용량을 알아봄으로서 버스의 유연한 배차 기획\n\n\n\n\n국가교통 데이터 오픈마켓\n\n대전시 대중교통 이용정보 데이터\n대전시 버스 정류장 속성 데이터\n\n\n\n\n\ngraph RL\n    A[버스 정류장 속성&lt;br&gt; - 버스정류장 코드&lt;br&gt; - 버스정류장 이름&lt;br&gt; - ..] --&gt;|버스정류장 코드| B[버스 이용량&lt;br&gt;  - 노선번호 &lt;br&gt; - 정류장 코드 &lt;br&gt; - 탑승객수&lt;br&gt; - 하차승객수&lt;br&gt; - ..]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시간대별 버스 이용량보다 작은 단위로 시각화 필요\n학생이 주 사용자기 때문에 방학시기는 따로 고려해야함"
  },
  {
    "objectID": "posts/data_visualizing_final.html#목적",
    "href": "posts/data_visualizing_final.html#목적",
    "title": "한남대학교 주변 버스정류장 이용량 시각화",
    "section": "",
    "text": "한남대 주변 버스정류장의 시간대별 사용량을 알아봄으로서 버스의 유연한 배차 기획"
  },
  {
    "objectID": "posts/data_visualizing_final.html#데이터",
    "href": "posts/data_visualizing_final.html#데이터",
    "title": "한남대학교 주변 버스정류장 이용량 시각화",
    "section": "",
    "text": "국가교통 데이터 오픈마켓\n\n대전시 대중교통 이용정보 데이터\n대전시 버스 정류장 속성 데이터\n\n\n\n\n\ngraph RL\n    A[버스 정류장 속성&lt;br&gt; - 버스정류장 코드&lt;br&gt; - 버스정류장 이름&lt;br&gt; - ..] --&gt;|버스정류장 코드| B[버스 이용량&lt;br&gt;  - 노선번호 &lt;br&gt; - 정류장 코드 &lt;br&gt; - 탑승객수&lt;br&gt; - 하차승객수&lt;br&gt; - ..]"
  },
  {
    "objectID": "posts/data_visualizing_final.html#한계점",
    "href": "posts/data_visualizing_final.html#한계점",
    "title": "한남대학교 주변 버스정류장 이용량 시각화",
    "section": "",
    "text": "시간대별 버스 이용량보다 작은 단위로 시각화 필요\n학생이 주 사용자기 때문에 방학시기는 따로 고려해야함"
  },
  {
    "objectID": "posts/geocomputation_apply.html",
    "href": "posts/geocomputation_apply.html",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "",
    "text": "rm(list = ls())\n\nlibrary(sf)\n\nLinking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rgdal)\n\n필요한 패키지를 로딩중입니다: sp\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\nPlease note that rgdal will be retired during October 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-7, (SVN revision 1203)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.0.4, released 2020/01/28\nPath to GDAL shared files: /usr/share/gdal\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 6.3.1, February 10th, 2020, [PJ_VERSION: 631]\nPath to PROJ shared files: /usr/share/proj\nLinking to sp version:2.0-0\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\nlibrary(plotly)\n\n\n다음의 패키지를 부착합니다: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(ggtext)\nlibrary(tmap)"
  },
  {
    "objectID": "posts/geocomputation_apply.html#library-packages",
    "href": "posts/geocomputation_apply.html#library-packages",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "",
    "text": "rm(list = ls())\n\nlibrary(sf)\n\nLinking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rgdal)\n\n필요한 패키지를 로딩중입니다: sp\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\nPlease note that rgdal will be retired during October 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-7, (SVN revision 1203)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.0.4, released 2020/01/28\nPath to GDAL shared files: /usr/share/gdal\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 6.3.1, February 10th, 2020, [PJ_VERSION: 631]\nPath to PROJ shared files: /usr/share/proj\nLinking to sp version:2.0-0\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\nlibrary(plotly)\n\n\n다음의 패키지를 부착합니다: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(ggtext)\nlibrary(tmap)"
  },
  {
    "objectID": "posts/geocomputation_apply.html#load-data",
    "href": "posts/geocomputation_apply.html#load-data",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "Load data",
    "text": "Load data\nFollowing data is geographic data for Chungcheongnam-do.\nhttp://data.nsdi.go.kr/dataset/15145\n\n\n\n\n\n\n\n\n\n\n\nEMD_CD\nEMD_NM\nSGG_OID\nCOL_ADM_SE\nGID\ngeometry\n\n\n\n\n읍면동 코드\n읍면동 이름\n시군구 코드\n시 코드\nGID\n지리정보\n\n\n\nConvert data type (sp -&gt; sf)\n\ndata_sp &lt;- readOGR(\"~/dataset/LSMD_ADM_SECT_UMD_44.shp\",encoding = \"euc-kr\")\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nOGR data source with driver: ESRI Shapefile \nSource: \"/home/sungil/dataset/LSMD_ADM_SECT_UMD_44.shp\", layer: \"LSMD_ADM_SECT_UMD_44\"\nwith 316 features\nIt has 5 fields\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\ndata_sf = st_as_sf(data_sp)\ndata_sf %&gt;% head()\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 911558.7 ymin: 1829456 xmax: 934941.7 ymax: 1853055\nProjected CRS: Korea 2000 / Unified CS\n    EMD_CD EMD_NM SGG_OID COL_ADM_SE GID                       geometry\n0 44800360 결성면    3817      44800 979 MULTIPOLYGON (((911891.8 18...\n1 44800350 은하면    3816      44800 980 MULTIPOLYGON (((920373.2 18...\n2 44800340 장곡면    3815      44800 981 MULTIPOLYGON (((931934 1838...\n3 44800330 홍동면    3814      44800 982 MULTIPOLYGON (((931821 1843...\n4 44800320 금마면    3813      44800 983 MULTIPOLYGON (((931724.3 18...\n5 44800256 홍북읍    3812      44800 984 MULTIPOLYGON (((931724.3 18...\n\n\n\ndata_sf %&gt;% ggplot(aes(fill = COL_ADM_SE))+\n  geom_sf()+\n  theme_minimal()+\n  labs(title = \"충청남도\")"
  },
  {
    "objectID": "posts/geocomputation_apply.html#filter-place-of-interest",
    "href": "posts/geocomputation_apply.html#filter-place-of-interest",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "Filter Place of Interest",
    "text": "Filter Place of Interest\n\ncheonan &lt;- data_sf %&gt;% \n  filter(COL_ADM_SE==\"44130\")\n\ncheonan_seobuk &lt;- cheonan %&gt;%  \n  filter(substr(EMD_CD,1,5)==\"44133\")\n\ncheonan_dongnam &lt;- cheonan %&gt;%  \n  filter(substr(EMD_CD,1,5)==\"44131\")\n\n\ncheonan %&gt;% ggplot(aes(fill=substr(EMD_CD,1,5)))+\n  geom_sf()+\n  theme_minimal()+\n  labs(title = \"천안시 (구별)\")+\n  scale_fill_discrete(name = \"구\",\n                      labels = c(\"동남구\",\"서북구\"))\n\n\n\n\n\nchsb &lt;- cheonan_seobuk %&gt;% ggplot(aes(fill=EMD_NM))+\n  geom_sf()+\n  geom_sf_text(mapping = aes(label = EMD_NM))+\n  labs(title = \"천안시 서북구 (읍면동)\")+\n  theme_minimal()+\n  scale_fill_discrete(name = \"읍면동\")\n\nchsb\n\n\n\n\n\nchdn &lt;- cheonan_dongnam %&gt;% ggplot(aes(fill=EMD_NM))+\n  geom_sf()+\n  geom_sf_text(mapping = aes(label = EMD_NM))+\n  labs(title = \"천안시 동남구 (읍면동)\")+\n  theme_minimal()+\n  scale_fill_discrete(name = \"읍면동\")\n\nchdn"
  },
  {
    "objectID": "posts/geocomputation_apply.html#건축년도",
    "href": "posts/geocomputation_apply.html#건축년도",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "건축년도",
    "text": "건축년도\n\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/GEOCOMPS.csv\")\ndata_sf &lt;- data %&gt;% \n  mutate(평단가 = (거래금액/전용면적) %&gt;% as.integer()) %&gt;% \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"))\nst_crs(data_sf) &lt;- 4737\nst_crs(data_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4737 \n  wkt:\nGEOGCRS[\"Korea 2000\",\n    DATUM[\"Geocentric datum of Korea\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"Korea, Republic of (South Korea)\"],\n        BBOX[28.6,122.71,40.27,134.28]],\n    ID[\"EPSG\",4737]]\n\ncheonan &lt;- cheonan %&gt;% \n  filter(EMD_NM!=\"광덕면\")\n\n\n\ngn &lt;- ggplot()+\n  geom_sf(data = cheonan,fill=NA)+\n  geom_sf(data = data_sf,mapping = aes(color = 평단가))+\n  scale_color_gradient(low = \"blue\", high = \"red\")+\n  theme_minimal()\n\ngn"
  },
  {
    "objectID": "posts/geocomputation_apply.html#아파트별-평단가",
    "href": "posts/geocomputation_apply.html#아파트별-평단가",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "아파트별 평단가",
    "text": "아파트별 평단가\n\nggplotly\n\nlibrary(readxl)\ndata_nm_raw &lt;- read_excel(\"~/Sungil_LAB/linear_model_dataset.xlsx\")\ndata_nm_raw &lt;- data_nm_raw %&gt;% mutate(평단가 = 거래금액/전용면적)\n\ndata_nm &lt;- data_nm_raw %&gt;% \n  group_by(아파트) %&gt;% \n  summarize(평균거래액 = mean(평단가) %&gt;% as.integer(),\n            Latitude = mean(Latitude),\n            Longitude = mean(Longitude)) %&gt;% \n  mutate(정보 = paste(아파트,평균거래액,sep = \", 평단가 :\"))\n\ndata_nm_sf &lt;- data_nm %&gt;% st_as_sf(coords = c(\"Longitude\",\"Latitude\"))\nst_crs(data_nm_sf) &lt;- 4737\n\ncheonan &lt;- cheonan %&gt;% \n  filter(EMD_NM!=\"광덕면\")\n\n  \ngn &lt;- ggplot()+\n  geom_sf(data = cheonan,color = \"black\")+\n  geom_sf(data = data_nm_sf,aes(fill = 정보))+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\ngn\n\n\n\nTmap library\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\n\ntm_shape(cheonan[\"EMD_NM\"])+\n  tm_polygons(col=\"white\",alpha = 0.3)+\n  tm_shape(data_nm_sf[\"정보\"])+\n  tm_symbols(shape = 2, col = \"red\", size = 0.05)"
  },
  {
    "objectID": "posts/geocomputation_apply.html#datamining",
    "href": "posts/geocomputation_apply.html#datamining",
    "title": "충남 천안시의 지리데이터 다루기",
    "section": "Datamining",
    "text": "Datamining\n\nrm(list = ls())\n\ndata = read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%B2%9C%EC%95%88%EC%8B%9C_%EC%95%84%ED%8C%8C%ED%8A%B8_%EB%A7%A4%EB%A7%A4_2018_2023.csv\")\n\n\ndata_use &lt;- data %&gt;% \n  filter(법정동 %in% c(\"백석동\",\"불당동\")) %&gt;% \n  select(4,6,7,9,10,11,12,13)\n\napt_name = data_use %&gt;% \n  filter(건축년도&lt;2018) %&gt;%  \n  group_by(아파트,년) %&gt;% \n  summarize() %&gt;% \n  group_by(아파트) %&gt;% \n  summarize(ss = sum(년)) %&gt;% \n  filter(ss == 12123)\n\n\ndata_1 &lt;- data_use %&gt;% \n  filter(아파트 %in% apt_name$아파트) %&gt;%\n  mutate(평단가=거래금액/전용면적) %&gt;% \n  group_by(아파트,년,법정동) %&gt;% \n  summarize(mean = mean(평단가))\n\n\n  \ndata_1 %&gt;% \n  plot_ly(\n    x= ~mean,\n    y= ~아파트,\n    frame = ~년,\n    text = ~아파트,\n    color = ~법정동,\n    hoverinfo = \"text\",\n    type = 'scatter',\n    mode = 'markers',\n    fill = ~'')\n\n\ndata_1$아파트 %&gt;% unique() %&gt;% length()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "지역사회 문제해결형 빅데이터/AI활용 공모전\n\n\n\n\n\n대전시 대중교통 통행수요 예측\n\n\n\n\n\n\nNov 25, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nCompetiton Data Visualization\n\n\n\n\n\n\n\npython\n\n\n\n\n타일별 버스정류장 수 / 가져다 쓰슈~\n\n\n\n\n\n\nNov 23, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\n대시보드 창고\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nHTML\n\n\nMarkdown\n\n\n\n\nFlexdashboard 연습\n\n\n\n\n\n\nJul 14, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\n데이터베이스 만들기\n\n\n\n\n\n\n\nSQL\n\n\nLINUX\n\n\n\n\n라즈베리파이, MySQL, MariaDB, 원격연결\n\n\n\n\n\n\nJul 1, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\n천안시 아파트 매매가 분석\n\n\n\n\n\n\n\npython\n\n\n\n\n아파트 실거래가, 네이버부동산, api활용 및 웹크롤링 / 데이터마이닝 (2023-1)\n\n\n\n\n\n\nJun 17, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\n천안시 아파트 매매가 회귀분석\n\n\n\n\n\n\n\nR\n\n\n\n\n선형회귀 (2023-1)\n\n\n\n\n\n\nApr 27, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nSQL\n\n\n\n\n\n\n\nSQL\n\n\n\n\nSQL Basics\n\n\n\n\n\n\nApr 10, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nPython playground\n\n\n\n\n\n\n\nPython\n\n\n\n\n파이썬 키보드 뚜드리기\n\n\n\n\n\n\nMar 28, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\npizza\n\n\n\n\n\n피자학개론\n\n\n\n\n\n\nMar 14, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nTraffic line detection using CV2\n\n\n\n\n\n\n\nPython\n\n\n\n\n파이썬 cv2 활용 차선 탐지 (2022-2)\n\n\n\n\n\n\nOct 18, 2022\n\n\nsungil_park\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "records/Swimming.html",
    "href": "records/Swimming.html",
    "title": "Stress is water soluble",
    "section": "",
    "text": "Updates at weekend\n\n\nRecord with Galaxy Watch 4\n\n\nGears\n\nSPEEDO Allover V-cut Jammer\nNIKE hydrastrong Jammer\nNIKE have a Nike day Swimming cap\nARENA X MARDI Swimming cap\nNIKE Vapor Mirrored Performance Goggle\nARENA Cobra Ultra Racing Goggle\n\n\n\n\nDate\nDistance(m)\nTime\nPace(/100m)\nReview\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/07/24\n\n\n\n\n\n\n2023/08/26\n4000\n77:45\n\n\n\n\n2023/08/23\n2000\n35:59\n1’47\n\n\n\n2023/08/21\n1550\n27:33\n1’45\n\n\n\n2023/08/18\n1500\n27:41\n1’51\n\n\n\n2023/08/16\n1750\n67:10\n1’14\n\n\n\n2023/08/14\n1300\n24:22\n1’51\n\n\n\n2023/08/13\n1050\n18:42\n1’43\n\n\n\n2023/08/11\n2000\n53:31\n1’49\n\n\n\n2023/08/09\n2200\n55:45\n1’30\n\n\n\n2023/08/07\n1500\n26:12\n1’44\n\n\n\n2023/08/05\n600\n10:48\n1’47\n\n\n\n2023/08/04\n2050\n36:31\n1’46\n\n\n\n2023/08/01\n1100\n20:31\n1’45\nPohang\n\n\n2023/07/31\n1100\n19:45\n1’28\n\n\n\n2023/07/28\n1250\n22:02\n1’45\n\n\n\n2023/07/27\n2100\n39:02\n1’51\n\n\n\n2023/07/24\n2000\n36:52\n1’50\n17’40/1500m\n\n\n2023/07/22\n2200\n40:25\n1’49\n\n\n\n2023/07/19\n2850\n58:06\n1’04\n\n\n\n2023/07/17\n1500\n28:06\n1’52\n\n\n\n2023/07/15\n700\n12:22\n1’45\n\n\n\n2023/07/13\n1150\n20:54\n1’47\n\n\n\n2023/07/11\n700\n14:50\n1’52\nRecovery\n\n\n2023/07/09\n2270\n33:45\n1’29\nFirst openwater competition\n\n\n2023/07/07\n600\n11:19\n1’53\n\n\n\n2023/07/06\n1250\n23:20\n1’44\n\n\n\n2023/07/05\n2600\n42:15\n1’37\n\n\n\n2023/07/04\n1500\n32:30\n1’56\n\n\n\n2023/07/03\n1600\n30:49\n1’55\n\n\n\n2023/07/01\n1500\n29:27\n1’57\n\n\n\n2023/06/30\n1500\n28:30\n1’53\n\n\n\n2023/06/29\n700\n13:54\n1’54\n\n\n\n2023/06/28\n2600\n46:02\n1’42\n\n\n\n2023/06/26\n1300\n25:11\n1’56\n\n\n\n2023/06/24\n1000\n19:26\n1’51\n\n\n\n2023/06/22\n2000\n38:23\n1’54\n\n\n\n2023/06/19\n1000\n19:19\n1’55\n\n\n\n2023/06/16\n1300\n27:09\n2’04\n\n\n\n2023/06/14\n1000\n14:48\n\n\n\n\n2023/06/12\n2050\n41:16\n1’58\n\n\n\n2023/06/10\n1000\n20:25\n2’02\n\n\n\n2023/06/09\n1500\n32:29\n1’58\n\n\n\n2023/06/07\n3400\n52:36\n1’15\n\n\n\n2023/06/06\n1600\n31:29\n1’57\n\n\n\n2023/06/05\n1200\n23:47\n1’58\n\n\n\n2023/06/03\n1000\n19:02\n1’53\n\n\n\n2023/05/31\n3100\n48:47\n1’14\nWetsuit day 9\n\n\n2023/05/29\n1000\n20:25\n2’00\n\n\n\n2023/05/26\n1600\n31:50\n1’59\n\n\n\n2023/05/25\n1250\n29:05\n1’59\n\n\n\n2023/05/24\n3200\n46:04\n1’07\nWetsuit day 8\n\n\n2023/05/22\n1000\n19:36\n1’57\n\n\n\n2023/05/20\n1000\n20:32\n2’02\nRecovery day\n\n\n2023/05/17\n3000\n50:28\n1’10\nWetsuit day 7\n\n\n2023/05/16\n1500\n31:06\n2’04\n\n\n\n2023/05/13\n1000\n34:55\n-\nRecovery day\n\n\n2023/05/12\n1000\n19:15\n1’53\nRecover & Dash\n\n\n2023/05/10\n3650\n61:40\n1’13\nWetsuit day 6\n\n\n2023/05/08\n2200\n45:53\n2’05\n2000\n\n\n2023/05/06\n1400\n29:16\n1’24\n25m\n\n\n2023/05/04\n1700\n31:46\n1’51\n\n\n\n2023/05/03\n2650\n62:10\n1’07\nWetsuit day 5\n\n\n2023/04/30\n1500\n30~\n2’~\n-\n\n\n2023/04/29\n650\n13:06\n1’59\nRecovery day\n\n\n2023/04/26\n2900\n60:40\n1’13\nWetsuit day 4\n\n\n2023/04/24\n2000\n39:43\n1’58\n\n\n\n2023/04/21\n1000\n20:37\n1’58\nRecovery day\n\n\n2023/04/19\n3000\n60:59\n1’09\nWetsuit day 3\n\n\n2023/04/18\n450\n09:02\n1’40\nRecovery day\n\n\n2023/04/17\n1700\n33:59\n1’59\n1500m 29:56\n\n\n2023/04/14\n1300\n25:59\n1’57\n\n\n\n2023/04/12\n3050\n60:22\n1’18\nWetsuit day 2\n\n\n2023/04/11\n1200\n23:54\n1’59\n\n\n\n2023/04/10\n1250\n36:05\n1’58\n\n\n\n2023/04/07\n1000\n22:56\n2’57\n25m\n\n\n2023/04/05\n3150\n67:44\n1’22\nWetsuit day 1\n\n\n2023/04/03\n2000\n41:55\n2’05\n\n\n\n2023/04/01\n1000\n19:10\n1’53\n\n\n\n2023/03/31\n1000\n19:08\n1’54\n\n\n\n2023/03/30\n700\n15:52\n2’07\nRecovery day\n\n\n2023/03/29\n2800\n51:31\n1’07\nOpenwater week6\n\n\n2023/03/23\n600\n28:59\n1’52\nRecovery day\n\n\n2023/03/22\n3050\n56:50\n1’38\nOpenwater Week5\n\n\n2023/03/21\n900\n18:53\n1’59\nRecovery day\n\n\n2023/03/20\n2100\n42:57\n2’02\n1800m\n\n\n2023/03/18\n1600\n29:31\n1’50\n50m Dash PR 42sec\n\n\n2023/03/16\n1800\n35:42\n1’58\ngood\n\n\n2023/03/14\n700\n13:52\n1’58\nDNF 40m Success\n\n\n2023/03/13\n1300\n26:25\n2’01\n\n\n\n2023/03/12\n1050\n27:28\n1’44\n\n\n\n2023/03/10\n1500\n30:00\n1’59\n1500m/30min Success\n\n\n2023/03/07\n2000\n41:52\n2’05\n\n\n\n2023/03/06\n1350\n27:28\n2’00\n\n\n\n2023/03/04\n450\n9:42\n2’06\nRecovery day\n\n\n2023/03/03\n1950\n50:13\n2’16\n\n\n\n2023/03/01\n3100\n65:00\n2’05\n3.1 to 3.1km"
  },
  {
    "objectID": "records/gallery.html",
    "href": "records/gallery.html",
    "title": "Sungil Gallery",
    "section": "",
    "text": "대한민국 해병대\n달리는 제 1 상륙사단\n최강킹콩 3여단\n제 1576 공정대대\n흑곰 화기중대\n스파르타 직사화기소대\n상승불패 K-4반\n해병공수 243차\n兵1255期 박성일"
  },
  {
    "objectID": "records/gallery.html#전역",
    "href": "records/gallery.html#전역",
    "title": "Sungil Gallery",
    "section": "전역",
    "text": "전역\n\n\nVideo"
  },
  {
    "objectID": "posts/database.html#네트워크",
    "href": "posts/database.html#네트워크",
    "title": "데이터베이스 만들기",
    "section": "네트워크",
    "text": "네트워크\n\n포트포워딩\n\n\nDDNS"
  },
  {
    "objectID": "posts/database.html#연결",
    "href": "posts/database.html#연결",
    "title": "데이터베이스 만들기",
    "section": "연결",
    "text": "연결\n\n동일 네트워크\n\n\n원격 연결"
  },
  {
    "objectID": "posts/database.html",
    "href": "posts/database.html",
    "title": "데이터베이스 만들기",
    "section": "",
    "text": "라즈베리파이에 os를 설치하기 위해 먼저 PC에 Raspberry Pi Imager를 설치한다.\n\n이미지 파일을 따로 다운받아서\n\n운영체제 -&gt; 사용자 정의 사용 -&gt; 다운받은 이미지 파일\n저장소 -&gt; 사용할 SD카드\n\n경로로 쓰기 하였다.\n이미지 쓰기를 완료한 후, SD카드를 라즈베리파이에 꽃아 부팅한다.\n\n\n\n라즈베리파이에 인터넷 연결을 하고 MariaDB를 설치한다.\n\n#Terminal\nsudo apt install mariadb-server\nsudo apt install mariadb-client\n\nroot 비밀번호 설정, 익명계정, 원격접속 허용 여부를 설정한다.\n실행\n\n#Terminal\nsudo mysql -u root -p\n\nroot 유저로(-u) 비밀번호(-p) 인증 접속\n\n#Terminal\nsungil@sungil-950QDB:~$ mysql -u root -p\nEnter password: \nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 11\nServer version: 10.3.38-MariaDB-0ubuntu0.20.04.1 Ubuntu 20.04\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nshow databases; 명령어로 데이터베이스 목록을 볼 수 있다.\n기본 데이터베이스 3개를 확인할 수 있다.\n\nMariaDB [(none)]&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n+--------------------+\n4 rows in set (0.010 sec)\n\ncreate database '데이터베이스 이름'; 명령어로 새 데이터베이스를 만들 수 있다.\ndrop database '데이터베이스 이름'; 으로 데이터베이스를 삭제한다.\n\nMariaDB [(none)]&gt; create database new_db;\nQuery OK, 1 row affected (0.001 sec)\n\n\nMariaDB [(none)]&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| new_db             |\n| performance_schema |\n+--------------------+\n5 rows in set (0.001 sec)\n\nselect host,user,plugin,authentication_string from mysql.user; 명령어로 유저명과 보안방식을 볼 수 있다.\n\nMariaDB [(none)]&gt; select host,user,plugin,authentication_string from mysql.user;\n+-----------+------+-----------------------+-------------------------------------------+\n| host      | user | plugin                | authentication_string                     |\n+-----------+------+-----------------------+-------------------------------------------+\n| localhost | root | mysql_native_password | *3E9FEA9A673179BFC73F9E1357E81FE84FDE72B6 |\n| %         | root |                       |                                           |\n+-----------+------+-----------------------+-------------------------------------------+\n3 rows in set (0.000 sec)"
  },
  {
    "objectID": "posts/database.html#라즈베리파이",
    "href": "posts/database.html#라즈베리파이",
    "title": "데이터베이스 만들기",
    "section": "",
    "text": "라즈베리파이에 os를 설치하기 위해 먼저 PC에 Raspberry Pi Imager를 설치한다.\n\n이미지 파일을 따로 다운받아서\n\n운영체제 -&gt; 사용자 정의 사용 -&gt; 다운받은 이미지 파일\n저장소 -&gt; 사용할 SD카드\n\n경로로 쓰기 하였다.\n이미지 쓰기를 완료한 후, SD카드를 라즈베리파이에 꽃아 부팅한다.\n\n\n\n라즈베리파이에 인터넷 연결을 하고 MariaDB를 설치한다.\n\n#Terminal\nsudo apt install mariadb-server\nsudo apt install mariadb-client\n\nroot 비밀번호 설정, 익명계정, 원격접속 허용 여부를 설정한다.\n실행\n\n#Terminal\nsudo mysql -u root -p\n\nroot 유저로(-u) 비밀번호(-p) 인증 접속\n\n#Terminal\nsungil@sungil-950QDB:~$ mysql -u root -p\nEnter password: \nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 11\nServer version: 10.3.38-MariaDB-0ubuntu0.20.04.1 Ubuntu 20.04\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nshow databases; 명령어로 데이터베이스 목록을 볼 수 있다.\n기본 데이터베이스 3개를 확인할 수 있다.\n\nMariaDB [(none)]&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n+--------------------+\n4 rows in set (0.010 sec)\n\ncreate database '데이터베이스 이름'; 명령어로 새 데이터베이스를 만들 수 있다.\ndrop database '데이터베이스 이름'; 으로 데이터베이스를 삭제한다.\n\nMariaDB [(none)]&gt; create database new_db;\nQuery OK, 1 row affected (0.001 sec)\n\n\nMariaDB [(none)]&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| new_db             |\n| performance_schema |\n+--------------------+\n5 rows in set (0.001 sec)\n\nselect host,user,plugin,authentication_string from mysql.user; 명령어로 유저명과 보안방식을 볼 수 있다.\n\nMariaDB [(none)]&gt; select host,user,plugin,authentication_string from mysql.user;\n+-----------+------+-----------------------+-------------------------------------------+\n| host      | user | plugin                | authentication_string                     |\n+-----------+------+-----------------------+-------------------------------------------+\n| localhost | root | mysql_native_password | *3E9FEA9A673179BFC73F9E1357E81FE84FDE72B6 |\n| %         | root |                       |                                           |\n+-----------+------+-----------------------+-------------------------------------------+\n3 rows in set (0.000 sec)"
  },
  {
    "objectID": "posts/dashoard_tr.html",
    "href": "posts/dashoard_tr.html",
    "title": "대시보드 창고",
    "section": "",
    "text": "천안시 아파트"
  },
  {
    "objectID": "basics_example/back_propagation.html",
    "href": "basics_example/back_propagation.html",
    "title": "Backpropagtion with R",
    "section": "",
    "text": "Backpropagation은 Artificial Neural Network를 학습시키기 위한 일반적인 알고리즘 중 하나이다.한국말로 직역하면 역전파라는 뜻인데,target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후, 그 오차값을 다시 뒤로 전파해가면서, 각 뉴런이 가지고 있는 변수들을 갱신하는 알고리즘인 것이다.\n\n\n\nset.seed(2023)\n\nX &lt;- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값\ny &lt;- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값\n\n\n\n\n\n시그모이드\n\n\nsigmoid &lt;- function(x){\n  1/(1+exp(-x))\n}\n\n\n시그모이드 도함수(기울기)\n\n\nsigmoid_derivative &lt;- function(x){\n  sigmoid(x)*(1-sigmoid(x))\n}\n\n\nmse 함수\n\n\nmse_loss &lt;- function(y_true,y_pred){\n  mean((y_true - y_pred)**2)\n}\n\n\n\n\n\n\n체인룰(chain rule)은 미적분학에서 사용되는 중요한 개념 중 하나로, 함수의 미분을 연쇄적으로 계산할 때 도움을 주는 규칙입니다.딥러닝에서도 backpropagation (역전파) 알고리즘을 이해하고 구현하는 데 필수적인 개념 중 하나입니다.\n체인룰은 다음과 같이 표현됩니다. 두 개의 함수 f(x)와 g(x)가 있을 때, f(g(x))의 도함수(미분)를 계산할 때 사용됩니다.\n(d/dx)[f(g(x))] = (df/dg) * (dg/dx)\n여기서 각 항목의 의미는 다음과 같습니다:\n\n(d/dx)는 x에 대한 미분을 의미합니다.\nf(g(x))는 함수 f를 함수 g의 출력에 적용한 것을 나타냅니다.\n(df/dg)는 함수 f를 함수 g의 출력에 대해서 미분한 것을 나타냅니다.\n(dg/dx)는 함수 g를 x에 대해서 미분한 것을 나타냅니다.\n\n이 규칙을 사용하여 복잡한 함수를 구성하는 동안 각 함수의 미분 값을 계산하고, 그 값을 이용하여 전체 함수의 미분을 계산할 수 있습니다.이러한 체인룰은 신경망에서 각 층의 미분을 효과적으로 계산하는 데 사용되며, 역전파 알고리즘의 핵심 구성 요소 중 하나입니다.딥러닝에서는 체인룰을 통해 모델을 훈련시키고, 가중치 및 편향을 업데이트하는 데 사용됩니다.\n\n\n\n1-레이어이고, input 변수가 2개인 신경망을 사용해서 backpropagation을 구현해봅시다.\n히든레이어의 뉴런의 수는 2개 입니다.\n\n\n\\[\n\\frac{\\partial L}{\\partial a_1} = (a_1 - y)\n\\] \\[\n\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\times \\sigma'(a_1)\n\\] \\[\n\\frac{\\partial L}{\\partial W_1} = X^T \\frac{\\partial L}{\\partial z_1}\n\\] \\[\n\\frac{\\partial L}{\\partial b_1} = \\sum \\frac{\\partial L}{\\partial z_1}\n\\]\n\n\n\n\\[\nW_1 = W_1 - \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial W_1}\n\\] \\[\nb_1 = b_1 - \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial b_1}\n\\]\n\ninput_size &lt;- 2\noutput_size &lt;- 1\n\nW1 &lt;- matrix(rnorm(input_size * output_size), nrow = input_size, ncol = output_size)\nb1 &lt;- matrix(0, nrow = 4, ncol = output_size)\n\nlearning_rate = 0.1\nepochs = 500\n\n# 초기화\nloss_2 &lt;- numeric()  # 빈 벡터로 손실을 저장\n\n# 학습 시작\nfor (epoch in 1:epochs) {\n  # Forward Pass\n  z1 &lt;- X %*% W1 + b1\n  a1 &lt;- sigmoid(z1)\n\n  # 손실 계산\n  loss &lt;- mse_loss(y, a1)\n\n  # Backward Pass\n  da1 &lt;- a1 - y\n  dz1 &lt;- da1 * sigmoid_derivative(a1)  # Sigmoid 함수의 미분\n  dW1 &lt;- t(X) %*% dz1\n  db1 &lt;- sum(dW1)\n\n  # 가중치 업데이트\n  W1 &lt;- W1 - learning_rate * dW1\n  b1 &lt;- b1 - learning_rate * db1\n\n  loss_2 &lt;- c(loss_2, loss)\n}\n\n\n\n\n\n\n\n\n\n          [,1]\n[1,] 0.6099791\n[2,] 0.5299729\n[3,] 0.5386502\n[4,] 0.4570364\n\n\n\n\n\n\nloss_data &lt;- data.frame(epoch = 1:length(loss_2), loss = loss_2)\n\nggplot(data = loss_data, aes(x = epoch, y = loss)) +\n  geom_line() +\n  labs(x = \"epoch\", y = \"loss\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n히든 레이어의 수가 2개이고, 각 레이어의 뉴런의 수가 3개인 뉴럴네트워크를 같은 형태로 구현해봅시다\n\nset.seed(2023)\n\nX &lt;- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값\ny &lt;- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값\n\ninput_size = 2\nhidden_size = 3\nhidden_2_size = 3\noutput_size = 1\n\nW1 &lt;- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)\nb1 &lt;- matrix(0, nrow = 4, ncol = hidden_size)\nW2 &lt;- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)\nb2 &lt;- matrix(0, nrow = 4, ncol = hidden_2_size)\nW3 &lt;- matrix(rnorm(hidden_2_size * output_size), nrow = hidden_2_size, ncol = output_size)\nb3 &lt;- matrix(0, nrow = 4, ncol = output_size)\n\nlearning_rate = 0.1\nepochs = 500\n\n# 초기화\nloss_2 &lt;- numeric()  # 빈 벡터로 손실을 저장\n\n# 학습 시작\nfor (epoch in 1:epochs) {\n  # Forward Pass\n  z1 &lt;- X %*% W1 + b1\n  a1 &lt;- sigmoid(z1)\n\n  z2 &lt;- a1 %*% W2 + b2\n  a2 &lt;- sigmoid(z2)\n  \n  z3 &lt;- a2 %*% W3 + b3\n  a3 &lt;- sigmoid(z3)\n  # 손실 계산\n  loss &lt;- mse_loss(y, a3)\n\n  # Backward Pass\n  da3 &lt;- a3 - y\n  dz3 &lt;- da3 * sigmoid_derivative(a3)\n  dW3 &lt;- t(a2) %*% dz3\n  db3 &lt;- sum(dz3)\n  \n  da2 &lt;- a2 - cbind(y,y,y)\n  dz2 &lt;- da2 * sigmoid_derivative(a2)\n  dW2 &lt;- t(a1) %*% dz2\n  db2 &lt;- colSums(dz2)\n  \n  da1 &lt;- dz2 - cbind(y,y,y)\n  dz1 &lt;- da1 * sigmoid_derivative(a1)\n  dW1 &lt;- t(X) %*% dz1\n  db1 &lt;- colSums(dz1)\n\n  # 가중치 업데이트\n  W1 &lt;- W1 - learning_rate * dW1\n  b1 &lt;- b1 - learning_rate * db1\n\n  loss_2 &lt;- c(loss_2, loss)\n}\n\n\n\n\n\n\n\n          [,1]\n[1,] 0.6914565\n[2,] 0.6914565\n[3,] 0.6914565\n[4,] 0.6914565\n\n\n\n\n\n\nloss_data &lt;- data.frame(epoch = 1:length(loss_2), loss = loss_2)\n\nggplot(data = loss_data, aes(x = epoch, y = loss)) +\n  geom_line() +\n  labs(x = \"epoch\", y = \"loss\") +\n  theme_minimal()"
  },
  {
    "objectID": "basics_example/back_propagation.html#backpropagation",
    "href": "basics_example/back_propagation.html#backpropagation",
    "title": "Back propagtion with R",
    "section": "",
    "text": "Backpropagation은 Artificial Neural Network를 학습시키기 위한 일반적인 알고리즘 중 하나이다. 한국말로 직역하면 역전파라는 뜻인데, target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후, 그 오차값을 다시 뒤로 전파해가면서, 각 노드가 가지고 있는 변수들을 갱신하는 알고리즘인 것이다.\n\nset.seed(2023)\n\nX &lt;- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE)\ny &lt;- matrix(c(0, 1, 1, 0), ncol = 1)\n\n\nx &lt;-seq(-5,5,0.001)\n\nsigmoid &lt;- function(x){\n  1/(1+exp(-x))\n}\n\nggplot()+\n  geom_point(data = data.frame(x,sigmoid(x)), aes(x,sigmoid(x)))+\n  theme_minimal()\n\n\n\n\n\nsigmoid_derivative &lt;- function(x){\n  x*(1-x)\n}\n\n\nggplot()+\n  geom_point(data = data.frame(x,sigmoid_derivative(x)), aes(x,sigmoid_derivative(x)))+\n  theme_minimal()\n\n\n\n\n\nmse_loss &lt;- function(y_true,y_pred){\n  ((y_true - y_pred)**2) %&gt;% mean()\n}\n\n\ninput_size &lt;- 2\noutput_size &lt;- 1\n\nW1 &lt;- matrix(rnorm(input_size * output_size), nrow = input_size, ncol = output_size)\nb1 &lt;- matrix(0, nrow = 4, ncol = output_size)\n\nlearning_rate = 0.1\nepochs = 500\n\n# 초기화\nloss_2 &lt;- numeric()  # 빈 벡터로 손실을 저장\n\n# 학습 시작\nfor (epoch in 1:epochs) {\n  # Forward Pass\n  z1 &lt;- X %*% W1 + b1\n  a1 &lt;- sigmoid(z1)\n\n  # 손실 계산\n  loss &lt;- mse_loss(y, a1)\n\n  # Backward Pass\n  da1 &lt;- a1 - y\n  dz1 &lt;- da1 * a1 * (1 - a1)  # Sigmoid 함수의 미분\n  dW1 &lt;- t(X) %*% dz1\n  db1 &lt;- sum(dW1)\n\n  # 가중치 업데이트\n  W1 &lt;- W1 - learning_rate * dW1\n  b1 &lt;- b1 - learning_rate * db1\n\n  loss_2 &lt;- c(loss_2, loss)\n}\n\n# 최종 예측 출력\nprint(\"Predicted Output:\")\n\n[1] \"Predicted Output:\"\n\nprint(a1)\n\n          [,1]\n[1,] 0.6123358\n[2,] 0.5342317\n[3,] 0.5417910\n[4,] 0.4619623\n\nloss_data &lt;- data.frame(epoch = 1:length(loss_2), loss = loss_2)\n\nggplot(data = loss_data, aes(x = epoch, y = loss)) +\n  geom_line() +\n  labs(title = \"Loss plot\", x = \"epoch\", y = \"loss\") +\n  theme_minimal()"
  },
  {
    "objectID": "basics_example/back_propagation.html#backpropagation-역전파",
    "href": "basics_example/back_propagation.html#backpropagation-역전파",
    "title": "Back propagtion with R",
    "section": "",
    "text": "Backpropagation은 Artificial Neural Network를 학습시키기 위한 일반적인 알고리즘 중 하나이다. 한국말로 직역하면 역전파라는 뜻인데, target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후, 그 오차값을 다시 뒤로 전파해가면서, 각 노드가 가지고 있는 변수들을 갱신하는 알고리즘인 것이다.\n\nset.seed(2023)\n\nX &lt;- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE)\ny &lt;- matrix(c(0, 1, 1, 0), ncol = 1)\n\n\nx &lt;-seq(-5,5,0.001)\n\nsigmoid &lt;- function(x){\n  1/(1+exp(-x))\n}\n\nggplot()+\n  geom_point(data = data.frame(x,sigmoid(x)), aes(x,sigmoid(x)))+\n  theme_minimal()\n\n\n\n\n\nsigmoid_derivative &lt;- function(x){\n  x*(1-x)\n}\n\n\nggplot()+\n  geom_point(data = data.frame(x,sigmoid_derivative(x)), aes(x,sigmoid_derivative(x)))+\n  theme_minimal()\n\n\n\n\n\nmse_loss &lt;- function(y_true,y_pred){\n  ((y_true - y_pred)**2) %&gt;% mean()\n}\n\n\ninput_size &lt;- 2\noutput_size &lt;- 1\n\nW1 &lt;- matrix(rnorm(input_size * output_size), nrow = input_size, ncol = output_size)\nb1 &lt;- matrix(0, nrow = 4, ncol = output_size)\n\nlearning_rate = 0.1\nepochs = 500\n\n# 초기화\nloss_2 &lt;- numeric()  # 빈 벡터로 손실을 저장\n\n# 학습 시작\nfor (epoch in 1:epochs) {\n  # Forward Pass\n  z1 &lt;- X %*% W1 + b1\n  a1 &lt;- sigmoid(z1)\n\n  # 손실 계산\n  loss &lt;- mse_loss(y, a1)\n\n  # Backward Pass\n  da1 &lt;- a1 - y\n  dz1 &lt;- da1 * a1 * (1 - a1)  # Sigmoid 함수의 미분\n  dW1 &lt;- t(X) %*% dz1\n  db1 &lt;- sum(dW1)\n\n  # 가중치 업데이트\n  W1 &lt;- W1 - learning_rate * dW1\n  b1 &lt;- b1 - learning_rate * db1\n\n  loss_2 &lt;- c(loss_2, loss)\n}\n\n# 최종 예측 출력\nprint(\"Predicted Output:\")\n\n[1] \"Predicted Output:\"\n\nprint(a1)\n\n          [,1]\n[1,] 0.6123358\n[2,] 0.5342317\n[3,] 0.5417910\n[4,] 0.4619623\n\nloss_data &lt;- data.frame(epoch = 1:length(loss_2), loss = loss_2)\n\nggplot(data = loss_data, aes(x = epoch, y = loss)) +\n  geom_line() +\n  labs(title = \"Loss plot\", x = \"epoch\", y = \"loss\") +\n  theme_minimal()"
  },
  {
    "objectID": "basics_example/back_propagation.html#example-2",
    "href": "basics_example/back_propagation.html#example-2",
    "title": "Backpropagtion with R",
    "section": "",
    "text": "히든 레이어의 수가 2개이고, 각 레이어의 뉴런의 수가 3개인 뉴럴네트워크를 같은 형태로 구현해봅시다\n\nset.seed(2023)\n\nX &lt;- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE) # XOR 연산의 입력값\ny &lt;- matrix(c(0, 1, 1, 0), ncol = 1) # XOR 연산의 출력값\n\ninput_size = 2\nhidden_size = 3\nhidden_2_size = 3\noutput_size = 1\n\nW1 &lt;- matrix(rnorm(input_size * hidden_size), nrow = input_size, ncol = hidden_size)\nb1 &lt;- matrix(0, nrow = 4, ncol = hidden_size)\nW2 &lt;- matrix(rnorm(hidden_size * hidden_2_size), nrow = hidden_size, ncol = hidden_2_size)\nb2 &lt;- matrix(0, nrow = 4, ncol = hidden_2_size)\nW3 &lt;- matrix(rnorm(hidden_2_size * output_size), nrow = hidden_2_size, ncol = output_size)\nb3 &lt;- matrix(0, nrow = 4, ncol = output_size)\n\nlearning_rate = 0.1\nepochs = 500\n\n# 초기화\nloss_2 &lt;- numeric()  # 빈 벡터로 손실을 저장\n\n# 학습 시작\nfor (epoch in 1:epochs) {\n  # Forward Pass\n  z1 &lt;- X %*% W1 + b1\n  a1 &lt;- sigmoid(z1)\n\n  z2 &lt;- a1 %*% W2 + b2\n  a2 &lt;- sigmoid(z2)\n  \n  z3 &lt;- a2 %*% W3 + b3\n  a3 &lt;- sigmoid(z3)\n  # 손실 계산\n  loss &lt;- mse_loss(y, a3)\n\n  # Backward Pass\n  da3 &lt;- a3 - y\n  dz3 &lt;- da3 * sigmoid_derivative(a3)\n  dW3 &lt;- t(a2) %*% dz3\n  db3 &lt;- sum(dz3)\n  \n  da2 &lt;- a2 - cbind(y,y,y)\n  dz2 &lt;- da2 * sigmoid_derivative(a2)\n  dW2 &lt;- t(a1) %*% dz2\n  db2 &lt;- colSums(dz2)\n  \n  da1 &lt;- dz2 - cbind(y,y,y)\n  dz1 &lt;- da1 * sigmoid_derivative(a1)\n  dW1 &lt;- t(X) %*% dz1\n  db1 &lt;- colSums(dz1)\n\n  # 가중치 업데이트\n  W1 &lt;- W1 - learning_rate * dW1\n  b1 &lt;- b1 - learning_rate * db1\n\n  loss_2 &lt;- c(loss_2, loss)\n}\n\n\n\n\n\n\n\n          [,1]\n[1,] 0.6914565\n[2,] 0.6914565\n[3,] 0.6914565\n[4,] 0.6914565\n\n\n\n\n\n\nloss_data &lt;- data.frame(epoch = 1:length(loss_2), loss = loss_2)\n\nggplot(data = loss_data, aes(x = epoch, y = loss)) +\n  geom_line() +\n  labs(x = \"epoch\", y = \"loss\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/competiton_visualize.html",
    "href": "posts/competiton_visualize.html",
    "title": "Competiton Data Visualization",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nimport seaborn as sns\nfrom shapely.geometry import Point, Polygon\nimport matplotlib.pyplot as plt\nimport folium\nfrom folium import GeoJson, Marker"
  },
  {
    "objectID": "posts/competiton_visualize.html#packages",
    "href": "posts/competiton_visualize.html#packages",
    "title": "Competiton Data Visualization",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nimport seaborn as sns\nfrom shapely.geometry import Point, Polygon\nimport matplotlib.pyplot as plt\nimport folium\nfrom folium import GeoJson, Marker"
  },
  {
    "objectID": "posts/competiton_visualize.html#load-datasets",
    "href": "posts/competiton_visualize.html#load-datasets",
    "title": "Competiton Data Visualization",
    "section": "Load Datasets",
    "text": "Load Datasets\n\ntrain = pd.read_csv(\"competition_data/train_data_modified.csv\")\ntest = pd.read_csv(\"competition_data/test_data_modified.csv\")\nbusstop_loc = pd.read_csv(\"competition_data/getStaionByRouteAll.txt\", delimiter=\"\\t\")\ntile = gpd.read_file(\"competition_data/nlsp_020001001.shp\", encoding = \"utf-8\")\n\ntile = tile.iloc[:,[0,3]]\ntile = gpd.GeoDataFrame(tile, geometry='geometry')\ntile = tile.to_crs(epsg=4326)\ntile[['longitude', 'latitude']] = tile[\"geometry\"].centroid.astype(str).str.extract(r'POINT \\(([^ ]+) ([^ ]+)\\)', expand=True)\ntile[\"latitude\"] ,tile[\"longitude\"] = tile[\"latitude\"].astype(float),tile[\"longitude\"].astype(float)\n\nbusstop_loc.columns = ['BUSSTOP_NM', 'BUS_STOP_ID', 'lat', 'lon']\nbusstop_loc['geometry'] = [Point(lon, lat) for lon, lat in zip(busstop_loc['lon'], busstop_loc['lat'])]\nbusstop_loc = gpd.GeoDataFrame(busstop_loc, geometry=\"geometry\", crs='EPSG:4326')\nbusstop_loc = busstop_loc[(busstop_loc[\"lon\"] &gt; tile[\"longitude\"].min()) & \n                          (busstop_loc[\"lon\"] &lt; tile[\"longitude\"].max()) &\n                          (busstop_loc[\"lat\"] &gt; tile[\"latitude\"].min()) &\n                          (busstop_loc[\"lat\"] &lt; tile[\"latitude\"].max())]\nbusstop_loc[\"busstop_cnt\"] = 1\n\na = gpd.sjoin(tile,busstop_loc,how=\"left\").groupby(\"gid\").sum().reset_index()[[\"gid\",\"busstop_cnt\"]]\ntile = pd.merge(tile,a,how=\"left\",on=\"gid\")\n\n\ntrain = train.merge(tile,on = \"gid\", how = \"left\")\n\ntrain[\"TIME_sin\"] = train[\"TIME\"].apply(lambda x: abs(np.sin(x*(np.pi/23))))\n\ntrain['TIME'] = train['TIME'].apply(lambda x: str(x).zfill(2))\ntrain['DATETIME'] = pd.to_datetime(train['DATE'] + ' ' + train['TIME'].astype(str), format='%Y-%m-%d %H')\n\ntrain[\"YEAR\"] = train[\"DATE\"].apply(lambda x: int(x[0:4]))\ntrain[\"MONTH\"] = train[\"DATE\"].apply(lambda x: int(x[5:7]))\ntrain[\"DATE\"] = train[\"DATE\"].apply(lambda x: int(x[8:10]))\ntrain['TIME'] = train['TIME'].astype(int)\n\ntest[\"YEAR\"] = test[\"DATE\"].apply(lambda x: int(x[0:4]))\ntest[\"MONTH\"] = test[\"DATE\"].apply(lambda x: int(x[5:7]))\ntest[\"DATE\"] = test[\"DATE\"].apply(lambda x: int(x[8:10]))\n\ntrain = gpd.GeoDataFrame(train, geometry='geometry')\ntrain.head()\n\n/tmp/ipykernel_108278/3950838779.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  tile[['longitude', 'latitude']] = tile[\"geometry\"].centroid.astype(str).str.extract(r'POINT \\(([^ ]+) ([^ ]+)\\)', expand=True)\n/tmp/ipykernel_108278/3950838779.py:21: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  a = gpd.sjoin(tile,busstop_loc,how=\"left\").groupby(\"gid\").sum().reset_index()[[\"gid\",\"busstop_cnt\"]]\n\n\n\n\n\n\n\n\n\ngid\nDATE\nTIME\nRIDE_DEMAND\nALIGHT_DEMAND\ngeometry\nlongitude\nlatitude\nbusstop_cnt\nTIME_sin\nDATETIME\nYEAR\nMONTH\n\n\n\n\n0\n다마9599\n1\n5\n1\n0\nPOLYGON ((127.44439 36.18812, 127.44438 36.197...\n127.449949\n36.192626\n3.0\n0.631088\n2023-06-01 05:00:00\n2023\n6\n\n\n1\n다마9599\n1\n6\n2\n5\nPOLYGON ((127.44439 36.18812, 127.44438 36.197...\n127.449949\n36.192626\n3.0\n0.730836\n2023-06-01 06:00:00\n2023\n6\n\n\n2\n다마9599\n1\n7\n2\n7\nPOLYGON ((127.44439 36.18812, 127.44438 36.197...\n127.449949\n36.192626\n3.0\n0.816970\n2023-06-01 07:00:00\n2023\n6\n\n\n3\n다마9599\n1\n8\n3\n12\nPOLYGON ((127.44439 36.18812, 127.44438 36.197...\n127.449949\n36.192626\n3.0\n0.887885\n2023-06-01 08:00:00\n2023\n6\n\n\n4\n다마9599\n1\n9\n2\n31\nPOLYGON ((127.44439 36.18812, 127.44438 36.197...\n127.449949\n36.192626\n3.0\n0.942261\n2023-06-01 09:00:00\n2023\n6"
  },
  {
    "objectID": "posts/competiton_visualize.html#map",
    "href": "posts/competiton_visualize.html#map",
    "title": "Competiton Data Visualization",
    "section": "Map",
    "text": "Map\n\nm = folium.Map(location=[\"36.34697230559345\", \"127.38257946793998\"], tiles='cartodbpositron', zoom_start=10)\n\nfolium.Choropleth(\n    geo_data=tile[\"geometry\"],\n    name='busstop_cnt',\n    data=tile[\"busstop_cnt\"],\n    key_on=\"feature.id\",\n    fill_color='YlGnBu',\n).add_to(m)\n\nfolium.GeoJson(tile,\n               tooltip=folium.features.GeoJsonTooltip(fields=['gid', 'busstop_cnt'],\n                                                      aliases=['ID', 'Bus Stop Count'],\n                                                      labels=True),\n\n    style_function=lambda feature: {\n        'fillColor': 'transparent',\n        'color': 'black',\n        'weight': 2,\n        'dashArray': '5, 5',\n        'fillOpacity': 0,\n    },\n    highlight_function=lambda x: {'weight': 3, 'color': 'black'},\n).add_to(m)\n\n&lt;folium.features.GeoJson at 0x7fa8d3337910&gt;\n\n\n\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/competition_full.html#datasets",
    "href": "posts/competition_full.html#datasets",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Datasets",
    "text": "Datasets\n\nLoad Datasets\n\nAdd DATETIME datetime type\n\n\ntrain = pd.read_csv(\"competition_data/train_data_modified.csv\")\ntest = pd.read_csv(\"competition_data/test_data_modified.csv\")\n\ntrain['TIME'] = train['TIME'].apply(lambda x: str(x).zfill(2))\ntrain['DATETIME'] = pd.to_datetime(train['DATE'] + ' ' + train['TIME'].astype(str), format='%Y-%m-%d %H')\n\ntest['TIME'] = test['TIME'].apply(lambda x: str(x).zfill(2))\ntest['DATETIME'] = pd.to_datetime(test['DATE'] + ' ' + test['TIME'].astype(str), format='%Y-%m-%d %H')\n\n\n\nLoad Additional Datasets\n\nDemographic tile data\n\nReplace NA with 0\nConvert to lat-lon coordinate\nAdd centroid as numeric value\n\n\ntile = gpd.read_file(\"competition_data/nlsp_020001001.shp\", encoding = \"utf-8\")\n\ntile = tile.iloc[:,[0,2,3]]\ntile.fillna(0,inplace=True)\ntile = gpd.GeoDataFrame(tile, geometry='geometry')\ntile = tile.to_crs(epsg=4326)\ntile[['longitude', 'latitude']] = tile[\"geometry\"].centroid.astype(str).str.extract(r'POINT \\(([^ ]+) ([^ ]+)\\)', expand=True)\ntile[\"latitude\"] ,tile[\"longitude\"] = tile[\"latitude\"].astype(float),tile[\"longitude\"].astype(float)\n\n\n\nBus station location data\n\nAdd geometry Point\nExtract data inside tile data for optimization\nMerge data with tile data for bus stop count per tile\n\n\nbusstop_loc = pd.read_csv(\"competition_data/getStaionByRouteAll.txt\", delimiter=\"\\t\")\n\nbusstop_loc.columns = ['BUSSTOP_NM', 'BUS_STOP_ID', 'lat', 'lon']\nbusstop_loc['geometry'] = [Point(lon, lat) for lon, lat in zip(busstop_loc['lon'], busstop_loc['lat'])]\nbusstop_loc = gpd.GeoDataFrame(busstop_loc, geometry=\"geometry\", crs='EPSG:4326')\nbusstop_loc = busstop_loc[(busstop_loc[\"lon\"] &gt; tile[\"longitude\"].min()) & \n                          (busstop_loc[\"lon\"] &lt; tile[\"longitude\"].max()) &\n                          (busstop_loc[\"lat\"] &gt; tile[\"latitude\"].min()) &\n                          (busstop_loc[\"lat\"] &lt; tile[\"latitude\"].max())]\nbusstop_loc[\"busstop_cnt\"] = 1\n\na = gpd.sjoin(tile,busstop_loc,how=\"left\").groupby(\"gid\").sum().reset_index()[[\"gid\",\"busstop_cnt\"]]\ntile = pd.merge(tile,a,how=\"left\",on=\"gid\")"
  },
  {
    "objectID": "posts/competition_full.html#split-data",
    "href": "posts/competition_full.html#split-data",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Split data",
    "text": "Split data\n\nSplit train dataset for evaluate models\nSplit a week in July and August into test data (8:2 ratio)\n\n\ntest_set = pd.concat([train[(train[\"DATETIME\"] &gt;= \"2023-07-17 00:00:00\") & (train[\"DATETIME\"] &lt;= \"2023-07-23 23:00:00\")],\n                     train[(train[\"DATETIME\"] &gt;= \"2023-08-18 00:00:00\") & (train[\"DATETIME\"] &lt;= \"2023-08-24 23:00:00\")]])\n\ntrain_set = train.drop(test_set.index)\n\nX_train = train_set.drop([\"RIDE_DEMAND\"],axis=1)\ny_train = train_set[[\"RIDE_DEMAND\"]]\n\nX_test = test_set.drop([\"RIDE_DEMAND\"],axis=1)\ny_test = test_set[[\"RIDE_DEMAND\"]]\n\nX_train.shape, X_test.shape\n\n((431592, 5), (107898, 5))"
  },
  {
    "objectID": "posts/competition_full.html#preprocessing-standard-scaling-onehotencoding",
    "href": "posts/competition_full.html#preprocessing-standard-scaling-onehotencoding",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Preprocessing, Standard scaling & OneHotEncoding",
    "text": "Preprocessing, Standard scaling & OneHotEncoding\n\nMerge with tile information data\nConvert TIME to numeric\nAdd ‘TIME_sin’ for time periodicity\nAdd YEAR,MONTH, DATE, DAY_OF_WEEK\n\n\nclass Transformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = pd.merge(X, tile, on='gid', how='left')\n        X[\"TIME\"] = X[\"TIME\"].astype(int)\n        X[\"TIME_sin\"] = X[\"TIME\"].apply(lambda x: abs(np.sin(x*(np.pi/23))))\n        X[\"YEAR\"] = X[\"DATETIME\"].dt.year\n        X[\"MONTH\"] = X[\"DATETIME\"].dt.month\n        X[\"DATE\"] = X[\"DATETIME\"].dt.day        \n        X['DAY_OF_WEEK'] = X['DATETIME'].dt.day_name()\n        return X\n\nlist_cat = ['gid', 'DAY_OF_WEEK']\nlist_num = ['TIME', 'ALIGHT_DEMAND', 'MONTH', 'busstop_cnt', 'DATE', 'val']\n\ntransformer = Transformer()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), list_num),\n        (\"cat\", OneHotEncoder(), list_cat),\n    ],\n)\n\npipeline = Pipeline([\n    ('transformer', transformer),\n    ('preprocessor', preprocessor),\n])\n\nX_train_prep_matrix = pipeline.fit_transform(X_train)\nX_test_prep_matrix = pipeline.fit_transform(X_test)"
  },
  {
    "objectID": "posts/competition_full.html#convert-output-type",
    "href": "posts/competition_full.html#convert-output-type",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Convert output type",
    "text": "Convert output type\n\nprepared train date to fitting model\nCSR Matrix -&gt; DataFrame\n\n\nfeature_names = preprocessor.get_feature_names_out()\n\nX_train_prep_df = pd.DataFrame(X_train_prep_matrix.toarray(), columns=feature_names)\nX_test_prep_df = pd.DataFrame(X_test_prep_matrix.toarray(), columns=feature_names)\n\nX_train_prep_df.head()\n\n   num__TIME  ...  cat__DAY_OF_WEEK_Wednesday\n0  -1.178190  ...                         0.0\n1  -1.025461  ...                         0.0\n2  -0.872733  ...                         0.0\n3  -0.720005  ...                         0.0\n4  -0.567276  ...                         0.0\n\n[5 rows x 380 columns]"
  },
  {
    "objectID": "posts/competition_full.html#evaluate-models",
    "href": "posts/competition_full.html#evaluate-models",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Evaluate models",
    "text": "Evaluate models\n\nML models\n\nRandomforest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(random_state = 2023)\nforest_reg.fit(X_train_prep_df,y_train)\n\nforest_pred = forest_reg.predict(X_test_prep_df)\nforest_mae = mean_absolute_error(y_test,forest_pred)\nforest_mae\n\n\n\nXGBoost\n\nimport xgboost\n\nxgb_reg = xgboost.XGBRegressor(objective='reg:squarederror',n_estimators=100,random_state = 2023)\nxgb_reg.fit(X_train_prep_df,y_train)\n\nxgb_pred = xgb_reg.predict(X_test_prep_df)\nxgb_mae = mean_absolute_error(y_test,xgb_pred)\nxgb_mae\n\n\n\nLightGBM\n\nfrom lightgbm import LGBMRegressor\n\nlgb_reg = LGBMRegressor()\nlgb_reg.fit(X_train_prep_df, y_train)\n\nlgb_pred = lgb_reg.predict(X_test_prep_df)\nlgb_mae = mean_absolute_error(y_test,lgb_pred)\nlgb_mae\n\n\n\nCatBoost\n\nimport catboost as cb\ncb_reg = cb.CatBoostRegressor()\ncb_reg.fit(X_train_prep_df,y_train)\n\ncb_pred = cb_reg.predict(X_test_prep_df)\ncb_mae = mean_absolute_error(y_test,cb_pred)\ncb_mae\n\n\n\n\nSome DL\n\nPackages\n\nimport tensorflow as tf\nimport sklearn\nfrom tensorflow import keras\n\n\n\nModel_1 (4 Layer * 30 Neurons)\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\nnormalization_layer = tf.keras.layers.Normalization()\n\nlayer1 = tf.keras.layers.Dense(30, activation=\"relu\")\nlayer2 = tf.keras.layers.Dense(30, activation=\"relu\")\nlayer3 = tf.keras.layers.Dense(30, activation=\"relu\")\nlayer4 = tf.keras.layers.Dense(30, activation=\"relu\")\n\noutput_layer = tf.keras.layers.Dense(1)\n\n\ninput_ = tf.keras.layers.Input(shape=X_train_prep_df.shape[1:])\nnormalized = normalization_layer(input_)\n\nhidden1 = layer1(normalized)\nhidden2 = layer2(hidden1)\nhidden3 = layer3(hidden2)\nhidden4 = layer4(hidden3)\n\noutput = output_layer(hidden4)\n\nmodel_1 = tf.keras.Model(inputs=[input_], outputs=[output])\n\nmodel_1.summary()\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\nmodel_1.compile(loss=\"mae\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n\nnormalization_layer.adapt(X_train_prep_df)\n\n\nhistory_1 = model_1.fit(X_train_prep_df, y_train, epochs=100)\n\n\nmod_1_pred = model_1.predict(X_test_prep_df)\nmod_1_mae = mean_absolute_error(y_test,mod_1_pred)\nmod_1_mae\n\n\n\nModel_2 (40 - 40 - 20 - 10)\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\nnormalization_layer = tf.keras.layers.Normalization()\n\nlayer1 = tf.keras.layers.Dense(40, activation=\"relu\")\nlayer2 = tf.keras.layers.Dense(40, activation=\"relu\")\nlayer3 = tf.keras.layers.Dense(20, activation=\"relu\")\nlayer4 = tf.keras.layers.Dense(10, activation=\"relu\")\n\noutput_layer = tf.keras.layers.Dense(1)\n\n\ninput_ = tf.keras.layers.Input(shape=X_train_prep_df.shape[1:])\nnormalized = normalization_layer(input_)\n\nhidden1 = layer1(normalized)\nhidden2 = layer2(hidden1)\nhidden3 = layer3(hidden2)\nhidden4 = layer4(hidden3)\n\noutput = output_layer(hidden4)\n\nmodel_2 = tf.keras.Model(inputs=[input_], outputs=[output])\n\nmodel_2.summary()\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\nmodel_2.compile(loss=\"mae\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n\nnormalization_layer.adapt(X_train_prep_df)\n\n\nhistory_2 = model_2.fit(X_train_prep_df, y_train, epochs=100)\n\n\nmod_2_pred = model_2.predict(X_test_prep_df)\nmod_2_mae = mean_absolute_error(y_test,mod_2_pred)\nmod_2_mae\n\n\n\n\nAutoGluon\n\nPreprocessing for AutoGluon\n\nclass Transformer_ag(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = pd.merge(X, tile, on='gid', how='left')\n        X[\"TIME\"] = X[\"TIME\"].astype(int)\n        X[\"TIME_sin\"] = X[\"TIME\"].apply(lambda x: abs(np.sin(x*(np.pi/23))))\n        X[\"YEAR\"] = X[\"DATETIME\"].dt.year\n        X[\"MONTH\"] = X[\"DATETIME\"].dt.month\n        X[\"DATE\"] = X[\"DATETIME\"].dt.day        \n        X['DAY_OF_WEEK'] = X['DATETIME'].dt.day_name()\n        return X\n\ntransformer_ag = Transformer_ag()\n\ntrain_set_ag = transformer_ag.fit_transform(train_set)\ntest_set_ag = transformer_ag.fit_transform(test_set)\n\nX_train_ag = train_set_ag.drop([\"RIDE_DEMAND\"],axis=1)\ny_train_ag = train_set_ag[[\"RIDE_DEMAND\"]]\n\nX_test_ag = test_set_ag.drop([\"RIDE_DEMAND\"],axis=1)\ny_test_ag = test_set_ag[[\"RIDE_DEMAND\"]]\n\ntrain_set_ag.head()\n\n\n\nDefault parameters\n\nag_reg_default = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_default') \n\nag_reg_default.fit(train_data = train_set_ag, verbosity = 2)\n\nag_default_pred = ag_reg_default.predict(X_test_ag)\nag_default_mae = mean_absolute_error(y_test, ag_default_pred)\nag_default_mae\n\n\n\nbag_5 stack_1\n\nag_reg_5_1 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_5_1') \n\nag_reg_5_1.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 5, num_bag_sets = 1, num_stack_levels = 1)\n\nag_5_1 = ag_reg_5_1.predict(X_test_ag)\nag_5_1_mae = mean_absolute_error(y_test, ag_5_1)\nag_5_1_mae\n\n\n\nbag_7 stack_1\n\nag_reg_7_1 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_7_1') \n\nag_reg_7_1.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 7, num_bag_sets = 1, num_stack_levels = 1)\n\nag_7_1 = ag_reg_7_1.predict(X_test_ag)\nag_7_1_mae = mean_absolute_error(y_test, ag_7_1)\nag_7_1_mae\n\n\n\nbag_10 stack_1\n\nag_reg_10_1 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_10_1') \n\nag_reg_10_1.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 10, num_bag_sets = 1, num_stack_levels = 1)\n\nag_10_1 = ag_reg_10_1.predict(X_test_ag)\nag_10_1_mae = mean_absolute_error(y_test, ag_10_1)\nag_10_1_mae\n\n\n\nbag_5 stack_2\n\nag_reg_5_2 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_5_2') \n\nag_reg_5_2.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 5, num_bag_sets = 1, num_stack_levels = 2)\n\nag_5_2 = ag_reg_5_2.predict(X_test_ag)\nag_5_2_mae = mean_absolute_error(y_test, ag_5_2)\nag_5_2_mae\n\n\n\nbag_7 stack_2\n\nag_reg_7_2 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_7_2') \n\nag_reg_7_2.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 7, num_bag_sets = 1, num_stack_levels = 2)\n\nag_7_2 = ag_reg_7_2.predict(X_test_ag)\nag_7_2_mae = mean_absolute_error(y_test, ag_7_2)\nag_7_2_mae\n\n\n\nbag_10 stack_2\n\nag_reg_10_2 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_10_2') \n\nag_reg_10_2.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 10, num_bag_sets = 1, num_stack_levels = 2)\n\nag_10_2 = ag_reg_10_2.predict(X_test_ag)\nag_10_2_mae = mean_absolute_error(y_test, ag_10_2)\nag_10_2_mae\n\n\n\nbag_5 stack_3\n\nag_reg_5_3 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_5_3') \n\nag_reg_5_3.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 5, num_bag_sets = 1, num_stack_levels = 3)\n\nag_5_3 = ag_reg_5_3.predict(X_test_ag)\nag_5_3_mae = mean_absolute_error(y_test, ag_5_3)\nag_5_3_mae\n\n\n\nbag_7 stack_3\n\nag_reg_7_3 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_7_3') \n\nag_reg_7_3.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 7, num_bag_sets = 1, num_stack_levels = 3)\n\nag_7_3 = ag_reg_7_3.predict(X_test_ag)\nag_7_3_mae = mean_absolute_error(y_test, ag_7_3)\nag_7_3_mae\n\n\n\nbag_10 stack_3\n\nag_reg_10_3 = TabularPredictor(label = 'RIDE_DEMAND', problem_type = 'regression', eval_metric = 'mae', path = './ag_reg_10_3') \n\nag_reg_10_3.fit(train_data = train_set_ag, presets = 'best_quality', auto_stack = True, \n              fit_weighted_ensemble = True, verbosity = 2, \n              num_bag_folds = 10, num_bag_sets = 1, num_stack_levels = 3)\n\nag_10_3 = ag_reg_10_3.predict(X_test_ag)\nag_10_3_mae = mean_absolute_error(y_test, ag_10_3)\nag_10_3_mae"
  },
  {
    "objectID": "posts/competition_full.html#summary",
    "href": "posts/competition_full.html#summary",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Summary",
    "text": "Summary\n\n\n\nModel\nMAE (Test_set/Pred)\nfitting time\n\n\n\n\nRandomForest\n5.1073\n14m 32s\n\n\nXGBoost\n7.0586\n01m 50s\n\n\nLightGBM\n7.76932\n3.7s\n\n\nCatBoost\n6.4667\n16s\n\n\nDL Model_1\n5.3349\n20m ~\n\n\nDL Model_2\n5.2322\n20m ~\n\n\nAG_Default\n5.15\n20m ~\n\n\nAG_5_1\n#\n5H ~\n\n\nAG_7_1\n#\n5H ~\n\n\nAG_10_1\n#\n5H ~\n\n\nAG_5_2\n#\n5H ~\n\n\nAG_7_2\n#\n5H ~\n\n\nAG_10_2\n#\n5H ~\n\n\nAG_5_3\n#\n5H ~\n\n\nAG_7_3\n#\n5H ~\n\n\nAG_10_3\n#\n5H ~"
  },
  {
    "objectID": "posts/competition_full.html#predict-submission",
    "href": "posts/competition_full.html#predict-submission",
    "title": "지역사회 문제해결형 빅데이터/AI활용 공모전",
    "section": "Predict & Submission",
    "text": "Predict & Submission\n\nPredict\n\nX_sub = transformer_ag.transform(test)\n\nsub_model = TabularPredictor.load(\"model_path\")\ny_sub = sub_model.predict(X_sub)\n\n\n\nSubmission\n\nsubmission = pd.read_csv(\"competition_data/test_data_modified.csv\")\nsubmission[\"RIDE_DEMAND\"] = y_sub\n\nsubmission.to_csv(\"test_data_modified_submi.csv\",index=False)"
  }
]