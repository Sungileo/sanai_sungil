[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sungil_Park",
    "section": "",
    "text": "Hello, I am Park Sung-il, majoring in Big-data application and Statistics at Hannam University in South Korea.\n\n\nThis is a quarto blog created with Rstudio.\n\n\nCreated for publishing my project and records about me.\n\n\nSoftware Environments\n\nR 4.2.3\nPython 3.11.2\n\n\n\nHardware Environments\n\nSamsung Galaxybook 2 pro 360 i7\nDesktop\n\nRyzen 5600X\nRadeon RX570\nG.SKILL DDR4 8G 25600 CL16 *2"
  },
  {
    "objectID": "posts/detect_line/detect_line.html",
    "href": "posts/detect_line/detect_line.html",
    "title": "Traffic line detection using CV2",
    "section": "",
    "text": "1.주제 선정\n주제는 ’CV2 활용 차선 탐지’로 최근 자율주행 자동차의 눈이 되는 부분을 이미지분석 수업과정으로 배운 CV2를 활용해 구현할 수 있어 선정하였다.\n\n\n2.이미지 로드 전처리\n프로젝트에 사용한 이미지는 인천대교를 건너는 자동차의 블랙박스 영상이다.\n\n2-1. 원근변환\n\n적색 네 점을 위에서 본 시점으로 변환한다.\ngetPerspectiveTransform함수와, warpPerspective함수를 사용해 원근 변환된 이미지와, 다시 원상복귀시키기 위한 getPerspectiveTransform함수를 역으로 적용시킨 값을 출력한다.\n\n\n2-2 색 범위 탐색\n\n이미지에서 흰색 차선이 있는 곳을 찾기 위해 이미지를 HSV(색상,채도,명도) 형식으로 변환 후 흰색 구간에 부합하는 값을 출력한다.\n\n\n2-3. 관심지역 설정\n\n색 구간으로 흰색을 탐지한 결과 좌상단에 노이즈가 있는 것을 볼 수 있다. 노이즈를 없애기 위해 차선이 있는 구간을 관심지역으로 정해 관심지역 안에서만 탐지하게 한다.\n원본 이미지와 해상도가 동일한 0으로 이루어진 1차원 이미지에 fillpoly함수를 사용하여 관심구간 좌표를 흰색으로 칠한다. 후에 bitwise_and 연산을 이용해 겹치는 구간만을 출력한다.\n\n\n2-4. 흑백화, threshold 연산\n185를 기준으로 threshold 연산을 한 이미지를 출력한다.\n\n\n\n3.탐지구간 분할\n\n3-1. 차선 히스토그램\n전처리 된 1차원 이미지를 행 기준으로 더하여 출력된 리스트를 히스토그램으로 그린후, 히스토그램의 좌측과 우측에서의 최댓값을 가지는 점을 출력한다.\n\n예시 사진에서는 (270,1082) 점에서 최댓값을 가졌다.\n\n\n3-2. 구간 분할 및 탐색\n각 차선을 n개의 구간으로 나누어 좌,우로 125의 마진을 갖는 상자를 그린다. 동영상의 각 프레임에서 상자안에 드는 차선들의 평균값을 리턴한다. 리턴된 값에 Polyfit연산을 사용해 차선의 예측선을 출력한다.\n\n\n\n\n4. 결과 도출\n차선의 예측선 사이를 fillpoly함수를 통해 칠한후, 2-1에서 출력한 원근변환을 이용해 원본 크기로 돌린다. 투명도를 위해 원본 동영상에 addWeighted 연산을 통해 결과 동영상을 출력한다.\n\n\n\n5. 응용 방안\n\n차선의 곡률, 이탈률을 계산하여 차선이탈 경고시스템에 적용 가능하다. 더 나아가 고속도로처럼 차선이 명확한 부분에서는 간단한 자율주행기능에도 적용할 수 있다.\n유사한 방식으로 주차선을 탐지한다면 주차보조 시스템에적용할 수 있다.\n\n\n\n6. 보완해야 할 점\n\n커브길을 진입할 경우에 2-3에서 지정한 관심지역 밖으로 차선이 나가는 경우에는 탐지를 하지 못한다. \n원근변환, 관심지역의 값을 변경해 보완해야 함\n\n\n\n\n그림자가 있거나 갑자기 밝아지는 상황에서 탐지율이 떨어진다. 주변의 밝기를 기준으로 threshold연산을 유동적으로 하여 보완할 필요가 있다.\n\n코드:\nhttps://drive.google.com/drive/folders/1Jfc9HOaVIaTQ48MsyXwG79ePtQIfflnl?usp=sharing\n소스 출처: https://github.com/sidroopdaska/SelfDrivingCar/tree/master/AdvancedLaneLinesDetection\n영상 출처:\nhttps://youtu.be/aItuTJYMj28"
  },
  {
    "objectID": "posts/Geocomputation_apply/geocomputation_apply.html#library-packages",
    "href": "posts/Geocomputation_apply/geocomputation_apply.html#library-packages",
    "title": "Geocomputation Apply",
    "section": "Library packages",
    "text": "Library packages\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(rgdal)\n\n필요한 패키지를 로딩중입니다: sp\nPlease note that rgdal will be retired during 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2022/04/12/evolution.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-6, (SVN revision 1201)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.5.2, released 2022/09/02\nPath to GDAL shared files: C:/Users/sungi/AppData/Local/R/win-library/4.2/rgdal/gdal\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 8.2.1, January 1st, 2022, [PJ_VERSION: 821]\nPath to PROJ shared files: C:/Users/sungi/AppData/Local/R/win-library/4.2/rgdal/proj\nPROJ CDN enabled: FALSE\nLinking to sp version:1.6-0\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\nlibrary(plotly)\n\n\n다음의 패키지를 부착합니다: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(ggtext)"
  },
  {
    "objectID": "posts/Geocomputation_apply/geocomputation_apply.html#load-data",
    "href": "posts/Geocomputation_apply/geocomputation_apply.html#load-data",
    "title": "Geocomputation Apply",
    "section": "Load data",
    "text": "Load data\nFollowing data is geographic data for Chungcheongnam-do.\nhttp://data.nsdi.go.kr/dataset/15145\n\n\n\n\n\n\n\n\n\n\n\nEMD_CD\nEMD_NM\nSGG_OID\nCOL_ADM_SE\nGID\ngeometry\n\n\n\n\n읍면동 코드\n읍면동 이름\n시군구 코드\n시 코드\nGID\n지리정보\n\n\n\nConvert data type (sp -> sf)\n\ndata_sp <- readOGR(\"C:/trainsets_2/LSMD_ADM_SECT_UMD_충남/LSMD_ADM_SECT_UMD_44.shp\",encoding = \"euc-kr\")\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nOGR data source with driver: ESRI Shapefile \nSource: \"C:\\trainsets_2\\LSMD_ADM_SECT_UMD_충남\\LSMD_ADM_SECT_UMD_44.shp\", layer: \"LSMD_ADM_SECT_UMD_44\"\nwith 316 features\nIt has 5 fields\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\nWarning: GDAL support is provided by the sf and terra packages among others\n\ndata_sf = st_as_sf(data_sp)\ndata_sf %>% head()\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 911558.7 ymin: 1829456 xmax: 934941.7 ymax: 1853055\nProjected CRS: Korea 2000 / Unified CS\n    EMD_CD EMD_NM SGG_OID COL_ADM_SE GID                       geometry\n0 44800360 결성면    3817      44800 979 MULTIPOLYGON (((911891.8 18...\n1 44800350 은하면    3816      44800 980 MULTIPOLYGON (((920373.2 18...\n2 44800340 장곡면    3815      44800 981 MULTIPOLYGON (((931934 1838...\n3 44800330 홍동면    3814      44800 982 MULTIPOLYGON (((931821 1843...\n4 44800320 금마면    3813      44800 983 MULTIPOLYGON (((931724.3 18...\n5 44800256 홍북읍    3812      44800 984 MULTIPOLYGON (((931724.3 18...\n\n\n\ndata_sf %>% ggplot(aes(fill = COL_ADM_SE))+\n  geom_sf()+\n  theme_minimal()+\n  labs(title = \"충청남도\")"
  },
  {
    "objectID": "posts/Geocomputation_apply/geocomputation_apply.html#filter-place-of-interest",
    "href": "posts/Geocomputation_apply/geocomputation_apply.html#filter-place-of-interest",
    "title": "Geocomputation Apply",
    "section": "Filter Place of Interest",
    "text": "Filter Place of Interest\n\ncheonan <- data_sf %>% \n  filter(COL_ADM_SE==\"44130\")\n\ncheonan_seobuk <- cheonan %>%  \n  filter(substr(EMD_CD,1,5)==\"44133\")\n\ncheonan_dongnam <- cheonan %>%  \n  filter(substr(EMD_CD,1,5)==\"44131\")\n\n\ncheonan %>% ggplot(aes(fill=substr(EMD_CD,1,5)))+\n  geom_sf()+\n  theme_minimal()+\n  labs(title = \"천안시 (구별)\")+\n  scale_fill_discrete(name = \"구\",\n                      labels = c(\"동남구\",\"서북구\"))\n\n\n\n\n\nchsb <- cheonan_seobuk %>% ggplot(aes(fill=EMD_NM))+\n  geom_sf()+\n  geom_sf_text(mapping = aes(label = EMD_NM))+\n  labs(title = \"천안시 서북구 (읍면동)\")+\n  theme_minimal()+\n  scale_fill_discrete(name = \"읍면동\")\n\nchsb %>% ggplotly()\n\n\n\n\n\n\nchdn <- cheonan_dongnam %>% ggplot(aes(fill=EMD_NM))+\n  geom_sf()+\n  geom_sf_text(mapping = aes(label = EMD_NM))+\n  labs(title = \"천안시 동남구 (읍면동)\")+\n  theme_minimal()+\n  scale_fill_discrete(name = \"읍면동\")\n\nchdn %>% ggplotly()\n\n\n\n\n\n\ndata <- read.csv(\"C:/trainsets_2/GEOCOMPS.csv\")\ndata_sf <- data %>% st_as_sf(coords = c(\"Longitude\",\"Latitude\"))\nst_crs(data_sf) <- 4737\nst_crs(data_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4737 \n  wkt:\nGEOGCRS[\"Korea 2000\",\n    DATUM[\"Geocentric datum of Korea\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"Republic of Korea (South Korea) - onshore and offshore.\"],\n        BBOX[28.6,122.71,40.27,134.28]],\n    ID[\"EPSG\",4737]]\n\ncheonan <- cheonan %>% \n  filter(EMD_NM!=\"광덕면\")\n\ngn <- ggplot()+\n  geom_sf(data = cheonan,fill=NA)+\n  geom_sf(data = data_sf,mapping = aes(color = 건축년도))+\n  scale_color_gradient(low = \"blue\", high = \"red\")+\n  theme_minimal()\n\ngn %>% ggplotly()"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html",
    "href": "posts/Geocomputation_with_R/Geocomputation.html",
    "title": "Geocomputation with R",
    "section": "",
    "text": "#|warning: False"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#packages",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#packages",
    "title": "Geocomputation with R",
    "section": "Packages",
    "text": "Packages\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(raster)\n\n필요한 패키지를 로딩중입니다: sp\n\nlibrary(spData)\nlibrary(spDataLarge)"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#world-dataset-from-spdata-package",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#world-dataset-from-spdata-package",
    "title": "Geocomputation with R",
    "section": "World dataset from spData Package",
    "text": "World dataset from spData Package\n\nworld %>% head()\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 FJ     Fiji      Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n2 TZ     Tanzania  Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n3 EH     Western … Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n4 CA     Canada    North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n5 US     United S… North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n6 KZ     Kazakhst… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nplot(world)\n\nWarning: plotting the first 9 out of 10 attributes; use max.plot = 10 to plot\nall\n\n\n\n\n\nsp 데이터는 st_as_sf()로 sf 형식으로 변환\n\nlibrary(sp)\n\nworld_sp = as(world, Class = \"Spatial\")\n\nworld_sf = st_as_sf(world_sp)"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#plot-map",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#plot-map",
    "title": "Geocomputation with R",
    "section": "Plot map",
    "text": "Plot map\n\nplot(world[\"continent\"])\n\n\n\n\n\nst_union() 공간정보 합치기\n\nworld_asia = world[world$continent == \"Asia\", ]\nasia = st_union(world_asia)\n\nplot(world[\"pop\"], reset = FALSE)\nplot(asia, add = TRUE, col = \"red\")\n\n\n\n\n\n\nst_centroid()공간정보의 중심점 계산\ncex : symbol size = sqrt(인구수)/1000\n\nplot(world[\"continent\"], reset = FALSE)\ncex = sqrt(world$pop) / 10000  \nworld_cents = st_centroid(world, of_largest = TRUE)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nplot(st_geometry(world_cents), add = TRUE, cex = cex)\n\n\n\n\n\n\nHighlight\n\nindia = world[world$name_long == \"India\", ]\nplot(st_geometry(india), expandBB = c(0.1, 0.1, 0.1, 0.1), col = \"gray\", lwd = 3)\nplot(world_asia[0], add = TRUE)"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#geometry-types",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#geometry-types",
    "title": "Geocomputation with R",
    "section": "Geometry types",
    "text": "Geometry types\n\nst_point(c(5, 2, 3, 1)) %>% plot()\n\n\n\nmultipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2))\nst_multipoint(multipoint_matrix) %>% plot()\n\n\n\nlinestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\nst_linestring(linestring_matrix) %>% plot()\n\n\n\npolygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\nst_polygon(polygon_list) %>% plot()\n\n\n\npolygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))\npolygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))\npolygon_with_hole_list = list(polygon_border, polygon_hole)\nst_polygon(polygon_with_hole_list) %>% plot()\n\n\n\nmultilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                            rbind(c(1, 2), c(2, 4)))\nst_multilinestring((multilinestring_list)) %>% plot()\n\n\n\nmultipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))),\n                         list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))))\nst_multipolygon(multipolygon_list) %>% plot()\n\n\n\ngemetrycollection_list = list(st_multipoint(multipoint_matrix),\n                              st_linestring(linestring_matrix))\nst_geometrycollection(gemetrycollection_list) %>% plot()"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#simple-feature-columns-sfc",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#simple-feature-columns-sfc",
    "title": "Geocomputation with R",
    "section": "Simple feature columns (sfc)",
    "text": "Simple feature columns (sfc)\n\nst_sfc() 지리 특성을 하나의 컬럼 객체로 합침\npoint\n\npoint1 = st_point(c(5, 2))\npoint2 = st_point(c(1, 3))\npoints_sfc = st_sfc(point1, point2)\npoints_sfc %>% plot()\n\n\n\n\npolygon\n\npolygon_list1 = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\npolygon1 = st_polygon(polygon_list1)\npolygon_list2 = list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))\npolygon2 = st_polygon(polygon_list2)\npolygon_sfc = st_sfc(polygon1, polygon2)\npolygon_sfc %>% plot()\n\n\n\n\nmultilinestring\n\nmultilinestring_list1 = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                             rbind(c(1, 2), c(2, 4)))\nmultilinestring1 = st_multilinestring((multilinestring_list1))\nmultilinestring_list2 = list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), \n                             rbind(c(1, 7), c(3, 8)))\nmultilinestring2 = st_multilinestring((multilinestring_list2))\nmultilinestring_sfc = st_sfc(multilinestring1, multilinestring2)\nmultilinestring_sfc %>% plot()\n\n\n\n\ngeometry\n\npoint_multilinestring_sfc = st_sfc(point1, multilinestring1)\npoint_multilinestring_sfc %>% plot()\n\n\n\n\n\n\nsfc 객체는 CRS에 대한 정보를 추가로 저장할 수 있음\nCRS(coordinate reference systems, 좌표계시스템)\nEPSG : European Petroleum Survey Group, 지도 투영과 datums 에 대한 좌표계 정보 데이터베이스를 제공\n\nst_crs(points_sfc)\n\nCoordinate Reference System: NA\n\npoints_sfc_wgs = st_sfc(point1, point2, crs = 4326)\nst_crs(points_sfc_wgs)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#위치데이터-속성데이터",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#위치데이터-속성데이터",
    "title": "Geocomputation with R",
    "section": "위치데이터 + 속성데이터",
    "text": "위치데이터 + 속성데이터\nst_sf() 를 이용하여 sfc와 class sf의 객체들을 하나로 통합할 수 있음\n\nlnd_point = st_point(c(0.1, 51.5))                 # sfg object\nlnd_geom = st_sfc(lnd_point, crs = 4326)           # sfc object\nlnd_attrib = data.frame(                           # data.frame object\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n)\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)    # sf object\nlnd_sf\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.1 ymin: 51.5 xmax: 0.1 ymax: 51.5\nGeodetic CRS:  WGS 84\n    name temperature       date         geometry\n1 London          25 2017-06-21 POINT (0.1 51.5)"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#crs",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#crs",
    "title": "Geocomputation with R",
    "section": "CRS",
    "text": "CRS\n지리좌표계\n\n위,경도\n각도로거리 측정\n\n투영좌표계\n\n“평평한 표면”위의 데카르트 좌표 기반\n원점, x,y축\nm와 같은 선형 측정 단위\n\n\nCRS in R\n\nepsg코드\n\n일반적으로 짧음\n잘 정의된 좌표 시스템 하나만을 참조\n\nproj4string 정의\n\n투영 우형, 데이텀 및 타원체와 같은 매개변수를 지정할때 더 많은 유연성\n다양한 투영 우형 지정, 기존 유형 수정 가능\n\n\nst_crs()로 좌표계 조회\nst_set_crs()로 좌표계 변경\n\nvector_filepath = system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")\nnew_vector = st_read(vector_filepath)\n\nReading layer `zion' from data source \n  `C:\\Users\\sungi\\AppData\\Local\\R\\win-library\\4.2\\spDataLarge\\vector\\zion.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 302903.1 ymin: 4112244 xmax: 334735.5 ymax: 4153087\nProjected CRS: UTM Zone 12, Northern Hemisphere\n\n## st_read() : read vector dataset in R sf package\n\nst_crs(new_vector)\n\nCoordinate Reference System:\n  User input: UTM Zone 12, Northern Hemisphere \n  wkt:\nBOUNDCRS[\n    SOURCECRS[\n        PROJCRS[\"UTM Zone 12, Northern Hemisphere\",\n            BASEGEOGCRS[\"GRS 1980(IUGG, 1980)\",\n                DATUM[\"unknown\",\n                    ELLIPSOID[\"GRS80\",6378137,298.257222101,\n                        LENGTHUNIT[\"metre\",1,\n                            ID[\"EPSG\",9001]]]],\n                PRIMEM[\"Greenwich\",0,\n                    ANGLEUNIT[\"degree\",0.0174532925199433]]],\n            CONVERSION[\"UTM zone 12N\",\n                METHOD[\"Transverse Mercator\",\n                    ID[\"EPSG\",9807]],\n                PARAMETER[\"Latitude of natural origin\",0,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8801]],\n                PARAMETER[\"Longitude of natural origin\",-111,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8802]],\n                PARAMETER[\"Scale factor at natural origin\",0.9996,\n                    SCALEUNIT[\"unity\",1],\n                    ID[\"EPSG\",8805]],\n                PARAMETER[\"False easting\",500000,\n                    LENGTHUNIT[\"Meter\",1],\n                    ID[\"EPSG\",8806]],\n                PARAMETER[\"False northing\",0,\n                    LENGTHUNIT[\"Meter\",1],\n                    ID[\"EPSG\",8807]],\n                ID[\"EPSG\",16012]],\n            CS[Cartesian,2],\n                AXIS[\"(E)\",east,\n                    ORDER[1],\n                    LENGTHUNIT[\"Meter\",1]],\n                AXIS[\"(N)\",north,\n                    ORDER[2],\n                    LENGTHUNIT[\"Meter\",1]]]],\n    TARGETCRS[\n        GEOGCRS[\"WGS 84\",\n            DATUM[\"World Geodetic System 1984\",\n                ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                    LENGTHUNIT[\"metre\",1]]],\n            PRIMEM[\"Greenwich\",0,\n                ANGLEUNIT[\"degree\",0.0174532925199433]],\n            CS[ellipsoidal,2],\n                AXIS[\"latitude\",north,\n                    ORDER[1],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n                AXIS[\"longitude\",east,\n                    ORDER[2],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n            ID[\"EPSG\",4326]]],\n    ABRIDGEDTRANSFORMATION[\"Transformation from GRS 1980(IUGG, 1980) to WGS84\",\n        METHOD[\"Position Vector transformation (geog2D domain)\",\n            ID[\"EPSG\",9606]],\n        PARAMETER[\"X-axis translation\",0,\n            ID[\"EPSG\",8605]],\n        PARAMETER[\"Y-axis translation\",0,\n            ID[\"EPSG\",8606]],\n        PARAMETER[\"Z-axis translation\",0,\n            ID[\"EPSG\",8607]],\n        PARAMETER[\"X-axis rotation\",0,\n            ID[\"EPSG\",8608]],\n        PARAMETER[\"Y-axis rotation\",0,\n            ID[\"EPSG\",8609]],\n        PARAMETER[\"Z-axis rotation\",0,\n            ID[\"EPSG\",8610]],\n        PARAMETER[\"Scale difference\",1,\n            ID[\"EPSG\",8611]]]]\n\n\nRaster 데이터에서 좌표계\nprojection() 함수로 확인하거나 설정\n\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nnew_raster = raster(raster_filepath) \nprojection(new_raster)\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n변경\n\nnew_raster3  <-  new_raster\nprojection(new_raster3) <-  \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 \n                            +units=m +no_defs\""
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#단위",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#단위",
    "title": "Geocomputation with R",
    "section": "단위",
    "text": "단위\nsf 객체 내에 단위가 들어가있음\nst_area() [m^2] 단위 같이 반환\nset_units(st_object,units) 로 반환 단위 설정\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nluxembourg = world[world$name_long == \"Luxembourg\", ]\nsouth_korea = world[world$name_long == \"Republic of Korea\", ] \n\n\nst_area(luxembourg)\n\n2408817306 [m^2]\n\nst_area(south_korea)\n\n99020196082 [m^2]\n\n\nRaster는 단위에 대한 속성 정보가 없음"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#벡터-속성-조작",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#벡터-속성-조작",
    "title": "Geocomputation with R",
    "section": "벡터 속성 조작",
    "text": "벡터 속성 조작\n\nlibrary(tidyr)\n\n\n다음의 패키지를 부착합니다: 'tidyr'\n\n\nThe following object is masked from 'package:raster':\n\n    extract\n\nlibrary(dplyr)\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:raster':\n\n    intersect, select, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nst_drop_geometry() sf객체에서 속성 데이터만 가져오기\n\ndim(world)\n\n[1] 177  11\n\nworld_df = st_drop_geometry(world)\nclass(world_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(world_df)\n\n[1] 177  10\n\n\n\nBase R, dplyr 구문으로 조작\n\nworld %>% \n  filter(continent == \"Asia\") %>% \n  select(name_long) %>% \n  plot()\n\n\n\n\n\nworld_agg1  <-  aggregate(pop ~ continent, FUN = sum, data = world, na.rm = TRUE)\nworld_agg1\n\n      continent        pop\n1        Africa 1154946633\n2          Asia 4311408059\n3        Europe  669036256\n4 North America  565028684\n5       Oceania   37757833\n6 South America  412060811\n\nstr(world_agg1)\n\n'data.frame':   6 obs. of  2 variables:\n $ continent: chr  \"Africa\" \"Asia\" \"Europe\" \"North America\" ...\n $ pop      : num  1.15e+09 4.31e+09 6.69e+08 5.65e+08 3.78e+07 ...\n\nclass(world_agg1)\n\n[1] \"data.frame\"\n\n\naggregate()함수를 사용해서 속성 데이터를 그룹별로 집계\n\nworld$pop 은 sf객체가 아닌 숫자형 객체, geometry 정보는 없음\n\n\nworld_agg2  <-  aggregate(world[\"pop\"], by = list(world$continent),\n                       FUN = sum, na.rm = TRUE)\nclass(world_agg2)\n\n[1] \"sf\"         \"data.frame\"\n\nclass(world['pop'])\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(world$pop)\n\n[1] \"numeric\"\n\n\ngroup_by() 사용\n\nworld_agg3 <- world %>% \n  group_by(continent) %>% \n  summarize(pop = sum(pop,na.rm = TRUE),n_countries = n())\n\nworld_agg3\n\nSimple feature collection with 8 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 8 × 4\n  continent                      pop n_countries                            geom\n  <chr>                        <dbl>       <int>                  <GEOMETRY [°]>\n1 Africa                  1154946633          51 MULTIPOLYGON (((36.86623 22, 3…\n2 Antarctica                       0           1 MULTIPOLYGON (((-180 -89.9, 18…\n3 Asia                    4311408059          47 MULTIPOLYGON (((36.14976 35.82…\n4 Europe                   669036256          39 MULTIPOLYGON (((26.29 35.29999…\n5 North America            565028684          18 MULTIPOLYGON (((-82.26815 23.1…\n6 Oceania                   37757833           7 MULTIPOLYGON (((166.7932 -15.6…\n7 Seven seas (open ocean)          0           1 POLYGON ((68.935 -48.625, 68.8…\n8 South America            412060811          13 MULTIPOLYGON (((-66.95992 -54.…\n\n\n인구 많은 3개 대륙\n\nworld %>% \n  group_by(continent) %>% \n  summarize(pop = sum(pop, na.rm = TRUE), n_countries = n()) %>% \n  top_n(n = 3, wt = pop) %>%\n  arrange(desc(pop)) %>%\n  plot()\n\n\n\n\n\n\njoin 가능\n\nworld_coffee <-  left_join(world, coffee_data)\n\nJoining with `by = join_by(name_long)`\n\nclass(world_coffee)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_coffee)\n\n [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n [4] \"region_un\"              \"subregion\"              \"type\"                  \n [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n[10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n[13] \"coffee_production_2017\"\n\nplot(world_coffee[\"coffee_production_2017\"])\n\n\n\n\nsetdiff()로 일치하지 않는 열 식별\n\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\"             \n\n\nstringr 패키지의 str_subset()\n\nlibrary(stringr)\nstr_subset(world$name_long, \"Dem*.+Congo\")\n\n[1] \"Democratic Republic of the Congo\"\n\n\n\ncoffee_data$name_long[grepl(\"Congo,\", coffee_data$name_long)]  <-  \n  str_subset(world$name_long, \"Dem*.+Congo\")\n\nworld_coffee_match <- inner_join(world, coffee_data)\n\nJoining with `by = join_by(name_long)`\n\nnrow(world_coffee_match)\n\n[1] 46\n\n\n\n\n새로운 열 만들기\nbase R, mutate(), transmute() 구문 사용\n\nworld %>% transmute(pop_dens = pop / area_km2) %>% head()\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 2\n  pop_dens                                                                  geom\n     <dbl>                                                    <MULTIPOLYGON [°]>\n1    45.9  (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -16.02088, -180 -1…\n2    56.0  (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.01455, 30.4191 -1.…\n3    NA    (((-8.66559 27.65643, -8.817828 27.65643, -8.794884 27.1207, -9.4130…\n4     3.54 (((-132.71 54.04001, -133.18 54.16998, -133.2397 53.85108, -133.0546…\n5    33.5  (((-171.7317 63.78252, -171.7911 63.40585, -171.5531 63.31779, -170.…\n6     6.33 (((87.35997 49.21498, 86.82936 49.82667, 85.54127 49.69286, 85.11556…\n\n\nunite() 사용 열 합치기\nseperate() 사용 열 분리\n\nworld %>%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE) %>% head()\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 10\n  iso_a2 name_long      con_reg    subre…¹ type  area_…²     pop lifeExp gdpPe…³\n  <chr>  <chr>          <chr>      <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 FJ     Fiji           Oceania:O… Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n2 TZ     Tanzania       Africa:Af… Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n3 EH     Western Sahara Africa:Af… Northe… Inde…  9.63e4 NA         NA       NA \n4 CA     Canada         North Ame… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n5 US     United States  North Ame… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n6 KZ     Kazakhstan     Asia:Asia  Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​subregion, ²​area_km2, ³​gdpPercap\n\nworld_unite <- world %>%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = FALSE)\n\n\nworld_unite %>% \n  separate(\"con_reg\", c(\"continent\", \"region_un\"), sep = \":\") %>% head()\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 FJ     Fiji      Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n2 TZ     Tanzania  Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n3 EH     Western … Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n4 CA     Canada    North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n5 US     United S… North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n6 KZ     Kazakhst… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\n\nrename(), setNames() 가능"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#raster-속성-조작",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#raster-속성-조작",
    "title": "Geocomputation with R",
    "section": "Raster 속성 조작",
    "text": "Raster 속성 조작\nR의 레스터 객체(raster objects)는 데이터 속성으로 숫자형(numeric), 정수형(integer), 논리형(logical), 요인형(factor) 데이터 유형을 지원하며, 문자형(character)은 지원하지 않음\n문자형으로 이루어진 범주형 변수 값(categorical variables’ values)을 가지고 레스터 객체의 속성을 만들고 싶으면\n\n먼저 문자형을 요인형으로 변환 (또는 논리형으로 변환)하고\n요인형 값을 속성 값으로 해서 레스터 객체를 만듬\n\n\nelev <- raster(nrows = 6, # integer > 0. Number of rows \n               ncols = 6, # integer > 0. Number of columns \n               #res = 0.5, # numeric vector of length 1 or 2 to set the resolution \n               xmn = -1.5, # minimum x coordinate (left border) \n               xmx = 1.5, # maximum x coordinate (right border) \n               ymn = -1.5, # minimum y coordinate (bottom border) \n               ymx = 1.5, # maximum y coordinate (top border) \n               vals = 1:36) # values for the new RasterLayer \n\nelev\n\nclass      : RasterLayer \ndimensions : 6, 6, 36  (nrow, ncol, ncell)\nresolution : 0.5, 0.5  (x, y)\nextent     : -1.5, 1.5, -1.5, 1.5  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 36  (min, max)\n\nplot(elev, main = 'raster datasets with numeric valeus')\n\n\n\n\nfactor 형식으로 변환\n\ngrain_order <- c(\"clay\", \"silt\", \"sand\")\ngrain_char <- sample(grain_order, 36, replace = TRUE)\ngrain_fact <- factor(grain_char, levels = grain_order)\ngrain <- raster(nrows = 6, ncols = 6, res = 0.5, \n               xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,\n               vals = grain_fact)\nplot(grain)\n\n\n\n\nfactor 추가\n\nlevels(grain)[[1]] <- cbind(levels(grain)[[1]], wetness = c(\"wet\", \"moist\", \"dry\"))\nlevels(grain)\n\n[[1]]\n  ID VALUE wetness\n1  1  clay     wet\n2  2  silt   moist\n3  3  sand     dry\n\ngrain[c(1, 11, 35)]\n\n[1] 3 2 3\n\nfactorValues(grain, grain[c(1, 11, 35)])\n\n  VALUE wetness\n1  sand     dry\n2  silt   moist\n3  sand     dry"
  },
  {
    "objectID": "posts/Geocomputation_with_R/Geocomputation.html#공간-부분-집합",
    "href": "posts/Geocomputation_with_R/Geocomputation.html#공간-부분-집합",
    "title": "Geocomputation with R",
    "section": "공간 부분 집합",
    "text": "공간 부분 집합\nst_disjoint는 sf 패키지에 포함된 함수로, 스페이스 객체끼리 겹치지 않는 부분을 반환하는 함수.\n따라서, canterbury와 공간적으로 겹치지 않는 부분에 대해서만 nz_height 객체에서 값을 선택\n\ncanterbury  <-  nz %>% filter(Name == \"Canterbury\")\ncanterbury_height <-  nz_height[canterbury, ]\n\nnz_height[canterbury, 2, op = st_disjoint]\n\nSimple feature collection with 31 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1204143 ymin: 5048309 xmax: 1822492 ymax: 5650492\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\nFirst 10 features:\n   elevation                geometry\n1       2723 POINT (1204143 5049971)\n2       2820 POINT (1234725 5048309)\n3       2830 POINT (1235915 5048745)\n4       3033 POINT (1259702 5076570)\n12      2759 POINT (1373264 5175442)\n25      2756 POINT (1374183 5177165)\n26      2800 POINT (1374469 5176966)\n27      2788 POINT (1375422 5177253)\n46      2782 POINT (1383006 5181085)\n47      2905 POINT (1383486 5181270)\n\nplot(nz_height[canterbury, 2, op = st_disjoint])\n\n\n\n\nst_intersects() 를 활용한 공간 부분 집합 추출\n\nsel_sgbp <-  st_intersects(x = nz_height, y = canterbury)\n\nsel_logical  <-  lengths(sel_sgbp) > 0\n\ncanterbury_height2 <-  nz_height[sel_logical, ]\n\ncanterbury_height3  <-  nz_height %>%\n  filter(st_intersects(x = ., y = canterbury, sparse = FALSE))\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\nℹ The deprecated feature was likely used in the dplyr package.\n  Please report the issue at <https://github.com/tidyverse/dplyr/issues>.\n\nclass(sel_sgbp)\n\n[1] \"sgbp\" \"list\"\n\nst_intersects(x = nz_height, y = canterbury)\n\nSparse geometry binary predicate list of length 101, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 1\n 6: 1\n 7: 1\n 8: 1\n 9: 1\n 10: 1\n\n\n\n위상관계\n\n# create a polygon\na_poly  <-  st_polygon(list(rbind(c(-1, -1), c(1, -1), c(1, 1), c(-1, -1))))\na <-   st_sfc(a_poly)\n# create a line\nl_line <-   st_linestring(x = matrix(c(-1, -1, -0.5, 1), ncol = 2))\nl <-   st_sfc(l_line)\n# create points\np_matrix <- matrix(c(0.5, 1, -1, 0, 0, 1, 0.5, 1), ncol = 2)\np_multi <- st_multipoint(x = p_matrix)\np <- st_cast(st_sfc(p_multi), \"POINT\")\n\nplot(a, col = c(\"gray\"), border = c(\"red\"))\nplot(l,add = T)\nplot(p,add = T)\nbox(col=\"black\")\n\naxis(side = 1, at = seq(-1.0, 1.0, 0.5), tck = 0.02)\naxis(side = 2, at = seq(-1, 1, 0.5), tck = 0.02, las=1)\ntext(p_matrix,pos=1)\n\n\n\n\npolygon과 point의 겹치는 부분을 반환\nst_intersects() 겹치는 부분\n\nst_intersects(p, a)\n\nSparse geometry binary predicate list of length 4, where the predicate\nwas `intersects'\n 1: 1\n 2: 1\n 3: (empty)\n 4: (empty)\n\nst_intersects(p, a,sparse = F)[,1]\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nst_within() 완전히 위에 있는 부분\n\nst_within(p, a, sparse = FALSE)[, 1]\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nst_touches() 테두리만 반환\n\nst_touches(p, a, sparse = FALSE)[, 1]\n\n[1] FALSE  TRUE FALSE FALSE\n\n\nst_is_within_distance() 는 삼각형에서 주어진 거리보다 가까운 객체들을 반환\n\nst_is_within_distance(p, a, dist = 0.9,sparse = F)\n\n      [,1]\n[1,]  TRUE\n[2,]  TRUE\n[3,] FALSE\n[4,]  TRUE"
  },
  {
    "objectID": "posts/Linear_model/Linear_model.html#패키지-로드",
    "href": "posts/Linear_model/Linear_model.html#패키지-로드",
    "title": "Linear model",
    "section": "패키지 로드",
    "text": "패키지 로드\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(car)\n\n필요한 패키지를 로딩중입니다: carData\n\n다음의 패키지를 부착합니다: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded"
  },
  {
    "objectID": "posts/Linear_model/Linear_model.html#데이터",
    "href": "posts/Linear_model/Linear_model.html#데이터",
    "title": "Linear model",
    "section": "데이터",
    "text": "데이터\n\n위도, 경도 데이터 제거\n컬럼명 영문 변환\n\n\ndata <- read_excel(\"C:/linear_model_dataset_R.xlsx\")\ndata <- data %>% select(-c(Latitude,Longitude))\nnames(data) <- c(\"year_built\",\"floor\",\"station_m\",\"pop\",\"dong_area\",\"foreign_ratio\",\"school_m\",\"market_m\",\"price_per_pyeong\")\n\n\n\n\n한\n영\n\n\n\n\n건축년도\nyear_built\n\n\n층\nfloor\n\n\n근처 역까지의 거리\nstation_m\n\n\n법정동 인구수\npop\n\n\n법정동크기\ndong_area\n\n\n외국인 비율\nforeign_ratio\n\n\n근처 초등학교까지의 거리\nschool_m\n\n\n근처 마트까지의 거리\nmarket_m\n\n\n평당 금액\nprice_per_pyeong\n\n\n\n\ndata %>% head()\n\n# A tibble: 6 × 9\n  year_built floor station_m   pop dong_area foreign_r…¹ schoo…² marke…³ price…⁴\n       <dbl> <dbl>     <dbl> <dbl>     <dbl>       <dbl>   <dbl>   <dbl>   <dbl>\n1       2015     9      324.  8191      0.61      0.0603    478.    1198    923.\n2       2015     9      324.  8191      0.61      0.0603    478.    1198    938.\n3       2015     9      324.  8191      0.61      0.0603    478.    1198    923.\n4       2022    20      451.  8191      0.61      0.0603    460.    1149   1361.\n5       2022    12      451.  8191      0.61      0.0603    460.    1149   1361.\n6       2022    11      451.  8191      0.61      0.0603    460.    1149   1295.\n# … with abbreviated variable names ¹​foreign_ratio, ²​school_m, ³​market_m,\n#   ⁴​price_per_pyeong"
  },
  {
    "objectID": "posts/Linear_model/Linear_model.html#상관행렬",
    "href": "posts/Linear_model/Linear_model.html#상관행렬",
    "title": "Linear model",
    "section": "상관행렬",
    "text": "상관행렬\n\ndata_cor <- cor(data)\ndata_cor\n\n                 year_built      floor  station_m        pop  dong_area\nyear_built        1.0000000  0.4452263 -0.1517927  0.5040012 -0.3377168\nfloor             0.4452263  1.0000000 -0.1499609  0.2791992 -0.2425241\nstation_m        -0.1517927 -0.1499609  1.0000000 -0.2742544  0.6625711\npop               0.5040012  0.2791992 -0.2742544  1.0000000 -0.4440357\ndong_area        -0.3377168 -0.2425241  0.6625711 -0.4440357  1.0000000\nforeign_ratio    -0.1367913 -0.1164478  0.2242779 -0.4610396  0.4211463\nschool_m         -0.2039064 -0.1784708  0.4277893 -0.3383860  0.5042992\nmarket_m         -0.3682454 -0.2615120  0.7017695 -0.5304463  0.7791119\nprice_per_pyeong  0.7890349  0.4754674 -0.2741590  0.6661318 -0.4809320\n                 foreign_ratio   school_m   market_m price_per_pyeong\nyear_built          -0.1367913 -0.2039064 -0.3682454        0.7890349\nfloor               -0.1164478 -0.1784708 -0.2615120        0.4754674\nstation_m            0.2242779  0.4277893  0.7017695       -0.2741590\npop                 -0.4610396 -0.3383860 -0.5304463        0.6661318\ndong_area            0.4211463  0.5042992  0.7791119       -0.4809320\nforeign_ratio        1.0000000  0.4879119  0.4624987       -0.2947559\nschool_m             0.4879119  1.0000000  0.6046044       -0.3129903\nmarket_m             0.4624987  0.6046044  1.0000000       -0.4731039\nprice_per_pyeong    -0.2947559 -0.3129903 -0.4731039        1.0000000\n\ncorrplot(data_cor,\n         method = \"shade\",\n         addCoef.col=\"black\",\n         tl.col = \"black\")"
  },
  {
    "objectID": "posts/Linear_model/Linear_model.html#선형회귀-모델",
    "href": "posts/Linear_model/Linear_model.html#선형회귀-모델",
    "title": "Linear model",
    "section": "선형회귀 모델",
    "text": "선형회귀 모델\n\nmodel <- lm(price_per_pyeong ~ year_built+floor+station_m+pop+dong_area+foreign_ratio+school_m + market_m, \n            data = data)\n\nsummary(model)\n\n\nCall:\nlm(formula = price_per_pyeong ~ year_built + floor + station_m + \n    pop + dong_area + foreign_ratio + school_m + market_m, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-846.41 -177.59   -1.48  137.05 1399.25 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -6.404e+04  2.324e+03 -27.555  < 2e-16 ***\nyear_built     3.225e+01  1.166e+00  27.652  < 2e-16 ***\nfloor          9.110e+00  1.327e+00   6.866 1.13e-11 ***\nstation_m     -8.977e-03  5.920e-03  -1.516 0.129720    \npop            9.438e-03  6.397e-04  14.754  < 2e-16 ***\ndong_area     -5.106e+00  8.185e-01  -6.238 6.41e-10 ***\nforeign_ratio -2.985e+02  3.057e+02  -0.977 0.329008    \nschool_m      -3.629e-02  2.370e-02  -1.531 0.126009    \nmarket_m       2.995e-02  8.922e-03   3.356 0.000818 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 285.9 on 1050 degrees of freedom\nMultiple R-squared:  0.7504,    Adjusted R-squared:  0.7485 \nF-statistic: 394.5 on 8 and 1050 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/Linear_model/Linear_model.html#회귀진단",
    "href": "posts/Linear_model/Linear_model.html#회귀진단",
    "title": "Linear model",
    "section": "회귀진단",
    "text": "회귀진단\n\nplot(model)"
  },
  {
    "objectID": "posts/Linear_model/Linear_model.html#변수선택",
    "href": "posts/Linear_model/Linear_model.html#변수선택",
    "title": "Linear model",
    "section": "변수선택",
    "text": "변수선택\n\n후진제거법\n\nmodel_2 <- step(model,direction = \"backward\")\n\nStart:  AIC=11987.65\nprice_per_pyeong ~ year_built + floor + station_m + pop + dong_area + \n    foreign_ratio + school_m + market_m\n\n                Df Sum of Sq       RSS   AIC\n- foreign_ratio  1     77956  85906426 11987\n<none>                        85828470 11988\n- station_m      1    187961  86016431 11988\n- school_m       1    191661  86020131 11988\n- market_m       1    920800  86749270 11997\n- dong_area      1   3180881  89009351 12024\n- floor          1   3853265  89681735 12032\n- pop            1  17792408 103620878 12185\n- year_built     1  62501062 148329532 12565\n\nStep:  AIC=11986.61\nprice_per_pyeong ~ year_built + floor + station_m + pop + dong_area + \n    school_m + market_m\n\n             Df Sum of Sq       RSS   AIC\n- station_m   1    151136  86057562 11986\n<none>                     85906426 11987\n- school_m    1    296692  86203118 11988\n- market_m    1    873033  86779458 11995\n- dong_area   1   3398846  89305272 12026\n- floor       1   3843713  89750139 12031\n- pop         1  20606383 106512809 12212\n- year_built  1  63903206 149809632 12574\n\nStep:  AIC=11986.47\nprice_per_pyeong ~ year_built + floor + pop + dong_area + school_m + \n    market_m\n\n             Df Sum of Sq       RSS   AIC\n<none>                     86057562 11986\n- school_m    1    290963  86348524 11988\n- market_m    1    721900  86779462 11993\n- floor       1   3858576  89916138 12031\n- dong_area   1   4146877  90204439 12034\n- pop         1  20481223 106538785 12211\n- year_built  1  64152329 150209891 12574\n\n\n\n\n후진 제거법을 적용한 모델델\n\nmodel_2 %>% summary\n\n\nCall:\nlm(formula = price_per_pyeong ~ year_built + floor + pop + dong_area + \n    school_m + market_m, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-840.97 -171.39   -1.21  136.01 1398.31 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.324e+04  2.268e+03 -27.878  < 2e-16 ***\nyear_built   3.184e+01  1.137e+00  28.004  < 2e-16 ***\nfloor        9.115e+00  1.327e+00   6.868 1.11e-11 ***\npop          9.536e-03  6.026e-04  15.823  < 2e-16 ***\ndong_area   -5.533e+00  7.771e-01  -7.120 2.00e-12 ***\nschool_m    -4.271e-02  2.264e-02  -1.886  0.05958 .  \nmarket_m     2.398e-02  8.074e-03   2.971  0.00304 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 286 on 1052 degrees of freedom\nMultiple R-squared:  0.7497,    Adjusted R-squared:  0.7483 \nF-statistic: 525.1 on 6 and 1052 DF,  p-value: < 2.2e-16\n\n\n\n\n회귀진단\n\nplot(model_2)"
  },
  {
    "objectID": "posts/penguin/penguin.html#펭귄의-종-서식지-부리크기-날개크기-성별을-통해-몸무게를-예측하는-딥러닝-회귀-모델",
    "href": "posts/penguin/penguin.html#펭귄의-종-서식지-부리크기-날개크기-성별을-통해-몸무게를-예측하는-딥러닝-회귀-모델",
    "title": "Tensorflow ex",
    "section": "펭귄의 종, 서식지, 부리크기, 날개크기, 성별을 통해 몸무게를 예측하는 딥러닝 회귀 모델",
    "text": "펭귄의 종, 서식지, 부리크기, 날개크기, 성별을 통해 몸무게를 예측하는 딥러닝 회귀 모델"
  },
  {
    "objectID": "posts/penguin/penguin.html#데이터-불러오기",
    "href": "posts/penguin/penguin.html#데이터-불러오기",
    "title": "Tensorflow ex",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\nseaborn 패키지에 포함된 “penguins” 데이터셋 사용\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")          # data 불러오기\npenguins = penguins.dropna()     \npenguins.head()\n\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n5  Adelie  Torgersen            39.3  ...              190.0       3650.0    Male\n\n[5 rows x 7 columns]\n\n\n데이터 정보\n\npenguins.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 333 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    float64\n 5   body_mass_g        333 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 20.8+ KB\n\n\nspecies 별 평균치\n\npenguins.groupby('species').mean()\n\n           bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nspecies                                                                 \nAdelie          38.823973      18.347260         190.102740  3706.164384\nChinstrap       48.833824      18.420588         195.823529  3733.088235\nGentoo          47.568067      14.996639         217.235294  5092.436975\n\n<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\ntensorflow 설치 & 임포트\n\n#!pip install tensorflow\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers"
  },
  {
    "objectID": "posts/penguin/penguin.html#전처리",
    "href": "posts/penguin/penguin.html#전처리",
    "title": "Tensorflow ex",
    "section": "전처리",
    "text": "전처리\n카테고리형 변수인 species, island, sex 를 더미 변수화를 통해 0,1로 변환\n\npenguins_cat = pd.get_dummies(penguins,columns=['species','island','sex'])\npenguins_cat.head()\n\n   bill_length_mm  bill_depth_mm  ...  sex_Female  sex_Male\n0            39.1           18.7  ...           0         1\n1            39.5           17.4  ...           1         0\n2            40.3           18.0  ...           1         0\n4            36.7           19.3  ...           1         0\n5            39.3           20.6  ...           0         1\n\n[5 rows x 12 columns]\n\n\n성능 검증을 위한 train_set, test_set로 분리\n\ntrain_set = penguins_cat.sample(frac=0.8, random_state=9)\ntest_set = penguins_cat.drop(train_set.index)\n\nlen(train_set),len(test_set)\n\n(266, 67)\n\n\ntrain_set, test_set 에서 종속변수 body_mass_g를 분리해 X,y로 만들기\n\nX_train = train_set.drop(['body_mass_g'],axis = 1)\ny_train = train_set['body_mass_g']\nX_test = test_set.drop(['body_mass_g'],axis = 1)\ny_test = test_set['body_mass_g']\n\n\nX_train.head()\n\n     bill_length_mm  bill_depth_mm  ...  sex_Female  sex_Male\n232            45.5           13.7  ...           1         0\n149            37.8           18.1  ...           0         1\n41             40.8           18.4  ...           0         1\n278            43.2           14.5  ...           1         0\n283            54.3           15.7  ...           0         1\n\n[5 rows x 11 columns]\n\n\n데이터를 정규화시켜주는 레이어 normalizer\n\nnormalizer = tf.keras.layers.Normalization(axis=-1)\n\nnormalizer.adapt(np.array(X_train))\n\nprint(normalizer.mean.numpy())\n\n[[4.4052250e+01 1.7237219e+01 2.0085341e+02 4.4360906e-01 2.1052630e-01\n  3.4586465e-01 4.7368425e-01 3.9097744e-01 1.3533835e-01 4.8496246e-01\n  5.1503760e-01]]\n\n\n첫번째 행 데이터와 정규화된 데이터 비교\n\nfirst = np.array(X_train[:1])\n\nwith np.printoptions(precision=2, suppress=True):\n    print('First example:', first)\n    print()\n    print('Normalized:', normalizer(first).numpy())\n\nFirst example: [[ 45.5  13.7 214.    0.    0.    1.    1.    0.    0.    1.    0. ]]\n\nNormalized: [[ 0.26 -1.85  0.93 -0.89 -0.52  1.38  1.05 -0.8  -0.4   1.03 -1.03]]"
  },
  {
    "objectID": "posts/penguin/penguin.html#모델-생성-함수",
    "href": "posts/penguin/penguin.html#모델-생성-함수",
    "title": "Tensorflow ex",
    "section": "모델 생성 함수",
    "text": "모델 생성 함수\n1번 레이어는 정규화 레이어\n각 레이어의 퍼셉트론 개수, 활성함수 지정\n마지막 레이어는 1개 퍼셉트론으로 최종 예측값 출력\n\ndef build_and_compile_model(norm):\n    model = keras.Sequential([\n          norm,\n          layers.Dense(64, activation='relu'),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(8, activation='relu'),\n        layers.Dense(1)\n      ])\n    \n    model.compile(loss='mean_absolute_error',\n                optimizer=tf.keras.optimizers.Adam(0.001),\n                 metrics = ['mae','mse'])\n    return model\n  \n\n\n모델 생성\n\ndnn_model = build_and_compile_model(normalizer)\ndnn_model.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n normalization (Normalizatio  (None, 11)               23        \n n)                                                              \n                                                                 \n dense (Dense)               (None, 64)                768       \n                                                                 \n dense_1 (Dense)             (None, 32)                2080      \n                                                                 \n dense_2 (Dense)             (None, 16)                528       \n                                                                 \n dense_3 (Dense)             (None, 8)                 136       \n                                                                 \n dense_4 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 3,544\nTrainable params: 3,521\nNon-trainable params: 23\n_________________________________________________________________\n\n\n\n\n학습\nEPOCH = 학습횟수\n\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('')\n        print('.', end='')\n  \nEPOCHS = 1000\n\nhistory = dnn_model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    verbose=0,epochs=EPOCHS,callbacks=[PrintDot()])\n\n\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n\n\n\n\n학습 결과\n\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()\n\n           loss         mae           mse  ...     val_mae        val_mse  epoch\n995  181.885559  181.885559  65249.484375  ...  258.904144  105564.718750    995\n996  182.002319  182.002319  65249.257812  ...  264.867310  108777.046875    996\n997  182.648376  182.648376  66288.390625  ...  264.468048  109167.984375    997\n998  181.680847  181.680847  65594.007812  ...  260.255157  107706.851562    998\n999  181.193863  181.193863  65398.644531  ...  265.319489  109055.539062    999\n\n[5 rows x 7 columns]\n\n\n\ndef plot_history(history):\n  plt.cla()\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n\n  plt.plot()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error')\n  plt.plot(hist['epoch'], hist['mse'],\n         label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mse'],\n         label = 'Val Error')\n    #plt.ylim([0,3000])\n  plt.legend()\n  plt.show()\n\n\nplot_history(history)\n\n\n\n\nValidation loss 기준 Early stop옵션 적용\n\nmodel = build_and_compile_model(normalizer)\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model.fit(X_train,\n    y_train, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\n\n..............................................................................................\n\nplot_history(history)"
  },
  {
    "objectID": "posts/pizza/pizza.html",
    "href": "posts/pizza/pizza.html",
    "title": "pizza",
    "section": "",
    "text": "Let the radius of the pizza be ‘z’ and the depth be ‘a’.\n\n\nThe volume of the pizza is π * z**2 * a.\npi z z a\ndelicious pizza"
  },
  {
    "objectID": "posts/Plotly/Plotly.html",
    "href": "posts/Plotly/Plotly.html",
    "title": "Plotly",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(plotly)\n\n\n다음의 패키지를 부착합니다: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(readxl)\nlibrary(scales)\n\n\n다음의 패키지를 부착합니다: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\nmtcars <- mtcars\nmtcars %>% head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nvehicles <- c(\"Four Cylinders\",\"Six Cylinders\",\"Eight Cylinders\")\ncylinders <- mtcars %>% group_by(cyl) %>% summarize(cnt = n())\nveh_cyl <- tibble(vehicles, cylinders$cnt)\n\n\n\n\n\nplot_ly(data = veh_cyl, x = vehicles, y = cylinders$cnt, type = \"bar\",\n        text = cylinders, textposition = \"auto\") %>% \n  layout(title = \"Number of Vehicles in mtcars with 4, 6, and 8 Cylinders\",\n         titlefont = list(size = 28, color = \"orange\", family = \"Calibri\"),\n    yaxis = list(title = \"Number of Vehicles\",\n                 font = list(color = \"black\", family = \"Arial\", size = 26),\n                 tickfont = list(color = \"black\", family = \"Arial\", size = 20)),\n    xaxis = list(title = \"Number of Cylinders\",\n                 titlefont = list(color = \"red\", family = \"Times New Roman\", size = 22),\n                 tickfont = list(color = \"green\", family = \"Cambria\", size = 18)))%>% \n  layout(margin = list( \n                l = 10,\n                r = 10,\n                b = 0,\n                t = 40))\n\nWarning: The titlefont attribute is deprecated. Use title = list(font = ...)\ninstead.\n\n\n\n\n\n\n\nveh <- mtcars %>% \n  group_by(cyl) %>% \n  summarize(cnt =n()) %>% \n  mutate(cyl = factor(cyl))\n\n\nplot_ly(data = veh,x=~cyl,y=~cnt,type=\"bar\",text = ~cyl,textposition = \"auto\") %>% \n  layout(title = \"Numberof Vehicles per cylinders\", \n         titlefont = list(size=28, color=\"orange\", family= \"Calibri\"))\n\nWarning: The titlefont attribute is deprecated. Use title = list(font = ...)\ninstead.\n\n\n\n\n\n\n\nlibrary(nycflights13)\n\ndep.delay.by.day <- flights %>% \n  group_by(day) %>%\n  summarise (mean_dep_delay=mean(dep_delay,na.rm=T))\n\n\nplot_ly( data = dep.delay.by.day, \n         x = ~day, \n         y =~mean_dep_delay) %>% \n  add_trace(type = \"scatter\" ,mode = \"lines+markers\")\n\n\n\n\n\n\nlibrary(gapminder)\n\n\ndf <- gapminder \n\n# No animation\nfig <- df %>%\n  plot_ly(\n    x = ~gdpPercap, \n    y = ~lifeExp, \n    size = ~pop, \n    color = ~continent, \n    text = ~country, \n    hoverinfo = \"text\",\n    type = 'scatter',\n    mode = 'markers',\n    fill = ~''\n  ) %>% layout(\n    xaxis = list(type = \"log\"))\n\nfig\n\n\n\n\n\n\nfig <- df %>%\n  plot_ly(\n    x = ~gdpPercap, \n    y = ~lifeExp, \n    size = ~pop, \n    color = ~continent, \n    frame = ~year, \n    text = ~country, \n    hoverinfo = \"text\",\n    type = 'scatter',\n    mode = 'markers',\n    fill = ~''\n  ) %>% layout(\n    xaxis = list(type = \"log\"))\n\nfig\n\n\n\n\n\n\np1 <- diamonds %>% ggplot(aes(x = cut, fill = clarity))+\n  geom_bar(position = \"dodge\")\n\nggplotly(p1)\n\n\n\n\n\n\nrange\n\nfunction (..., na.rm = FALSE)  .Primitive(\"range\")\n\n\n\naxx <- list(nticks = 4,\n            range = c(-25,75))\n\naxy <- list(nticks = 4,\n            range = c(-25,75))\n\naxz <- list(nticks = 4,\n            range = c(0,50))\n\nx <- 70*(runif(70, 0, 1))\ny <- 55*(runif(70, 0, 1))\nz <- 40*(runif(70, 0, 1))\n\nfig <- plot_ly(x = ~x, \n               y = ~y, \n               z = ~z, \n               type = 'mesh3d') \nfig <- fig %>% \n  layout(scene = list(xaxis=axx,yaxis=axy,zaxis=axz))\n\nfig\n\n\n\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\ncheonan <- readRDS(\"C:/cheonan_geo.rds\")\n\nfig_2 <- plot_ly(cheonan)\nfig_2\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter"
  },
  {
    "objectID": "posts/python_playground/pyhton_playground.html",
    "href": "posts/python_playground/pyhton_playground.html",
    "title": "Python playground",
    "section": "",
    "text": "pandas can draw plot\n리샘플링\n업생플링& 보간"
  },
  {
    "objectID": "posts/python_playground/pyhton_playground.html#파이썬-심화",
    "href": "posts/python_playground/pyhton_playground.html#파이썬-심화",
    "title": "Python playground",
    "section": "파이썬 심화",
    "text": "파이썬 심화\n\nfruit = [\"apple\",\"banana\", \"cherry\"]\n\nupper_fruit = []\n\n# for 루프를 사용하여 리스트의 각 요소를 대문자로 변환하여 빈 리스트에 추가\nfor x in fruit:\n  upper_fruit.append(x.upper())\n\n\nprint(upper_fruit)\n\n['APPLE', 'BANANA', 'CHERRY']\n\n\n\ntotal = []\nfor i in range(1, 1000):\n    if i % 5 == 0 or i % 7 == 0:\n        total.append(i)\n\nprint(\"list:\", total)\n\nlist: [5, 7, 10, 14, 15, 20, 21, 25, 28, 30, 35, 40, 42, 45, 49, 50, 55, 56, 60, 63, 65, 70, 75, 77, 80, 84, 85, 90, 91, 95, 98, 100, 105, 110, 112, 115, 119, 120, 125, 126, 130, 133, 135, 140, 145, 147, 150, 154, 155, 160, 161, 165, 168, 170, 175, 180, 182, 185, 189, 190, 195, 196, 200, 203, 205, 210, 215, 217, 220, 224, 225, 230, 231, 235, 238, 240, 245, 250, 252, 255, 259, 260, 265, 266, 270, 273, 275, 280, 285, 287, 290, 294, 295, 300, 301, 305, 308, 310, 315, 320, 322, 325, 329, 330, 335, 336, 340, 343, 345, 350, 355, 357, 360, 364, 365, 370, 371, 375, 378, 380, 385, 390, 392, 395, 399, 400, 405, 406, 410, 413, 415, 420, 425, 427, 430, 434, 435, 440, 441, 445, 448, 450, 455, 460, 462, 465, 469, 470, 475, 476, 480, 483, 485, 490, 495, 497, 500, 504, 505, 510, 511, 515, 518, 520, 525, 530, 532, 535, 539, 540, 545, 546, 550, 553, 555, 560, 565, 567, 570, 574, 575, 580, 581, 585, 588, 590, 595, 600, 602, 605, 609, 610, 615, 616, 620, 623, 625, 630, 635, 637, 640, 644, 645, 650, 651, 655, 658, 660, 665, 670, 672, 675, 679, 680, 685, 686, 690, 693, 695, 700, 705, 707, 710, 714, 715, 720, 721, 725, 728, 730, 735, 740, 742, 745, 749, 750, 755, 756, 760, 763, 765, 770, 775, 777, 780, 784, 785, 790, 791, 795, 798, 800, 805, 810, 812, 815, 819, 820, 825, 826, 830, 833, 835, 840, 845, 847, 850, 854, 855, 860, 861, 865, 868, 870, 875, 880, 882, 885, 889, 890, 895, 896, 900, 903, 905, 910, 915, 917, 920, 924, 925, 930, 931, 935, 938, 940, 945, 950, 952, 955, 959, 960, 965, 966, 970, 973, 975, 980, 985, 987, 990, 994, 995]\n\n\n\n죄수의 딜레마\n나와 공범이 체포되었다.\n서로 협조를 하면 각자 5년의 형량을 받는다.\n한명만 협조를 하게 되면 협조한 자는 석방, 협조하지 않은 자는 10년의 형량을 받는다.\n내가 협조를 하였을떄 상대방이 협조할 확률이 50%라면 내 형량의 기댓값은 얼마인가?\n(1000번 반복한다.)\n\nme_decision=[]\nme_term=[]\nyou_decision=[]\nyou_term=[]\n\nimport random\n\nfor k in range(1,1001):\n  me=['협조']\n  you=random.sample(['묵비권','협조'],1)\n  me_decision.append(me)\n  you_decision.append(you)\n  \n  if you==me:\n    me_term.append(5)\n    you_term.append(5)\n    \n  else:\n    me_term.append(0)\n    you_term.append(10)\n\n    \nprint(me_decision, me_term, you_decision, you_term)\n\n[['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조']] [5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 0, 5, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 0, 5, 5, 5, 0, 0, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 0, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 0, 0, 0, 5, 0, 5, 5, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 0, 5, 0, 0] [['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['묵비권'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['협조'], ['묵비권'], ['묵비권'], ['협조'], ['묵비권'], ['협조'], ['묵비권'], ['묵비권']] [5, 5, 5, 10, 5, 5, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 5, 10, 10, 10, 10, 5, 10, 10, 5, 10, 10, 5, 10, 5, 5, 10, 10, 10, 5, 10, 10, 10, 10, 10, 5, 10, 5, 10, 10, 10, 5, 5, 5, 10, 5, 5, 5, 5, 5, 10, 10, 10, 5, 5, 5, 10, 5, 5, 5, 10, 5, 5, 5, 5, 10, 5, 10, 5, 5, 5, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 10, 5, 10, 10, 5, 5, 10, 5, 5, 5, 5, 10, 10, 10, 5, 5, 5, 5, 10, 5, 5, 10, 5, 5, 10, 10, 10, 5, 5, 5, 10, 10, 5, 10, 10, 5, 10, 5, 10, 5, 10, 10, 10, 5, 10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 10, 5, 5, 5, 10, 10, 5, 10, 10, 10, 5, 5, 5, 10, 5, 10, 5, 5, 10, 10, 5, 10, 10, 5, 10, 5, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 5, 5, 10, 5, 5, 5, 5, 5, 5, 10, 10, 5, 10, 10, 5, 10, 10, 5, 10, 10, 5, 5, 5, 5, 10, 5, 10, 5, 5, 5, 5, 5, 10, 5, 10, 5, 5, 5, 10, 10, 10, 10, 5, 10, 10, 5, 10, 5, 5, 10, 10, 5, 10, 10, 5, 5, 10, 5, 10, 10, 5, 10, 5, 5, 5, 10, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 5, 10, 10, 10, 10, 5, 10, 10, 5, 5, 5, 5, 5, 5, 10, 5, 10, 10, 10, 5, 10, 5, 10, 5, 5, 10, 10, 10, 10, 5, 10, 10, 5, 10, 10, 10, 10, 10, 5, 5, 10, 10, 10, 10, 5, 5, 5, 10, 10, 10, 10, 10, 10, 5, 10, 5, 10, 10, 5, 5, 5, 5, 10, 5, 10, 10, 5, 5, 10, 5, 5, 5, 10, 5, 10, 5, 5, 5, 10, 5, 5, 5, 10, 5, 10, 5, 5, 10, 10, 5, 10, 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 10, 10, 5, 10, 10, 5, 10, 10, 5, 5, 10, 10, 10, 10, 5, 10, 10, 10, 5, 10, 5, 10, 10, 5, 10, 5, 10, 10, 5, 10, 10, 10, 10, 10, 10, 5, 10, 5, 5, 10, 5, 10, 10, 5, 5, 5, 10, 10, 10, 5, 10, 10, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 5, 10, 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 10, 5, 10, 5, 10, 5, 10, 10, 10, 10, 5, 5, 5, 10, 5, 5, 10, 5, 10, 5, 10, 10, 10, 5, 10, 5, 10, 5, 10, 5, 10, 10, 5, 10, 5, 5, 5, 10, 10, 10, 5, 10, 10, 5, 5, 10, 5, 10, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 5, 5, 5, 5, 10, 5, 5, 5, 10, 10, 5, 5, 5, 5, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 10, 10, 10, 10, 5, 10, 5, 5, 10, 10, 5, 5, 5, 5, 5, 10, 5, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 10, 10, 5, 5, 5, 10, 5, 10, 5, 5, 5, 5, 10, 10, 10, 5, 5, 10, 10, 10, 10, 10, 10, 5, 10, 10, 5, 10, 10, 5, 5, 10, 10, 5, 5, 10, 5, 5, 10, 5, 5, 5, 5, 5, 10, 5, 10, 10, 5, 10, 5, 5, 10, 5, 10, 10, 5, 10, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 10, 5, 10, 5, 5, 10, 10, 5, 10, 5, 10, 10, 10, 10, 5, 10, 5, 5, 10, 5, 10, 10, 5, 5, 5, 10, 5, 5, 5, 5, 5, 10, 5, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, 10, 5, 10, 10, 5, 10, 5, 5, 10, 10, 10, 5, 10, 5, 10, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 5, 5, 5, 10, 10, 10, 10, 10, 5, 5, 10, 5, 5, 10, 5, 5, 5, 10, 5, 5, 5, 5, 10, 10, 5, 10, 5, 10, 10, 10, 5, 5, 10, 10, 10, 5, 5, 5, 5, 10, 10, 10, 10, 10, 5, 5, 10, 10, 5, 5, 10, 10, 5, 5, 10, 10, 10, 5, 5, 10, 5, 5, 10, 10, 5, 10, 5, 10, 5, 5, 10, 10, 5, 5, 10, 5, 5, 10, 10, 10, 5, 5, 5, 10, 10, 10, 10, 10, 5, 10, 10, 10, 10, 5, 10, 5, 5, 10, 5, 5, 5, 10, 5, 10, 5, 10, 5, 5, 10, 10, 5, 10, 10, 10, 10, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 10, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 5, 5, 10, 10, 5, 10, 5, 10, 5, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 5, 10, 5, 10, 5, 10, 5, 5, 5, 10, 10, 10, 10, 5, 5, 10, 10, 5, 10, 10, 10, 5, 10, 5, 5, 5, 10, 5, 5, 5, 10, 5, 5, 5, 10, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 5, 5, 10, 10, 5, 5, 10, 10, 10, 5, 10, 5, 5, 10, 5, 5, 10, 10, 5, 10, 10, 10, 10, 10, 5, 10, 5, 10, 5, 5, 5, 5, 5, 10, 10, 5, 5, 5, 10, 5, 5, 10, 5, 5, 5, 10, 10, 5, 10, 5, 5, 10, 10, 10, 10, 10, 5, 5, 5, 10, 5, 5, 10, 5, 5, 5, 5, 5, 5, 10, 5, 5, 5, 5, 5, 10, 5, 5, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 5, 10, 5, 10, 10]\n\nprint('내형량',sum(me_term)/1000,'공범형량',sum(you_term)/1000)\n\n내형량 2.57 공범형량 7.43\n\n\n\nfruit = [\"apple\",\"banana\", \"cherry\"]\nage = [1,2,3]\n\nfor x in range(3):\n  print(\"i'm\",fruit[x],\", my age\",age[x])\n\ni'm apple , my age 1\ni'm banana , my age 2\ni'm cherry , my age 3\n\n\n\na = ([1,2,3],[4,5,6],[7,8])\nb = []\nfor i in a:\n  for x in i:\n    b.append(x)\n    \nb\n\n[1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\nimport pandas as pd\nmpg = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/mpg.csv\")\n\n\nmpg['hwy+displ'] = mpg.apply(lambda x: x[\"displ\"]+x[\"hwy\"],axis=1)\n\n\nmpg['hwy+displ']\n\n0      30.8\n1      30.8\n2      33.0\n3      32.0\n4      28.8\n       ... \n229    30.0\n230    31.0\n231    28.8\n232    28.8\n233    29.6\nName: hwy+displ, Length: 234, dtype: float64"
  },
  {
    "objectID": "posts/python_playground/pyhton_playground.html#img",
    "href": "posts/python_playground/pyhton_playground.html#img",
    "title": "Python playground",
    "section": "img",
    "text": "img\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n\nimg_raw = cv2.imread(\"lena_color.png\")\n\n\nimg_gray = cv2.cvtColor(img_raw,cv2.COLOR_BGR2GRAY)\n\n\nplt.imshow(img_gray,\"gray\")\nplt.show()"
  },
  {
    "objectID": "posts/python_playground/pyhton_playground.html#geopandas",
    "href": "posts/python_playground/pyhton_playground.html#geopandas",
    "title": "Python playground",
    "section": "Geopandas",
    "text": "Geopandas\n\nimport geopandas as gpd\n\n\nfull_data = gpd.read_file(\"C:/archive/DEC_lands/DEC_lands/DEC_lands.shp\")\n\n\ntype(full_data)\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\n\n\n\ndata = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\ndata.head()\n\n         CLASS    COUNTY                                           geometry\n0  WILD FOREST  DELAWARE  POLYGON ((486093.245 4635308.586, 486787.235 4...\n1  WILD FOREST  DELAWARE  POLYGON ((491931.514 4637416.256, 491305.424 4...\n2  WILD FOREST  DELAWARE  POLYGON ((486000.287 4635834.453, 485007.550 4...\n3  WILD FOREST    GREENE  POLYGON ((541716.775 4675243.268, 541217.579 4...\n4  WILD FOREST     ESSEX  POLYGON ((583896.043 4909643.187, 583891.200 4...\n\n\n\nHow many lands of each type are there?\n\ndata.CLASS.value_counts()\n\nWILD FOREST                   965\nINTENSIVE USE                 108\nPRIMITIVE                      60\nWILDERNESS                     52\nADMINISTRATIVE                 17\nUNCLASSIFIED                    7\nHISTORIC                        5\nPRIMITIVE BICYCLE CORRIDOR      4\nCANOE AREA                      1\nName: CLASS, dtype: int64\n\n\n\n# Select lands that fall under the \"WILD FOREST\" or \"WILDERNESS\" category\nwild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()\nwild_lands.head()\n\n         CLASS    COUNTY                                           geometry\n0  WILD FOREST  DELAWARE  POLYGON ((486093.245 4635308.586, 486787.235 4...\n1  WILD FOREST  DELAWARE  POLYGON ((491931.514 4637416.256, 491305.424 4...\n2  WILD FOREST  DELAWARE  POLYGON ((486000.287 4635834.453, 485007.550 4...\n3  WILD FOREST    GREENE  POLYGON ((541716.775 4675243.268, 541217.579 4...\n4  WILD FOREST     ESSEX  POLYGON ((583896.043 4909643.187, 583891.200 4...\n\n\n\nwild_lands.plot()"
  },
  {
    "objectID": "posts/R basic/R basic.html",
    "href": "posts/R basic/R basic.html",
    "title": "R basics",
    "section": "",
    "text": "R basics"
  },
  {
    "objectID": "posts/R basic/R basic.html#데이터-로드",
    "href": "posts/R basic/R basic.html#데이터-로드",
    "title": "R basics",
    "section": "데이터 로드",
    "text": "데이터 로드\n\nemploytable <- read.csv(\"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\")\n\nhead(employtable)\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         198     Donald  OConnell DOCONNEL 650.507.9833 21-JUN-07 SH_CLERK\n2         199    Douglas     Grant   DGRANT 650.507.9844 13-JAN-08 SH_CLERK\n3         200   Jennifer    Whalen  JWHALEN 515.123.4444 17-SEP-03  AD_ASST\n4         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04   MK_MAN\n5         202        Pat       Fay     PFAY 603.123.6666 17-AUG-05   MK_REP\n6         203      Susan    Mavris  SMAVRIS 515.123.7777 07-JUN-02   HR_REP\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   2600             -         124            50\n2   2600             -         124            50\n3   4400             -         101            10\n4  13000             -         100            20\n5   6000             -         201            20\n6   6500             -         101            40"
  },
  {
    "objectID": "posts/R basic/R basic.html#차원-확인-dim",
    "href": "posts/R basic/R basic.html#차원-확인-dim",
    "title": "R basics",
    "section": "차원 확인 dim()",
    "text": "차원 확인 dim()\n\ndim(employtable)\n\n[1] 50 11"
  },
  {
    "objectID": "posts/R basic/R basic.html#행-개수-nrow",
    "href": "posts/R basic/R basic.html#행-개수-nrow",
    "title": "R basics",
    "section": "행 개수 nrow()",
    "text": "행 개수 nrow()\n\nnrow(employtable)\n\n[1] 50"
  },
  {
    "objectID": "posts/R basic/R basic.html#열-개수-ncol",
    "href": "posts/R basic/R basic.html#열-개수-ncol",
    "title": "R basics",
    "section": "열 개수 ncol()",
    "text": "열 개수 ncol()\n\nncol(employtable)\n\n[1] 11"
  },
  {
    "objectID": "posts/R basic/R basic.html#기초통계량",
    "href": "posts/R basic/R basic.html#기초통계량",
    "title": "R basics",
    "section": "기초통계량",
    "text": "기초통계량\n\nsummary(employtable)\n\n  EMPLOYEE_ID     FIRST_NAME         LAST_NAME            EMAIL          \n Min.   :100.0   Length:50          Length:50          Length:50         \n 1st Qu.:112.2   Class :character   Class :character   Class :character  \n Median :124.5   Mode  :character   Mode  :character   Mode  :character  \n Mean   :134.8                                                           \n 3rd Qu.:136.8                                                           \n Max.   :206.0                                                           \n PHONE_NUMBER        HIRE_DATE            JOB_ID              SALARY     \n Length:50          Length:50          Length:50          Min.   : 2100  \n Class :character   Class :character   Class :character   1st Qu.: 2725  \n Mode  :character   Mode  :character   Mode  :character   Median : 4600  \n                                                          Mean   : 6182  \n                                                          3rd Qu.: 8150  \n                                                          Max.   :24000  \n COMMISSION_PCT      MANAGER_ID        DEPARTMENT_ID  \n Length:50          Length:50          Min.   : 10.0  \n Class :character   Class :character   1st Qu.: 50.0  \n Mode  :character   Mode  :character   Median : 50.0  \n                                       Mean   : 57.6  \n                                       3rd Qu.: 60.0  \n                                       Max.   :110.0"
  },
  {
    "objectID": "posts/R basic/R basic.html#원하는-컬럼열-뽑기-기호-사용",
    "href": "posts/R basic/R basic.html#원하는-컬럼열-뽑기-기호-사용",
    "title": "R basics",
    "section": "원하는 컬럼(열) 뽑기 $ 기호 사용",
    "text": "원하는 컬럼(열) 뽑기 $ 기호 사용\n\nemploytable$EMAIL\n\n [1] \"DOCONNEL\" \"DGRANT\"   \"JWHALEN\"  \"MHARTSTE\" \"PFAY\"     \"SMAVRIS\" \n [7] \"HBAER\"    \"SHIGGINS\" \"WGIETZ\"   \"SKING\"    \"NKOCHHAR\" \"LDEHAAN\" \n[13] \"AHUNOLD\"  \"BERNST\"   \"DAUSTIN\"  \"VPATABAL\" \"DLORENTZ\" \"NGREENBE\"\n[19] \"DFAVIET\"  \"JCHEN\"    \"ISCIARRA\" \"JMURMAN\"  \"LPOPP\"    \"DRAPHEAL\"\n[25] \"AKHOO\"    \"SBAIDA\"   \"STOBIAS\"  \"GHIMURO\"  \"KCOLMENA\" \"MWEISS\"  \n[31] \"AFRIPP\"   \"PKAUFLIN\" \"SVOLLMAN\" \"KMOURGOS\" \"JNAYER\"   \"IMIKKILI\"\n[37] \"JLANDRY\"  \"SMARKLE\"  \"LBISSOT\"  \"MATKINSO\" \"JAMRLOW\"  \"TJOLSON\" \n[43] \"JMALLIN\"  \"MROGERS\"  \"KGEE\"     \"HPHILTAN\" \"RLADWIG\"  \"SSTILES\" \n[49] \"JSEO\"     \"JPATEL\""
  },
  {
    "objectID": "posts/R basic/R basic.html#인덱싱-df행열",
    "href": "posts/R basic/R basic.html#인덱싱-df행열",
    "title": "R basics",
    "section": "인덱싱 df[행,열]",
    "text": "인덱싱 df[행,열]\nR에서 데이터의 일부분을 선택/선별하는 작업을 Indexing 한다고 합니다.\nR로 데이터 전처리, 분석을 하다보면 R Indexing을 부지기수로 사용하게 되니 R Indexing은 제대로 알고 넘어가야할 매우 중요한 부분입니다.\n특히, R에서는 벡터 Indexing 후 연산하는게 매우 큰 강점이고 유용한 기능이랍니다. ### 파이썬과 달리 1부터 시작 ### 행 인덱싱\n\nemploytable[1,]\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         198     Donald  OConnell DOCONNEL 650.507.9833 21-JUN-07 SH_CLERK\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   2600             -         124            50\n\n\n\nemploytable[1:5,]\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         198     Donald  OConnell DOCONNEL 650.507.9833 21-JUN-07 SH_CLERK\n2         199    Douglas     Grant   DGRANT 650.507.9844 13-JAN-08 SH_CLERK\n3         200   Jennifer    Whalen  JWHALEN 515.123.4444 17-SEP-03  AD_ASST\n4         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04   MK_MAN\n5         202        Pat       Fay     PFAY 603.123.6666 17-AUG-05   MK_REP\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   2600             -         124            50\n2   2600             -         124            50\n3   4400             -         101            10\n4  13000             -         100            20\n5   6000             -         201            20\n\n\n\n열 인덱싱\n\nemploytable[,2]\n\n [1] \"Donald\"      \"Douglas\"     \"Jennifer\"    \"Michael\"     \"Pat\"        \n [6] \"Susan\"       \"Hermann\"     \"Shelley\"     \"William\"     \"Steven\"     \n[11] \"Neena\"       \"Lex\"         \"Alexander\"   \"Bruce\"       \"David\"      \n[16] \"Valli\"       \"Diana\"       \"Nancy\"       \"Daniel\"      \"John\"       \n[21] \"Ismael\"      \"Jose Manuel\" \"Luis\"        \"Den\"         \"Alexander\"  \n[26] \"Shelli\"      \"Sigal\"       \"Guy\"         \"Karen\"       \"Matthew\"    \n[31] \"Adam\"        \"Payam\"       \"Shanta\"      \"Kevin\"       \"Julia\"      \n[36] \"Irene\"       \"James\"       \"Steven\"      \"Laura\"       \"Mozhe\"      \n[41] \"James\"       \"TJ\"          \"Jason\"       \"Michael\"     \"Ki\"         \n[46] \"Hazel\"       \"Renske\"      \"Stephen\"     \"John\"        \"Joshua\"     \n\n\n\nemploytable[,1:5]\n\n   EMPLOYEE_ID  FIRST_NAME   LAST_NAME    EMAIL PHONE_NUMBER\n1          198      Donald    OConnell DOCONNEL 650.507.9833\n2          199     Douglas       Grant   DGRANT 650.507.9844\n3          200    Jennifer      Whalen  JWHALEN 515.123.4444\n4          201     Michael   Hartstein MHARTSTE 515.123.5555\n5          202         Pat         Fay     PFAY 603.123.6666\n6          203       Susan      Mavris  SMAVRIS 515.123.7777\n7          204     Hermann        Baer    HBAER 515.123.8888\n8          205     Shelley     Higgins SHIGGINS 515.123.8080\n9          206     William       Gietz   WGIETZ 515.123.8181\n10         100      Steven        King    SKING 515.123.4567\n11         101       Neena     Kochhar NKOCHHAR 515.123.4568\n12         102         Lex     De Haan  LDEHAAN 515.123.4569\n13         103   Alexander      Hunold  AHUNOLD 590.423.4567\n14         104       Bruce       Ernst   BERNST 590.423.4568\n15         105       David      Austin  DAUSTIN 590.423.4569\n16         106       Valli   Pataballa VPATABAL 590.423.4560\n17         107       Diana     Lorentz DLORENTZ 590.423.5567\n18         108       Nancy   Greenberg NGREENBE 515.124.4569\n19         109      Daniel      Faviet  DFAVIET 515.124.4169\n20         110        John        Chen    JCHEN 515.124.4269\n21         111      Ismael     Sciarra ISCIARRA 515.124.4369\n22         112 Jose Manuel       Urman  JMURMAN 515.124.4469\n23         113        Luis        Popp    LPOPP 515.124.4567\n24         114         Den    Raphaely DRAPHEAL 515.127.4561\n25         115   Alexander        Khoo    AKHOO 515.127.4562\n26         116      Shelli       Baida   SBAIDA 515.127.4563\n27         117       Sigal      Tobias  STOBIAS 515.127.4564\n28         118         Guy      Himuro  GHIMURO 515.127.4565\n29         119       Karen  Colmenares KCOLMENA 515.127.4566\n30         120     Matthew       Weiss   MWEISS 650.123.1234\n31         121        Adam       Fripp   AFRIPP 650.123.2234\n32         122       Payam    Kaufling PKAUFLIN 650.123.3234\n33         123      Shanta     Vollman SVOLLMAN 650.123.4234\n34         124       Kevin     Mourgos KMOURGOS 650.123.5234\n35         125       Julia       Nayer   JNAYER 650.124.1214\n36         126       Irene Mikkilineni IMIKKILI 650.124.1224\n37         127       James      Landry  JLANDRY 650.124.1334\n38         128      Steven      Markle  SMARKLE 650.124.1434\n39         129       Laura      Bissot  LBISSOT 650.124.5234\n40         130       Mozhe    Atkinson MATKINSO 650.124.6234\n41         131       James      Marlow  JAMRLOW 650.124.7234\n42         132          TJ       Olson  TJOLSON 650.124.8234\n43         133       Jason      Mallin  JMALLIN 650.127.1934\n44         134     Michael      Rogers  MROGERS 650.127.1834\n45         135          Ki         Gee     KGEE 650.127.1734\n46         136       Hazel  Philtanker HPHILTAN 650.127.1634\n47         137      Renske      Ladwig  RLADWIG 650.121.1234\n48         138     Stephen      Stiles  SSTILES 650.121.2034\n49         139        John         Seo     JSEO 650.121.2019\n50         140      Joshua       Patel   JPATEL 650.121.1834\n\n\n\n\n벡터 넣어도 됨\n\nemploytable[,c(2,3,5)]\n\n    FIRST_NAME   LAST_NAME PHONE_NUMBER\n1       Donald    OConnell 650.507.9833\n2      Douglas       Grant 650.507.9844\n3     Jennifer      Whalen 515.123.4444\n4      Michael   Hartstein 515.123.5555\n5          Pat         Fay 603.123.6666\n6        Susan      Mavris 515.123.7777\n7      Hermann        Baer 515.123.8888\n8      Shelley     Higgins 515.123.8080\n9      William       Gietz 515.123.8181\n10      Steven        King 515.123.4567\n11       Neena     Kochhar 515.123.4568\n12         Lex     De Haan 515.123.4569\n13   Alexander      Hunold 590.423.4567\n14       Bruce       Ernst 590.423.4568\n15       David      Austin 590.423.4569\n16       Valli   Pataballa 590.423.4560\n17       Diana     Lorentz 590.423.5567\n18       Nancy   Greenberg 515.124.4569\n19      Daniel      Faviet 515.124.4169\n20        John        Chen 515.124.4269\n21      Ismael     Sciarra 515.124.4369\n22 Jose Manuel       Urman 515.124.4469\n23        Luis        Popp 515.124.4567\n24         Den    Raphaely 515.127.4561\n25   Alexander        Khoo 515.127.4562\n26      Shelli       Baida 515.127.4563\n27       Sigal      Tobias 515.127.4564\n28         Guy      Himuro 515.127.4565\n29       Karen  Colmenares 515.127.4566\n30     Matthew       Weiss 650.123.1234\n31        Adam       Fripp 650.123.2234\n32       Payam    Kaufling 650.123.3234\n33      Shanta     Vollman 650.123.4234\n34       Kevin     Mourgos 650.123.5234\n35       Julia       Nayer 650.124.1214\n36       Irene Mikkilineni 650.124.1224\n37       James      Landry 650.124.1334\n38      Steven      Markle 650.124.1434\n39       Laura      Bissot 650.124.5234\n40       Mozhe    Atkinson 650.124.6234\n41       James      Marlow 650.124.7234\n42          TJ       Olson 650.124.8234\n43       Jason      Mallin 650.127.1934\n44     Michael      Rogers 650.127.1834\n45          Ki         Gee 650.127.1734\n46       Hazel  Philtanker 650.127.1634\n47      Renske      Ladwig 650.121.1234\n48     Stephen      Stiles 650.121.2034\n49        John         Seo 650.121.2019\n50      Joshua       Patel 650.121.1834\n\n\n\n\n행, 열 인덱싱\n\nemploytable[1:5,c(2,3,5)]\n\n  FIRST_NAME LAST_NAME PHONE_NUMBER\n1     Donald  OConnell 650.507.9833\n2    Douglas     Grant 650.507.9844\n3   Jennifer    Whalen 515.123.4444\n4    Michael Hartstein 515.123.5555\n5        Pat       Fay 603.123.6666"
  },
  {
    "objectID": "posts/R basic/R basic.html#employtable의-job_id-종류-확인-unique",
    "href": "posts/R basic/R basic.html#employtable의-job_id-종류-확인-unique",
    "title": "R basics",
    "section": "Employtable의 JOB_ID 종류 확인 unique()",
    "text": "Employtable의 JOB_ID 종류 확인 unique()\n\nunique(employtable$JOB_ID)\n\n [1] \"SH_CLERK\"   \"AD_ASST\"    \"MK_MAN\"     \"MK_REP\"     \"HR_REP\"    \n [6] \"PR_REP\"     \"AC_MGR\"     \"AC_ACCOUNT\" \"AD_PRES\"    \"AD_VP\"     \n[11] \"IT_PROG\"    \"FI_MGR\"     \"FI_ACCOUNT\" \"PU_MAN\"     \"PU_CLERK\"  \n[16] \"ST_MAN\"     \"ST_CLERK\""
  },
  {
    "objectID": "posts/R basic/R basic.html#기존-데이터로-새로운-열-만들기",
    "href": "posts/R basic/R basic.html#기존-데이터로-새로운-열-만들기",
    "title": "R basics",
    "section": "기존 데이터로 새로운 열 만들기",
    "text": "기존 데이터로 새로운 열 만들기\n\n문자열\nfirst_name, last_name 를 합치기\n문자열을 합칠때는 paste() 사용\n\nemploytable_2 = employtable\n\nemploytable_2$FULL_NAME <- paste(employtable_2$FIRST_NAME,employtable_2$LAST_NAME)\nhead(employtable_2)\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         198     Donald  OConnell DOCONNEL 650.507.9833 21-JUN-07 SH_CLERK\n2         199    Douglas     Grant   DGRANT 650.507.9844 13-JAN-08 SH_CLERK\n3         200   Jennifer    Whalen  JWHALEN 515.123.4444 17-SEP-03  AD_ASST\n4         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04   MK_MAN\n5         202        Pat       Fay     PFAY 603.123.6666 17-AUG-05   MK_REP\n6         203      Susan    Mavris  SMAVRIS 515.123.7777 07-JUN-02   HR_REP\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID         FULL_NAME\n1   2600             -         124            50   Donald OConnell\n2   2600             -         124            50     Douglas Grant\n3   4400             -         101            10   Jennifer Whalen\n4  13000             -         100            20 Michael Hartstein\n5   6000             -         201            20           Pat Fay\n6   6500             -         101            40      Susan Mavris\n\n\n\n\n숫자\n예시 데이터\n\nname <- c(\"david\",\"johns\",\"yoon\",\"michael\")\nmath <- c(90,85,70,80)\nart <- c(60,70,80,90)\n\nex <- data.frame(name,math,art)\nex\n\n     name math art\n1   david   90  60\n2   johns   85  70\n3    yoon   70  80\n4 michael   80  90\n\n\n\nex$Total <- ex$math + ex$art\nex\n\n     name math art Total\n1   david   90  60   150\n2   johns   85  70   155\n3    yoon   70  80   150\n4 michael   80  90   170"
  },
  {
    "objectID": "posts/R basic/R basic.html#조건에-맞는-데이터-추출",
    "href": "posts/R basic/R basic.html#조건에-맞는-데이터-추출",
    "title": "R basics",
    "section": "조건에 맞는 데이터 추출",
    "text": "조건에 맞는 데이터 추출\nJOB_ID가 ST_CLERK인 행 추출\n\n조건을 치면 불리언 타입으로 추출됨\n\n\nemploytable$JOB_ID == \"ST_CLERK\"\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[49]  TRUE  TRUE\n\n\n\n인덱싱\n\n\nemploytable[employtable$JOB_ID == \"ST_CLERK\",]\n\n   EMPLOYEE_ID FIRST_NAME   LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n35         125      Julia       Nayer   JNAYER 650.124.1214 16-JUL-05 ST_CLERK\n36         126      Irene Mikkilineni IMIKKILI 650.124.1224 28-SEP-06 ST_CLERK\n37         127      James      Landry  JLANDRY 650.124.1334 14-JAN-07 ST_CLERK\n38         128     Steven      Markle  SMARKLE 650.124.1434 08-MAR-08 ST_CLERK\n39         129      Laura      Bissot  LBISSOT 650.124.5234 20-AUG-05 ST_CLERK\n40         130      Mozhe    Atkinson MATKINSO 650.124.6234 30-OCT-05 ST_CLERK\n41         131      James      Marlow  JAMRLOW 650.124.7234 16-FEB-05 ST_CLERK\n42         132         TJ       Olson  TJOLSON 650.124.8234 10-APR-07 ST_CLERK\n43         133      Jason      Mallin  JMALLIN 650.127.1934 14-JUN-04 ST_CLERK\n44         134    Michael      Rogers  MROGERS 650.127.1834 26-AUG-06 ST_CLERK\n45         135         Ki         Gee     KGEE 650.127.1734 12-DEC-07 ST_CLERK\n46         136      Hazel  Philtanker HPHILTAN 650.127.1634 06-FEB-08 ST_CLERK\n47         137     Renske      Ladwig  RLADWIG 650.121.1234 14-JUL-03 ST_CLERK\n48         138    Stephen      Stiles  SSTILES 650.121.2034 26-OCT-05 ST_CLERK\n49         139       John         Seo     JSEO 650.121.2019 12-FEB-06 ST_CLERK\n50         140     Joshua       Patel   JPATEL 650.121.1834 06-APR-06 ST_CLERK\n   SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n35   3200             -         120            50\n36   2700             -         120            50\n37   2400             -         120            50\n38   2200             -         120            50\n39   3300             -         121            50\n40   2800             -         121            50\n41   2500             -         121            50\n42   2100             -         121            50\n43   3300             -         122            50\n44   2900             -         122            50\n45   2400             -         122            50\n46   2200             -         122            50\n47   3600             -         123            50\n48   3200             -         123            50\n49   2700             -         123            50\n50   2500             -         123            50\n\n\nSALARY가 5000 이상인 행 추출\n\nemploytable[employtable$SALARY > 5000,]\n\n   EMPLOYEE_ID  FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE     JOB_ID\n4          201     Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04     MK_MAN\n5          202         Pat       Fay     PFAY 603.123.6666 17-AUG-05     MK_REP\n6          203       Susan    Mavris  SMAVRIS 515.123.7777 07-JUN-02     HR_REP\n7          204     Hermann      Baer    HBAER 515.123.8888 07-JUN-02     PR_REP\n8          205     Shelley   Higgins SHIGGINS 515.123.8080 07-JUN-02     AC_MGR\n9          206     William     Gietz   WGIETZ 515.123.8181 07-JUN-02 AC_ACCOUNT\n10         100      Steven      King    SKING 515.123.4567 17-JUN-03    AD_PRES\n11         101       Neena   Kochhar NKOCHHAR 515.123.4568 21-SEP-05      AD_VP\n12         102         Lex   De Haan  LDEHAAN 515.123.4569 13-JAN-01      AD_VP\n13         103   Alexander    Hunold  AHUNOLD 590.423.4567 03-JAN-06    IT_PROG\n14         104       Bruce     Ernst   BERNST 590.423.4568 21-MAY-07    IT_PROG\n18         108       Nancy Greenberg NGREENBE 515.124.4569 17-AUG-02     FI_MGR\n19         109      Daniel    Faviet  DFAVIET 515.124.4169 16-AUG-02 FI_ACCOUNT\n20         110        John      Chen    JCHEN 515.124.4269 28-SEP-05 FI_ACCOUNT\n21         111      Ismael   Sciarra ISCIARRA 515.124.4369 30-SEP-05 FI_ACCOUNT\n22         112 Jose Manuel     Urman  JMURMAN 515.124.4469 07-MAR-06 FI_ACCOUNT\n23         113        Luis      Popp    LPOPP 515.124.4567 07-DEC-07 FI_ACCOUNT\n24         114         Den  Raphaely DRAPHEAL 515.127.4561 07-DEC-02     PU_MAN\n30         120     Matthew     Weiss   MWEISS 650.123.1234 18-JUL-04     ST_MAN\n31         121        Adam     Fripp   AFRIPP 650.123.2234 10-APR-05     ST_MAN\n32         122       Payam  Kaufling PKAUFLIN 650.123.3234 01-MAY-03     ST_MAN\n33         123      Shanta   Vollman SVOLLMAN 650.123.4234 10-OCT-05     ST_MAN\n34         124       Kevin   Mourgos KMOURGOS 650.123.5234 16-NOV-07     ST_MAN\n   SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n4   13000             -         100            20\n5    6000             -         201            20\n6    6500             -         101            40\n7   10000             -         101            70\n8   12008             -         101           110\n9    8300             -         205           110\n10  24000             -          -             90\n11  17000             -         100            90\n12  17000             -         100            90\n13   9000             -         102            60\n14   6000             -         103            60\n18  12008             -         101           100\n19   9000             -         108           100\n20   8200             -         108           100\n21   7700             -         108           100\n22   7800             -         108           100\n23   6900             -         108           100\n24  11000             -         100            30\n30   8000             -         100            50\n31   8200             -         100            50\n32   7900             -         100            50\n33   6500             -         100            50\n34   5800             -         100            50"
  },
  {
    "objectID": "posts/R basic/R basic.html#조건에-맞는-데이터-추출2",
    "href": "posts/R basic/R basic.html#조건에-맞는-데이터-추출2",
    "title": "R basics",
    "section": "조건에 맞는 데이터 추출(2)",
    "text": "조건에 맞는 데이터 추출(2)"
  },
  {
    "objectID": "posts/R basic/R basic.html#파이프-연산자",
    "href": "posts/R basic/R basic.html#파이프-연산자",
    "title": "R basics",
    "section": "파이프 연산자",
    "text": "파이프 연산자\n파이프 연산자는 이름 그대로 어떤 값들이 파이프를 통과하는 것처럼 함수와 함수들을 타고다닐 수 있게 해줍니다.\n가독성이 좋음\n단축키 ctrl + shift + M\n\n#head(employtable) 과 동일\nemploytable %>% head()\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         198     Donald  OConnell DOCONNEL 650.507.9833 21-JUN-07 SH_CLERK\n2         199    Douglas     Grant   DGRANT 650.507.9844 13-JAN-08 SH_CLERK\n3         200   Jennifer    Whalen  JWHALEN 515.123.4444 17-SEP-03  AD_ASST\n4         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04   MK_MAN\n5         202        Pat       Fay     PFAY 603.123.6666 17-AUG-05   MK_REP\n6         203      Susan    Mavris  SMAVRIS 515.123.7777 07-JUN-02   HR_REP\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   2600             -         124            50\n2   2600             -         124            50\n3   4400             -         101            10\n4  13000             -         100            20\n5   6000             -         201            20\n6   6500             -         101            40"
  },
  {
    "objectID": "posts/R basic/R basic.html#조건에-맞는-열-추출",
    "href": "posts/R basic/R basic.html#조건에-맞는-열-추출",
    "title": "R basics",
    "section": "조건에 맞는 열 추출",
    "text": "조건에 맞는 열 추출\nfilter() 사용\n\nhead(filter(employtable,JOB_ID == \"ST_CLERK\"))\n\n  EMPLOYEE_ID FIRST_NAME   LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         125      Julia       Nayer   JNAYER 650.124.1214 16-JUL-05 ST_CLERK\n2         126      Irene Mikkilineni IMIKKILI 650.124.1224 28-SEP-06 ST_CLERK\n3         127      James      Landry  JLANDRY 650.124.1334 14-JAN-07 ST_CLERK\n4         128     Steven      Markle  SMARKLE 650.124.1434 08-MAR-08 ST_CLERK\n5         129      Laura      Bissot  LBISSOT 650.124.5234 20-AUG-05 ST_CLERK\n6         130      Mozhe    Atkinson MATKINSO 650.124.6234 30-OCT-05 ST_CLERK\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   3200             -         120            50\n2   2700             -         120            50\n3   2400             -         120            50\n4   2200             -         120            50\n5   3300             -         121            50\n6   2800             -         121            50\n\nemploytable %>% filter(JOB_ID == \"ST_CLERK\") %>% head()\n\n  EMPLOYEE_ID FIRST_NAME   LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         125      Julia       Nayer   JNAYER 650.124.1214 16-JUL-05 ST_CLERK\n2         126      Irene Mikkilineni IMIKKILI 650.124.1224 28-SEP-06 ST_CLERK\n3         127      James      Landry  JLANDRY 650.124.1334 14-JAN-07 ST_CLERK\n4         128     Steven      Markle  SMARKLE 650.124.1434 08-MAR-08 ST_CLERK\n5         129      Laura      Bissot  LBISSOT 650.124.5234 20-AUG-05 ST_CLERK\n6         130      Mozhe    Atkinson MATKINSO 650.124.6234 30-OCT-05 ST_CLERK\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   3200             -         120            50\n2   2700             -         120            50\n3   2400             -         120            50\n4   2200             -         120            50\n5   3300             -         121            50\n6   2800             -         121            50\n\n\n\nfilter(employtable,SALARY > 5000) %>% head()\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE     JOB_ID\n1         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04     MK_MAN\n2         202        Pat       Fay     PFAY 603.123.6666 17-AUG-05     MK_REP\n3         203      Susan    Mavris  SMAVRIS 515.123.7777 07-JUN-02     HR_REP\n4         204    Hermann      Baer    HBAER 515.123.8888 07-JUN-02     PR_REP\n5         205    Shelley   Higgins SHIGGINS 515.123.8080 07-JUN-02     AC_MGR\n6         206    William     Gietz   WGIETZ 515.123.8181 07-JUN-02 AC_ACCOUNT\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1  13000             -         100            20\n2   6000             -         201            20\n3   6500             -         101            40\n4  10000             -         101            70\n5  12008             -         101           110\n6   8300             -         205           110"
  },
  {
    "objectID": "posts/R basic/R basic.html#정렬",
    "href": "posts/R basic/R basic.html#정렬",
    "title": "R basics",
    "section": "정렬",
    "text": "정렬\narrange() 사용\nSALARY 순으로 정렬\n\nhead(arrange(employtable,SALARY))\n\n  EMPLOYEE_ID FIRST_NAME  LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         132         TJ      Olson  TJOLSON 650.124.8234 10-APR-07 ST_CLERK\n2         128     Steven     Markle  SMARKLE 650.124.1434 08-MAR-08 ST_CLERK\n3         136      Hazel Philtanker HPHILTAN 650.127.1634 06-FEB-08 ST_CLERK\n4         127      James     Landry  JLANDRY 650.124.1334 14-JAN-07 ST_CLERK\n5         135         Ki        Gee     KGEE 650.127.1734 12-DEC-07 ST_CLERK\n6         119      Karen Colmenares KCOLMENA 515.127.4566 10-AUG-07 PU_CLERK\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   2100             -         121            50\n2   2200             -         120            50\n3   2200             -         122            50\n4   2400             -         120            50\n5   2400             -         122            50\n6   2500             -         114            30\n\nemploytable %>% arrange(SALARY) %>% head()\n\n  EMPLOYEE_ID FIRST_NAME  LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE   JOB_ID\n1         132         TJ      Olson  TJOLSON 650.124.8234 10-APR-07 ST_CLERK\n2         128     Steven     Markle  SMARKLE 650.124.1434 08-MAR-08 ST_CLERK\n3         136      Hazel Philtanker HPHILTAN 650.127.1634 06-FEB-08 ST_CLERK\n4         127      James     Landry  JLANDRY 650.124.1334 14-JAN-07 ST_CLERK\n5         135         Ki        Gee     KGEE 650.127.1734 12-DEC-07 ST_CLERK\n6         119      Karen Colmenares KCOLMENA 515.127.4566 10-AUG-07 PU_CLERK\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1   2100             -         121            50\n2   2200             -         120            50\n3   2200             -         122            50\n4   2400             -         120            50\n5   2400             -         122            50\n6   2500             -         114            30\n\n\n역순으로 할때는 desc() 사용\n\nhead(arrange(employtable,desc(SALARY)))\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE  JOB_ID\n1         100     Steven      King    SKING 515.123.4567 17-JUN-03 AD_PRES\n2         101      Neena   Kochhar NKOCHHAR 515.123.4568 21-SEP-05   AD_VP\n3         102        Lex   De Haan  LDEHAAN 515.123.4569 13-JAN-01   AD_VP\n4         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04  MK_MAN\n5         205    Shelley   Higgins SHIGGINS 515.123.8080 07-JUN-02  AC_MGR\n6         108      Nancy Greenberg NGREENBE 515.124.4569 17-AUG-02  FI_MGR\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1  24000             -          -             90\n2  17000             -         100            90\n3  17000             -         100            90\n4  13000             -         100            20\n5  12008             -         101           110\n6  12008             -         101           100\n\nemploytable %>% arrange(desc(SALARY)) %>% head()\n\n  EMPLOYEE_ID FIRST_NAME LAST_NAME    EMAIL PHONE_NUMBER HIRE_DATE  JOB_ID\n1         100     Steven      King    SKING 515.123.4567 17-JUN-03 AD_PRES\n2         101      Neena   Kochhar NKOCHHAR 515.123.4568 21-SEP-05   AD_VP\n3         102        Lex   De Haan  LDEHAAN 515.123.4569 13-JAN-01   AD_VP\n4         201    Michael Hartstein MHARTSTE 515.123.5555 17-FEB-04  MK_MAN\n5         205    Shelley   Higgins SHIGGINS 515.123.8080 07-JUN-02  AC_MGR\n6         108      Nancy Greenberg NGREENBE 515.124.4569 17-AUG-02  FI_MGR\n  SALARY COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n1  24000             -          -             90\n2  17000             -         100            90\n3  17000             -         100            90\n4  13000             -         100            20\n5  12008             -         101           110\n6  12008             -         101           100\n\n\n\n인덱싱\n조건에 맞는 데이터 추출\n파이프 연산자에 익숙해지기"
  },
  {
    "objectID": "posts/R basic/R basic.html#가-작업경로-확인-및-불러오기",
    "href": "posts/R basic/R basic.html#가-작업경로-확인-및-불러오기",
    "title": "R basics",
    "section": "가) 작업경로 확인 및 불러오기",
    "text": "가) 작업경로 확인 및 불러오기\n\nlibrary(dplyr)\nacc_death = read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/acc_death.csv\")\nacc_death %>% head()\n\n  발생년     발생년월일시 주야 요일 사망자수 부상자수 중상자수 경상자수\n1   2021 2021-01-01 03:00   야   금        1        3        0        3\n2   2021 2021-01-01 09:00   주   금        1        0        0        0\n3   2021 2021-01-01 15:00   주   금        1        0        0        0\n4   2021 2021-01-01 19:00   야   금        1        0        0        0\n5   2021 2021-01-01 21:00   야   금        1        0        0        0\n6   2021 2021-01-01 21:00   야   금        1        0        0        0\n  부상신고자수 발생지시도 발생지시군구 사고유형_대분류 사고유형_중분류\n1            0       경북       군위군          차대차            추돌\n2            0       충남       서산시        차량단독      공작물충돌\n3            0       강원       강릉시          차대차        측면충돌\n4            0       전남       진도군        차대사람          횡단중\n5            0       경기       수원시        차대사람            기타\n6            0       전남       순천시        차대사람            기타\n    사고유형       가해자법규위반 도로형태_대분류   도로형태 가해자_당사자종별\n1       추돌 안전운전 의무 불이행          교차로 교차로부근            승용차\n2 공작물충돌 안전운전 의무 불이행          단일로 기타단일로            승용차\n3   측면충돌 안전운전 의무 불이행          교차로   교차로내  원동기장치자전거\n4     횡단중 안전운전 의무 불이행          단일로 기타단일로            화물차\n5       기타                 기타          단일로 기타단일로            승용차\n6       기타                 기타            기타       기타            승용차\n  피해자_당사자종별 발생위치X.UTMK. 발생위치Y.UTMK.     경도     위도\n1            승용차         1097010         1793385 128.5782 36.13265\n2              없음          902369         1847109 126.4082 36.61685\n3            승용차         1123975         1974509 128.9075 37.76184\n4            보행자          886507         1613961 126.2636 34.51339\n5            보행자          953522         1915403 126.9760 37.23633\n6            보행자         1001659         1662376 127.5182 34.95624"
  },
  {
    "objectID": "posts/R basic/R basic.html#나-데이터-확인",
    "href": "posts/R basic/R basic.html#나-데이터-확인",
    "title": "R basics",
    "section": "나) 데이터 확인",
    "text": "나) 데이터 확인\n\na. 데이터의 상위6개, 하위 6개 관측치를 미리보기 하시오\n\nacc_death %>% head(6)\n\n  발생년     발생년월일시 주야 요일 사망자수 부상자수 중상자수 경상자수\n1   2021 2021-01-01 03:00   야   금        1        3        0        3\n2   2021 2021-01-01 09:00   주   금        1        0        0        0\n3   2021 2021-01-01 15:00   주   금        1        0        0        0\n4   2021 2021-01-01 19:00   야   금        1        0        0        0\n5   2021 2021-01-01 21:00   야   금        1        0        0        0\n6   2021 2021-01-01 21:00   야   금        1        0        0        0\n  부상신고자수 발생지시도 발생지시군구 사고유형_대분류 사고유형_중분류\n1            0       경북       군위군          차대차            추돌\n2            0       충남       서산시        차량단독      공작물충돌\n3            0       강원       강릉시          차대차        측면충돌\n4            0       전남       진도군        차대사람          횡단중\n5            0       경기       수원시        차대사람            기타\n6            0       전남       순천시        차대사람            기타\n    사고유형       가해자법규위반 도로형태_대분류   도로형태 가해자_당사자종별\n1       추돌 안전운전 의무 불이행          교차로 교차로부근            승용차\n2 공작물충돌 안전운전 의무 불이행          단일로 기타단일로            승용차\n3   측면충돌 안전운전 의무 불이행          교차로   교차로내  원동기장치자전거\n4     횡단중 안전운전 의무 불이행          단일로 기타단일로            화물차\n5       기타                 기타          단일로 기타단일로            승용차\n6       기타                 기타            기타       기타            승용차\n  피해자_당사자종별 발생위치X.UTMK. 발생위치Y.UTMK.     경도     위도\n1            승용차         1097010         1793385 128.5782 36.13265\n2              없음          902369         1847109 126.4082 36.61685\n3            승용차         1123975         1974509 128.9075 37.76184\n4            보행자          886507         1613961 126.2636 34.51339\n5            보행자          953522         1915403 126.9760 37.23633\n6            보행자         1001659         1662376 127.5182 34.95624\n\nacc_death %>% tail(6)\n\n     발생년     발생년월일시 주야 요일 사망자수 부상자수 중상자수 경상자수\n2811   2021 2021-12-31 11:00   주   금        1        1        0        1\n2812   2021 2021-12-31 16:00   주   금        1        0        0        0\n2813   2021 2021-12-31 17:00   주   금        1        0        0        0\n2814   2021 2021-12-31 18:00   야   금        1        0        0        0\n2815   2021 2021-12-31 19:00   야   금        1        0        0        0\n2816   2021 2021-12-31 21:00   야   금        1        0        0        0\n     부상신고자수 발생지시도 발생지시군구 사고유형_대분류 사고유형_중분류\n2811            0       부산       사하구          차대차            기타\n2812            0       경북       경산시          차대차        정면충돌\n2813            0       제주       제주시          차대차            추돌\n2814            0       강원       춘천시        차대사람          횡단중\n2815            0       경북       상주시        차대사람          횡단중\n2816            0       강원       양구군        차량단독            전복\n     사고유형       가해자법규위반 도로형태_대분류         도로형태\n2811     기타             신호위반          교차로         교차로내\n2812 정면충돌 안전운전 의무 불이행          교차로         교차로내\n2813     추돌 안전운전 의무 불이행          단일로       기타단일로\n2814   횡단중 보행자 보호의무 위반          단일로       기타단일로\n2815   횡단중 보행자 보호의무 위반          교차로 교차로횡단보도내\n2816     전복          중앙선 침범          단일로       기타단일로\n     가해자_당사자종별 피해자_당사자종별 발생위치X.UTMK. 발생위치Y.UTMK.\n2811            이륜차            이륜차         1136096         1678718\n2812            승용차            이륜차         1119020         1766895\n2813            화물차            화물차          940588         1503050\n2814            승용차            보행자         1023127         1982332\n2815            승용차            보행자         1058805         1824755\n2816            승용차              없음         1042559         2010975\n         경도     위도\n2811 128.9931 35.09441\n2812 128.8187 35.89143\n2813 126.8602 33.51770\n2814 127.7628 37.84046\n2815 128.1559 36.41852\n2816 127.9854 38.09791\n\n\n\n\nb. 데이터 구조, 변수개수, 변수 명, 관측치 개수를 학인하시오\n\nacc_death %>% str()\n\n'data.frame':   2816 obs. of  23 variables:\n $ 발생년           : int  2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 ...\n $ 발생년월일시     : chr  \"2021-01-01 03:00\" \"2021-01-01 09:00\" \"2021-01-01 15:00\" \"2021-01-01 19:00\" ...\n $ 주야             : chr  \"야\" \"주\" \"주\" \"야\" ...\n $ 요일             : chr  \"금\" \"금\" \"금\" \"금\" ...\n $ 사망자수         : int  1 1 1 1 1 1 1 1 1 1 ...\n $ 부상자수         : int  3 0 0 0 0 0 0 3 0 0 ...\n $ 중상자수         : int  0 0 0 0 0 0 0 1 0 0 ...\n $ 경상자수         : int  3 0 0 0 0 0 0 2 0 0 ...\n $ 부상신고자수     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ 발생지시도       : chr  \"경북\" \"충남\" \"강원\" \"전남\" ...\n $ 발생지시군구     : chr  \"군위군\" \"서산시\" \"강릉시\" \"진도군\" ...\n $ 사고유형_대분류  : chr  \"차대차\" \"차량단독\" \"차대차\" \"차대사람\" ...\n $ 사고유형_중분류  : chr  \"추돌\" \"공작물충돌\" \"측면충돌\" \"횡단중\" ...\n $ 사고유형         : chr  \"추돌\" \"공작물충돌\" \"측면충돌\" \"횡단중\" ...\n $ 가해자법규위반   : chr  \"안전운전 의무 불이행\" \"안전운전 의무 불이행\" \"안전운전 의무 불이행\" \"안전운전 의무 불이행\" ...\n $ 도로형태_대분류  : chr  \"교차로\" \"단일로\" \"교차로\" \"단일로\" ...\n $ 도로형태         : chr  \"교차로부근\" \"기타단일로\" \"교차로내\" \"기타단일로\" ...\n $ 가해자_당사자종별: chr  \"승용차\" \"승용차\" \"원동기장치자전거\" \"화물차\" ...\n $ 피해자_당사자종별: chr  \"승용차\" \"없음\" \"승용차\" \"보행자\" ...\n $ 발생위치X.UTMK.  : num  1097010 902369 1123975 886507 953522 ...\n $ 발생위치Y.UTMK.  : num  1793385 1847109 1974509 1613961 1915403 ...\n $ 경도             : num  129 126 129 126 127 ...\n $ 위도             : num  36.1 36.6 37.8 34.5 37.2 ...\n\n\n\n\nc. 데이터 객체의 차원(행, 열) 을 확인하시오\n\nacc_death %>% dim()\n\n[1] 2816   23"
  },
  {
    "objectID": "posts/R basic/R basic.html#다-데이터-전처리",
    "href": "posts/R basic/R basic.html#다-데이터-전처리",
    "title": "R basics",
    "section": "다) 데이터 전처리",
    "text": "다) 데이터 전처리\n“발생년월일시” 열을 활용하여 년월일”yyyymmdd” (numeric) 와 시분”hhmm” (numeric) 을 나타내는 새로운 열을 추가하시오\n\nacc_death$yyyymmdd <- acc_death$발생년월일시 %>% \n  as.POSIXct() %>% \n  as.character(\"%Y%m%d\") %>% \n  as.numeric()\n\n\nacc_death$hhmm <- acc_death$발생년월일시 %>% \n  as.POSIXct(format = \"%Y-%m-%d %H:%M\") %>% \n  as.character(\"%H%M\") %>% \n  as.numeric()\n\n\nacc_death %>% str()\n\n'data.frame':   2816 obs. of  25 variables:\n $ 발생년           : int  2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 ...\n $ 발생년월일시     : chr  \"2021-01-01 03:00\" \"2021-01-01 09:00\" \"2021-01-01 15:00\" \"2021-01-01 19:00\" ...\n $ 주야             : chr  \"야\" \"주\" \"주\" \"야\" ...\n $ 요일             : chr  \"금\" \"금\" \"금\" \"금\" ...\n $ 사망자수         : int  1 1 1 1 1 1 1 1 1 1 ...\n $ 부상자수         : int  3 0 0 0 0 0 0 3 0 0 ...\n $ 중상자수         : int  0 0 0 0 0 0 0 1 0 0 ...\n $ 경상자수         : int  3 0 0 0 0 0 0 2 0 0 ...\n $ 부상신고자수     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ 발생지시도       : chr  \"경북\" \"충남\" \"강원\" \"전남\" ...\n $ 발생지시군구     : chr  \"군위군\" \"서산시\" \"강릉시\" \"진도군\" ...\n $ 사고유형_대분류  : chr  \"차대차\" \"차량단독\" \"차대차\" \"차대사람\" ...\n $ 사고유형_중분류  : chr  \"추돌\" \"공작물충돌\" \"측면충돌\" \"횡단중\" ...\n $ 사고유형         : chr  \"추돌\" \"공작물충돌\" \"측면충돌\" \"횡단중\" ...\n $ 가해자법규위반   : chr  \"안전운전 의무 불이행\" \"안전운전 의무 불이행\" \"안전운전 의무 불이행\" \"안전운전 의무 불이행\" ...\n $ 도로형태_대분류  : chr  \"교차로\" \"단일로\" \"교차로\" \"단일로\" ...\n $ 도로형태         : chr  \"교차로부근\" \"기타단일로\" \"교차로내\" \"기타단일로\" ...\n $ 가해자_당사자종별: chr  \"승용차\" \"승용차\" \"원동기장치자전거\" \"화물차\" ...\n $ 피해자_당사자종별: chr  \"승용차\" \"없음\" \"승용차\" \"보행자\" ...\n $ 발생위치X.UTMK.  : num  1097010 902369 1123975 886507 953522 ...\n $ 발생위치Y.UTMK.  : num  1793385 1847109 1974509 1613961 1915403 ...\n $ 경도             : num  129 126 129 126 127 ...\n $ 위도             : num  36.1 36.6 37.8 34.5 37.2 ...\n $ yyyymmdd         : num  20210101 20210101 20210101 20210101 20210101 ...\n $ hhmm             : num  300 900 1500 1900 2100 2100 2200 2200 200 700 ..."
  },
  {
    "objectID": "posts/R basic/R basic.html#라-데이터-분석",
    "href": "posts/R basic/R basic.html#라-데이터-분석",
    "title": "R basics",
    "section": "라) 데이터 분석",
    "text": "라) 데이터 분석\n\na. 2021년에 “사망자수”, “부상자수”, “중상자수”, “경상자수”, “부상신고자수” 각 열의 총 합을 계산하시오\n\nlapply(acc_death[,5:9],sum)\n\n$사망자수\n[1] 2916\n\n$부상자수\n[1] 1325\n\n$중상자수\n[1] 545\n\n$경상자수\n[1] 728\n\n$부상신고자수\n[1] 52\n\n\n\n\nb. 사망사고가 가장 많이 발생한 날짜는 몇월 몇일인가\n\nacc_death %>% group_by(yyyymmdd) %>% \n  summarize(death_count = n()) %>% \n  arrange(desc(death_count))\n\n# A tibble: 365 × 2\n   yyyymmdd death_count\n      <dbl>       <int>\n 1 20211125          19\n 2 20211027          17\n 3 20211028          17\n 4 20211101          17\n 5 20211014          16\n 6 20211103          16\n 7 20210121          15\n 8 20210610          15\n 9 20211029          15\n10 20211209          15\n# … with 355 more rows\n\n\n\n\nc. 사망사고는 주간, 야간 중 언제 많이 발생하였는가\n\ntable(acc_death$주야)\n\n\n  야   주 \n1261 1555 \n\n\n\n\nd. 대전 지역의 사망사고 건수는 몇건인가\n\nacc_death %>% \n  filter(발생지시도==\"대전\") %>% \n  count()\n\n   n\n1 59\n\n\n\n\ne. 발생지시도 열을 기준으로 지역별 사망사고 건수를 계산하시오, 대전은 몇 위인가\n\nacc_death %>% \n  group_by(발생지시도) %>% \n  summarize(death_count = n()) %>% \n  arrange(desc(death_count))\n\n# A tibble: 17 × 2\n   발생지시도 death_count\n   <chr>            <int>\n 1 경기               525\n 2 경북               326\n 3 경남               254\n 4 충남               248\n 5 서울               238\n 6 전남               234\n 7 전북               188\n 8 충북               156\n 9 강원               137\n10 부산               111\n11 인천               102\n12 대구                81\n13 대전                59\n14 울산                50\n15 광주                49\n16 제주                48\n17 세종                10\n\n\n\n\nf. 요일별로 사망사고 건수를 계산하시오\n\nacc_death$요일 %>% table()\n\n.\n 금  목  수  월  일  토  화 \n427 449 404 433 340 368 395 \n\n\n\n\ng. 월별 사망사고 건수를 계산하시오\n\ntable(substr(acc_death$yyyymmdd,5,6))\n\n\n 01  02  03  04  05  06  07  08  09  10  11  12 \n194 191 205 202 250 217 241 238 238 307 275 258 \n\n\n\n\nh. “사고유형”별 발생 비율을 계산하시오\n\nacc_death %>% group_by(사고유형) %>% \n  summarize(건수 = n()) %>% \n  mutate(비율 = 건수/sum(건수)*100)\n\n# A tibble: 15 × 3\n   사고유형              건수    비율\n   <chr>                <int>   <dbl>\n 1 공작물충돌             326 11.6   \n 2 기타                   712 25.3   \n 3 길가장자리구역통행중    30  1.07  \n 4 도로이탈 기타           17  0.604 \n 5 도로이탈 추락           67  2.38  \n 6 보도통행중              21  0.746 \n 7 전도                    81  2.88  \n 8 전복                    30  1.07  \n 9 정면충돌               191  6.78  \n10 주/정차차량 충돌         3  0.107 \n11 차도통행중             146  5.18  \n12 추돌                   355 12.6   \n13 측면충돌               394 14.0   \n14 횡단중                 441 15.7   \n15 후진중충돌               2  0.0710\n\n\n\n\ni. “도로형태”의 “교차로내” 에서 발생한 사고는 몇건인가\n\nacc_death %>% filter(도로형태==\"교차로내\") %>% count()\n\n    n\n1 574"
  },
  {
    "objectID": "posts/R basic/R basic.html#마-1.-화물차",
    "href": "posts/R basic/R basic.html#마-1.-화물차",
    "title": "R basics",
    "section": "마) 1. 화물차",
    "text": "마) 1. 화물차\n화물차는 일반승용차에 비해서 사고 발생시 사망사고를 일으킬 위험성이 큽니 다. 정부는 화물차 사고의 심각성을 깨닫고 화물차로 인한 사망사고 줄이기 캠 페인을 펼칠 예정입니다. 이에 앞서, 화물차 사망사고의 심각성을 데이터에 기 반하여 살펴보고 대책을 마련하고자 합니다. 아래에 해당되는 답을 데이터를 활용하여 찾아보세요. ### a. 화물차가 일으킨(가해자_당사자종별) 사망사고는 몇% 인가\n\ncount(filter(acc_death,가해자_당사자종별==\"화물차\"))/nrow(acc_death) * 100\n\n         n\n1 23.40199\n\n\n\nb. 화물차가 일으킨 사망사고 중, “가해자법규위반” 별로 사고 건수를 계산하시오\n\nacc_death %>% \n  filter(가해자_당사자종별==\"화물차\") %>% \n  group_by(가해자법규위반) %>% \n  summarize(건수= n()) %>% \n  arrange(desc(건수))\n\n# A tibble: 7 × 2\n  가해자법규위반        건수\n  <chr>                <int>\n1 안전운전 의무 불이행   483\n2 신호위반                41\n3 기타                    40\n4 보행자 보호의무 위반    36\n5 중앙선 침범             36\n6 안전거리 미확보         14\n7 교차로 통행방법 위반     9\n\n\n\n\nc. 화물차가 일으킨 사고의 시간대별 사고건수를 계산하시오\n\nacc_death %>% \n  filter(가해자_당사자종별==\"화물차\") %>% \n  group_by(hhmm) %>% \n  summarize(건수= n()) %>% \n  arrange(desc(건수))\n\n# A tibble: 24 × 2\n    hhmm  건수\n   <dbl> <int>\n 1  1100    50\n 2  1200    42\n 3  1500    42\n 4  1400    41\n 5  1800    41\n 6   600    38\n 7  1000    38\n 8  1300    38\n 9   800    35\n10  1700    34\n# … with 14 more rows\n\n\n\n\nd. 화물차(가해자)와 보행자(피해자)인 사망사고 건수는 몇건인가\n\nacc_death %>% \n  filter(가해자_당사자종별==\"화물차\" & 피해자_당사자종별==\"보행자\") %>% \n  count()\n\n    n\n1 224"
  },
  {
    "objectID": "posts/R basic/R basic.html#바-2.-이륜차",
    "href": "posts/R basic/R basic.html#바-2.-이륜차",
    "title": "R basics",
    "section": "바) 2. 이륜차",
    "text": "바) 2. 이륜차\n1인가구 증가와 배달문화의 확대로 이륜차 사고가 증가하고 있다. 정부에서는 이륜차 사고의 심각성을 파악하고자 한다. 아래의 물음에 답하세요. ### a. 이륜차와 연관된 사망사고는 몇건인가(가해자 or 피해자)\n\nacc_death %>% \n  filter(가해자_당사자종별==\"이륜차\"|피해자_당사자종별==\"이륜차\") %>% \n  count()\n\n    n\n1 588\n\n\n\nb. 월별 이륜차 사고 건수를 계산한 뒤 그래프를 그리시오\nplot()``{base}\n\nacc_death %>% \n  filter(가해자_당사자종별==\"이륜차\"|피해자_당사자종별==\"이륜차\") %>% \n  group_by(substr(yyyymmdd,5,6)) %>% \n  summarize(n=n()) %>% \n  plot(main = \"월별 이륜차 사고 건수\",\n       xlab = \"2021년 월\",\n       ylab = \"이륜차 사고 건수(월별)\",\n       ylim = c(0,80),\n       type = \"o\",\n       col = \"red\")\n\n\n\n\nggplot()\n\nlibrary(ggplot2)\n\n\nd <- acc_death %>% \n  filter(가해자_당사자종별==\"이륜차\"|피해자_당사자종별==\"이륜차\") %>% \n  group_by(substr(yyyymmdd,5,6)) %>% \n  summarize(n=n())\n \ncolnames(d) <- c(\"month\", \"n\")\nd$month <- d$month %>% as.numeric()\n\nd %>% ggplot(aes(month,n))+\n  geom_point()+\n  geom_line(color = \"red\")+\n  scale_x_continuous(breaks = seq(1,12,1),\n                     name = \"2021년 월\")+\n  scale_y_continuous(limits = c(10,80),\n                     breaks = seq(10,80,5),\n                     name = \"이륜차 사고 건수(월별)\")+\n  theme_light()+\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/SQL/SQL.html",
    "href": "posts/SQL/SQL.html",
    "title": "SQL",
    "section": "",
    "text": "rm(list=ls())\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(DBI)\nlibrary(RSQLite)\nlibrary(dbplyr)\n\n\n다음의 패키지를 부착합니다: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\n\n\nus_census<-read_csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/drive-download-20230405T011215Z-001/US_census.csv\")\n\nRows: 3143 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): state, name\ndbl (51): FIPS, pop2010, pop2000, age_under_5, age_under_18, age_over_65, fe...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#us_census %>% View()\n\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \n                      dbname = \"testsqlite\")\ncopy_to(dest = con, \n        df = us_census, \n        name = \"us_census\")\n\ncopy_to(dest = con,\n        df = mtcars,\n        name = \"mtcars\")\n\n\ndbListTables(con)\n\n[1] \"mtcars\"       \"sqlite_stat1\" \"sqlite_stat4\" \"us_census\"   \n\n\n\nSELECT * FROM us_census\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\nname\nFIPS\npop2010\npop2000\nage_under_5\nage_under_18\nage_over_65\nfemale\nwhite\nblack\nnative\nasian\npac_isl\ntwo_plus_races\nhispanic\nwhite_not_hispanic\nno_move_in_one_plus_year\nforeign_born\nforeign_spoken_at_home\nhs_grad\nbachelors\nveterans\nmean_work_travel\nhousing_units\nhome_ownership\nhousing_multi_unit\nmedian_val_owner_occupied\nhouseholds\npersons_per_household\nper_capita_income\nmedian_household_income\npoverty\nprivate_nonfarm_establishments\nprivate_nonfarm_employment\npercent_change_private_nonfarm_employment\nnonemployment_establishments\nfirms\nblack_owned_firms\nnative_owned_firms\nasian_owned_firms\npac_isl_owned_firms\nhispanic_owned_firms\nwomen_owned_firms\nmanufacturer_shipments_2007\nmercent_whole_sales_2007\nsales\nsales_per_capita\naccommodation_food_service\nbuilding_permits\nfed_spending\narea\ndensity\n\n\n\n\nAlabama\nAutauga County\n1001\n54571\n43671\n6.6\n26.8\n12.0\n51.3\n78.5\n17.7\n0.4\n0.9\nNA\n1.6\n2.4\n77.2\n86.3\n2.0\n3.7\n85.3\n21.7\n5817\n25.1\n22135\n77.5\n7.2\n133900\n19718\n2.70\n24568\n53255\n10.6\n877\n10628\n16.6\n2971\n4067\n15.2\nNA\n1.3\nNA\n0.7\n31.7\nNA\nNA\n598175\n12003\n88157\n191\n331142\n594.44\n91.8\n\n\nAlabama\nBaldwin County\n1003\n182265\n140415\n6.1\n23.0\n16.8\n51.1\n85.7\n9.4\n0.7\n0.7\nNA\n1.5\n4.4\n83.5\n83.0\n3.6\n5.5\n87.6\n26.8\n20396\n25.8\n104061\n76.7\n22.6\n177200\n69476\n2.50\n26469\n50147\n12.2\n4812\n52233\n17.4\n14175\n19035\n2.7\n0.4\n1.0\nNA\n1.3\n27.3\n1410273\nNA\n2966489\n17166\n436955\n696\n1119082\n1589.78\n114.6\n\n\nAlabama\nBarbour County\n1005\n27457\n29038\n6.2\n21.9\n14.2\n46.9\n48.0\n46.9\n0.4\n0.4\nNA\n0.9\n5.1\n46.8\n83.0\n2.8\n4.7\n71.9\n13.5\n2327\n23.8\n11829\n68.0\n11.1\n88200\n9795\n2.52\n15875\n33219\n25.0\n522\n7990\n-27.0\n1527\n1667\nNA\nNA\nNA\nNA\nNA\n27.0\nNA\nNA\n188337\n6334\nNA\n10\n240308\n884.88\n31.0\n\n\nAlabama\nBibb County\n1007\n22915\n20826\n6.0\n22.7\n12.7\n46.3\n75.8\n22.0\n0.3\n0.1\nNA\n0.9\n1.8\n75.0\n90.5\n0.7\n1.5\n74.5\n10.0\n1883\n28.3\n8981\n82.9\n6.6\n81200\n7441\n3.02\n19918\n41770\n12.6\n318\n2927\n-14.0\n1192\n1385\n14.9\nNA\nNA\nNA\nNA\nNA\n0\nNA\n124707\n5804\n10757\n8\n163201\n622.58\n36.8\n\n\nAlabama\nBlount County\n1009\n57322\n51024\n6.3\n24.6\n14.7\n50.5\n92.6\n1.3\n0.5\n0.2\nNA\n1.2\n8.1\n88.9\n87.2\n4.7\n7.2\n74.7\n12.5\n4072\n33.2\n23887\n82.0\n3.7\n113700\n20605\n2.73\n21070\n45549\n13.4\n749\n6968\n-11.4\n3501\n4458\nNA\nNA\nNA\nNA\nNA\n23.2\n341544\nNA\n319700\n5622\n20941\n18\n294114\n644.78\n88.9\n\n\nAlabama\nBullock County\n1011\n10914\n11714\n6.8\n22.3\n13.5\n45.8\n23.0\n70.2\n0.2\n0.2\nNA\n0.8\n7.1\n21.9\n88.5\n1.1\n3.8\n74.7\n12.0\n943\n28.1\n4493\n76.9\n9.9\n66300\n3732\n2.85\n20289\n31602\n25.3\n120\n1919\n-18.5\n390\n417\nNA\nNA\nNA\nNA\nNA\n38.8\nNA\nNA\n43810\n3995\n3670\n1\n108846\n622.81\n17.5\n\n\nAlabama\nButler County\n1013\n20947\n21399\n6.5\n24.1\n16.7\n53.0\n54.4\n43.4\n0.3\n0.8\n0.0\n0.8\n0.9\n54.1\n92.8\n1.1\n1.6\n74.8\n11.0\n1675\n25.1\n9964\n69.0\n13.7\n70200\n8019\n2.58\n16916\n30659\n25.0\n446\n5400\n2.1\n1180\n1769\nNA\nNA\n3.3\nNA\nNA\nNA\n399132\n56712\n229277\n11326\n28427\n3\n195055\n776.83\n27.0\n\n\nAlabama\nCalhoun County\n1015\n118572\n112249\n6.1\n22.9\n14.3\n51.8\n74.9\n20.6\n0.5\n0.7\n0.1\n1.7\n3.3\n73.6\n82.9\n2.5\n4.5\n78.5\n16.1\n11757\n22.1\n53289\n70.7\n14.3\n98200\n46421\n2.46\n20574\n38407\n19.5\n2444\n38324\n-5.6\n6329\n8713\n7.2\nNA\n1.6\nNA\n0.5\n24.7\n2679991\nNA\n1542981\n13678\n186533\n107\n1830659\n605.87\n195.7\n\n\nAlabama\nChambers County\n1017\n34215\n36583\n5.7\n22.5\n16.7\n52.2\n58.8\n38.7\n0.2\n0.5\n0.0\n1.1\n1.6\n58.1\n86.2\n0.9\n1.6\n71.8\n10.8\n2893\n23.6\n17004\n71.4\n8.7\n82200\n13681\n2.51\n16626\n31467\n20.3\n568\n6241\n-45.8\n2074\n1981\nNA\nNA\nNA\nNA\nNA\n29.3\n667283\nNA\n264650\n7620\n23237\n10\n294718\n596.53\n57.4\n\n\nAlabama\nCherokee County\n1019\n25989\n23988\n5.3\n21.4\n17.9\n50.4\n92.7\n4.6\n0.5\n0.2\n0.0\n1.5\n1.2\n92.1\n88.1\n0.5\n1.4\n73.4\n10.5\n2172\n26.2\n16267\n77.5\n4.3\n97100\n11352\n2.22\n21322\n40690\n17.6\n350\n3600\n5.4\n1627\n2180\nNA\nNA\nNA\nNA\nNA\n14.5\n307439\n62293\n186321\n7613\n13948\n6\n184642\n553.70\n46.9\n\n\n\n\n\n\nselect * from mtcars\n\n\nDisplaying records 1 - 10\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4"
  },
  {
    "objectID": "posts/Statistic/Statistic.html",
    "href": "posts/Statistic/Statistic.html",
    "title": "R Statistics",
    "section": "",
    "text": "example with mtcars dataset\n\nrm(list = ls())\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\n\n\nmtcars <- mtcars\nmtcars %>% head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "posts/Statistic/Statistic.html#calculate-linear-regression",
    "href": "posts/Statistic/Statistic.html#calculate-linear-regression",
    "title": "R Statistics",
    "section": "Calculate Linear Regression",
    "text": "Calculate Linear Regression\nTo perform linear regression, we use the lm() function.\nIn this case, the model is predicting mpg(fuel efficiency) using wt(weight).\n\nmodel <- lm(mpg ~ wt, data = mtcars)"
  },
  {
    "objectID": "posts/Statistic/Statistic.html#summary-of-the-regression-model",
    "href": "posts/Statistic/Statistic.html#summary-of-the-regression-model",
    "title": "R Statistics",
    "section": "Summary of the Regression model",
    "text": "Summary of the Regression model\nyou can get a summary of the model by using the summary() function.\nThe summary includes information on the residuals, coefficients, R-squared value, F-statistic and p-value.\n\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10"
  },
  {
    "objectID": "posts/Statistic/Statistic.html#interpret-the-linear-regression-results",
    "href": "posts/Statistic/Statistic.html#interpret-the-linear-regression-results",
    "title": "R Statistics",
    "section": "Interpret the Linear Regression Results",
    "text": "Interpret the Linear Regression Results\n\nThe output shows that the coefficient for wt (weight) coefficient is -5.34447, and the intercept is 37.28536.\nThe p-value is less than 0.05, indicating that the relationship between weight and fuel efficiency is statistically significant.\nThe R-squared value, which measures the proportion of the variation in the response variable explained by the predictor variable, is 0.7528. This means that the car’s weight can explain 75.28% of the variation in fuel efficiency."
  },
  {
    "objectID": "posts/Statistic/Statistic.html#plot-the-regression-line-in-a-graph",
    "href": "posts/Statistic/Statistic.html#plot-the-regression-line-in-a-graph",
    "title": "R Statistics",
    "section": "Plot the Regression Line in a Graph",
    "text": "Plot the Regression Line in a Graph\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point() +\n  geom_smooth(method='lm', se=FALSE) +\n  ggtitle(\"Linear Regression of mpg ~ wt\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_2 <- lm(mpg~wt+disp,data = mtcars)\n\nsummary(model_2)\n\n\nCall:\nlm(formula = mpg ~ wt + disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4087 -2.3243 -0.7683  1.7721  6.3484 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.96055    2.16454  16.151 4.91e-16 ***\nwt          -3.35082    1.16413  -2.878  0.00743 ** \ndisp        -0.01773    0.00919  -1.929  0.06362 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.917 on 29 degrees of freedom\nMultiple R-squared:  0.7809,    Adjusted R-squared:  0.7658 \nF-statistic: 51.69 on 2 and 29 DF,  p-value: 2.744e-10\n\nresid <- resid(model_2)\n\nhist(resid)\n\n\n\nqqnorm(resid)\nqqline(resid)\n\n\n\nplot(fitted(model_2),resid)\n\n\n\nplot(mtcars$wt,resid)"
  },
  {
    "objectID": "posts/Statistic/Statistic.html#prediction",
    "href": "posts/Statistic/Statistic.html#prediction",
    "title": "R Statistics",
    "section": "Prediction",
    "text": "Prediction\n\nnewdata <- data.frame(wt=c(3,4))\n\npredictions <- predict(model, newdata=newdata)\npredictions\n\n       1        2 \n21.25171 15.90724 \n\n\n\nCompare with calculated value\n\nbeta0 <- model$coefficients[[1]][1]\nbeta1 <- model$coefficients[[2]][1]\n\nnewdata_mpg <- data.frame(mpg = c(beta0 + beta1*3, beta0 + beta1*4))\nnewdata <- cbind(newdata,newdata_mpg)\nnewdata\n\n  wt      mpg\n1  3 21.25171\n2  4 15.90724"
  },
  {
    "objectID": "posts/tests/Issue_report.html#section",
    "href": "posts/tests/Issue_report.html#section",
    "title": "Issue Report",
    "section": "2023/03/27",
    "text": "2023/03/27\nQuarto markdown의 코드 청크가 전부 파이썬으로 구성되면 같은 디렉토리에 .ipynb가 생성된다.\nRstudio ipykernel에는 .ipynb를 렌더링 할 수 없기에 에러가 발생한다.\n이에 Quarto markdown 에 비어있는 R코드청크를 넣어두면 .ipynb가 생성되지 않고 렌더링 할 수 있다."
  },
  {
    "objectID": "posts/tests/Issue_report.html#section-1",
    "href": "posts/tests/Issue_report.html#section-1",
    "title": "Issue Report",
    "section": "2023/04/04",
    "text": "2023/04/04\n.ipynb 파일 위에 --- title --- 생성해보기\n\n#!pip install matplotlib\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = np.arange(-1,1,0.01)\nb = a**2\nplt.plot(a,b)"
  },
  {
    "objectID": "posts/vis/vis.html",
    "href": "posts/vis/vis.html",
    "title": "ggplot, ggplot",
    "section": "",
    "text": "Train markdown, ggplot https://clauswilke.com/dataviz/"
  },
  {
    "objectID": "posts/vis/vis.html#library-packages",
    "href": "posts/vis/vis.html#library-packages",
    "title": "ggplot, ggplot",
    "section": "library packages",
    "text": "library packages\n\nrm(list = ls())\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggrepel)\nlibrary(cowplot)\nlibrary(colorspace)\nlibrary(stringr)\nlibrary(geojsonsf)\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(forcats)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/vis/vis.html#mpg-datasets",
    "href": "posts/vis/vis.html#mpg-datasets",
    "title": "ggplot, ggplot",
    "section": "mpg datasets",
    "text": "mpg datasets\nRead data, check dimension and data summary\n\ndata_raw <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/mpg.csv\")\ndata_raw %>% dim()\n\n[1] 234  12\n\ndata_raw %>% head()\n\n  X manufacturer model displ year cyl      trans drv cty hwy fl   class\n1 1         audi    a4   1.8 1999   4   auto(l5)   f  18  29  p compact\n2 2         audi    a4   1.8 1999   4 manual(m5)   f  21  29  p compact\n3 3         audi    a4   2.0 2008   4 manual(m6)   f  20  31  p compact\n4 4         audi    a4   2.0 2008   4   auto(av)   f  21  30  p compact\n5 5         audi    a4   2.8 1999   6   auto(l5)   f  16  26  p compact\n6 6         audi    a4   2.8 1999   6 manual(m5)   f  18  26  p compact\n\ndata_raw %>% summary()\n\n       X          manufacturer          model               displ      \n Min.   :  1.00   Length:234         Length:234         Min.   :1.600  \n 1st Qu.: 59.25   Class :character   Class :character   1st Qu.:2.400  \n Median :117.50   Mode  :character   Mode  :character   Median :3.300  \n Mean   :117.50                                         Mean   :3.472  \n 3rd Qu.:175.75                                         3rd Qu.:4.600  \n Max.   :234.00                                         Max.   :7.000  \n      year           cyl           trans               drv           \n Min.   :1999   Min.   :4.000   Length:234         Length:234        \n 1st Qu.:1999   1st Qu.:4.000   Class :character   Class :character  \n Median :2004   Median :6.000   Mode  :character   Mode  :character  \n Mean   :2004   Mean   :5.889                                        \n 3rd Qu.:2008   3rd Qu.:8.000                                        \n Max.   :2008   Max.   :8.000                                        \n      cty             hwy             fl               class          \n Min.   : 9.00   Min.   :12.00   Length:234         Length:234        \n 1st Qu.:14.00   1st Qu.:18.00   Class :character   Class :character  \n Median :17.00   Median :24.00   Mode  :character   Mode  :character  \n Mean   :16.86   Mean   :23.44                                        \n 3rd Qu.:19.00   3rd Qu.:27.00                                        \n Max.   :35.00   Max.   :44.00                                        \n\n\nDrop index column followed during data load\n\ndata_use <- data_raw %>% select(-1)\ndata_use %>% head()\n\n  manufacturer model displ year cyl      trans drv cty hwy fl   class\n1         audi    a4   1.8 1999   4   auto(l5)   f  18  29  p compact\n2         audi    a4   1.8 1999   4 manual(m5)   f  21  29  p compact\n3         audi    a4   2.0 2008   4 manual(m6)   f  20  31  p compact\n4         audi    a4   2.0 2008   4   auto(av)   f  21  30  p compact\n5         audi    a4   2.8 1999   6   auto(l5)   f  16  26  p compact\n6         audi    a4   2.8 1999   6 manual(m5)   f  18  26  p compact\n\n\n\nscatter plot\nUse displ and hwy\ndispl : Displacement\nhwy : Highway fuel economy\n\nggplot(data_use, aes(x=displ, y=hwy)) +\n  geom_point()\n\n\n\n\n\n\nsmooth line\n\nggplot(data_use, aes(x=displ, y=hwy)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nscatter & smooth line\n\nggplot(data_use, aes(x=displ, y=hwy)) + \n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nBoxplot example\nhwy average per class\n\nmpg %>% \n  ggplot(mapping = aes(x=class, y=hwy, fill=class))+\n  geom_boxplot()\n\n\n\n\n\n\nGroup_by, errorbar example\n\nmpg %>% \n  group_by(class) %>%\n  summarize(mean = mean(hwy), sd = sd(hwy)) %>% \n  ggplot(aes(x=class, y=mean, fill=class))+\n  geom_bar(stat = \"identity\")+ \n  geom_errorbar(\n    aes(ymin=mean-sd, ymax=mean+sd),\n                width = 0.5,\n                position = position_dodge(width = 0.9))"
  },
  {
    "objectID": "posts/vis/vis.html#ncdc_normals",
    "href": "posts/vis/vis.html#ncdc_normals",
    "title": "ggplot, ggplot",
    "section": "ncdc_normals",
    "text": "ncdc_normals\nThe U.S. Climate Normals are a large suite of data products that provide information about typical climate conditions for thousands of locations across the United States.\nLoad data\n\ndata_file <- read.csv(\"C:/sanai_sungil/posts/vis/ncdc_normals.csv\")\n\nCheck data information\n\ndim(data_file)\n\n[1] 2745366       6\n\ndata_file %>% head()\n\n   station_id month day temperature flag       date\n1 AQW00061705     1   1        82.4    C 0000-01-01\n2 AQW00061705     1   2        82.4    C 0000-01-02\n3 AQW00061705     1   3        82.4    C 0000-01-03\n4 AQW00061705     1   4        82.4    C 0000-01-04\n5 AQW00061705     1   5        82.4    C 0000-01-05\n6 AQW00061705     1   6        82.4    C 0000-01-06\n\ndata_file %>% sapply(class) # 날짜 형식 변경\n\n station_id       month         day temperature        flag        date \n\"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\" \n\n\nConvert date format (character -> date)\n\ndata_file$date <- data_file$date %>% as.Date(\"%Y-%m-%d\")\n\nThere are 7501 measurement locations\n\ndata_file$station_id %>% unique() %>% length()\n\n[1] 7501\n\n\nSpecifying 4 Station and use join() function\n\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death valley\",\"San diego\",\"Houston\"))\n\n\ntemps_long <- data_file %>% inner_join(station_loc,by=\"station_id\")\ntemps_long %>% head()\n\n   station_id month day temperature flag       date     location\n1 USC00042319     1   1        51.0    S 0000-01-01 Death valley\n2 USC00042319     1   2        51.2    S 0000-01-02 Death valley\n3 USC00042319     1   3        51.3    S 0000-01-03 Death valley\n4 USC00042319     1   4        51.4    S 0000-01-04 Death valley\n5 USC00042319     1   5        51.6    S 0000-01-05 Death valley\n6 USC00042319     1   6        51.7    S 0000-01-06 Death valley\n\n\n\nLine plot by location\n\nggplot(temps_long, aes(x=date,y=temperature,color=location))+\n  geom_line()\n\n\n\n\nCreate a scale to display on the x-axis.\n\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\")  #Y는 대문자\ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\n\n\n\nLine plot, date x scale\n\nggplot(temps_long, aes(x=date, y=temperature, color=location))+\n  geom_line()+\n  scale_x_date(name=\"month\", \n               breaks = break_date,\n               labels = c(\"jan\", \"apr\", \"jul\", \"oct\", \"jan\"))+\n  theme_light()\n\n\n\n\n\n\nLine plot, date x scale, y limits, labs variable\n\nggplot(temps_long, aes(x=date, y=temperature, color=location))+\n  geom_line()+\n  scale_x_date(name=\"month\", \n               breaks = break_date,\n               labels = c(\"jan\", \"apr\", \"jul\", \"oct\", \"jan\"))+\n  scale_y_continuous(name = \"temp\",\n                     limits = c(20,105))+\n  theme_light()+\n  labs(title = \"title\", subtitle = \"subtitle\", caption = \"caption\", tag = \"tag\")\n\n\n\n\n\n\nTile plot, mean temperature by month, location\n\nmean_temps <- temps_long %>% \n  group_by(month,location) %>% \n  summarize(mean = mean(temperature)) %>% \n  ungroup() %>% \n  mutate(month = factor(month %>% paste(),\n                        levels = 1:12 %>% paste()))\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\nViridis is Color Palette\n\nggplot(mean_temps,aes(x = month, y = location, fill = mean))+\n  geom_tile(width = .95,height = 0.95)+\n  scale_fill_viridis_c(option = \"B\",begin = 0.15, end = 0.98,\n                       name = \"temperature\")+\n  coord_fixed(expand = FALSE)+\n  ylab(NULL)\n\n\n\n\n\n\nPie chart, Polar coordinates\nThe polar coordinate system is most commonly used for pie charts, which are a stacked bar chart in polar coordinates.\nformat(x,\"%B\") Outputs unabbreviated month\n\ndate_lab <- format(break_date,\"%B\")\ndate_lab\n\n[1] \"1월\"  \"4월\"  \"7월\"  \"10월\" \"1월\" \n\n\n\nggplot(temps_long, aes(x=date,y=temperature,color=location))+\n  geom_line(linewidth = 1.1)+\n  scale_x_date(name = \"month\",\n               breaks = break_date,\n               labels = date_lab)+\n  scale_y_continuous(name = \"temperature\",\n                     limits = c(0,105))+\n  coord_polar(theta = \"x\",\n              start = pi,\n              direction = 1)+\n  theme_light()+\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/vis/vis.html#obs-asos-2021-korea",
    "href": "posts/vis/vis.html#obs-asos-2021-korea",
    "title": "ggplot, ggplot",
    "section": "OBS ASOS 2021 Korea",
    "text": "OBS ASOS 2021 Korea\nObservations Automated Surface Observing System\nAverage, minimum, and maximum temperatures by region(서울, 대전, 세종, 제주)\nLoad data\n\ndata_2021 <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/OBS_ASOS_DD_20220308125952.csv\", fileEncoding = \"euc-kr\")\n\nCheck data information\n\ndata_2021 %>% dim()\n\n[1] 1460    6\n\ndata_2021 %>% head()\n\n  지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n1  108   서울 2021-01-01         -4.2         -9.8          1.6\n2  108   서울 2021-01-02         -5.0         -8.4         -1.4\n3  108   서울 2021-01-03         -5.6         -9.1         -2.0\n4  108   서울 2021-01-04         -3.5         -8.4          0.3\n5  108   서울 2021-01-05         -5.5         -9.9         -2.1\n6  108   서울 2021-01-06         -7.4        -12.0         -1.9\n\ndata_2021 %>% sapply(class)\n\n        지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n   \"integer\"  \"character\"  \"character\"    \"numeric\"    \"numeric\"    \"numeric\" \n\n\nConvert date format (character -> date)\n\ndata_2021$일시 <-data_2021$일시 %>% as.Date(\"%Y-%m-%d\")\ndata_2021 %>% sapply(class)\n\n        지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n   \"integer\"  \"character\"       \"Date\"    \"numeric\"    \"numeric\"    \"numeric\" \n\n\nSummary data\n\ndata_2021 %>% summary()\n\n      지점          지점명               일시             평균기온..C.   \n Min.   :108.0   Length:1460        Min.   :2021-01-01   Min.   :-14.90  \n 1st Qu.:126.8   Class :character   1st Qu.:2021-04-02   1st Qu.:  7.90  \n Median :158.5   Mode  :character   Median :2021-07-02   Median : 15.00  \n Mean   :166.0                      Mean   :2021-07-02   Mean   : 14.77  \n 3rd Qu.:197.8                      3rd Qu.:2021-10-01   3rd Qu.: 23.10  \n Max.   :239.0                      Max.   :2021-12-31   Max.   : 31.70  \n  최저기온..C.     최고기온..C.   \n Min.   :-19.10   Min.   :-10.70  \n 1st Qu.:  3.10   1st Qu.: 13.18  \n Median : 11.10   Median : 20.15  \n Mean   : 10.69   Mean   : 19.56  \n 3rd Qu.: 19.60   3rd Qu.: 27.70  \n Max.   : 28.10   Max.   : 36.50  \n\n\nCreate a scale to display on the x-axis.\n\ndate_21s <- \"2021-01-01\" %>% as.Date(\"%Y-%m-%d\")  #Y는 대문자\ndate_21e <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date_21 <- seq.Date(date_21s, date_21e, by = \"3 month\")\n\n\nPloting\n\nggplot(data_2021,aes(x = 일시,y = 평균기온..C., color = 지점명))+\n  geom_line(linewidth = 1) +\n  scale_x_date(name =\"월\", \n               breaks = break_date_21,\n               labels = c(\"1월\",\"4월\",\"7월\",\"10월\",\"1월\")) +\n  scale_y_continuous(name = \"평균기온\")+\n  theme_light()"
  },
  {
    "objectID": "posts/vis/vis.html#obs-asos-2022-korea",
    "href": "posts/vis/vis.html#obs-asos-2022-korea",
    "title": "ggplot, ggplot",
    "section": "OBS ASOS 2022 Korea",
    "text": "OBS ASOS 2022 Korea\nSame process\n\ndata_2022 <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/OBS_ASOS_DD_20230322080932.csv\", fileEncoding = \"euc-kr\")\ndata_2022$일시 <-data_2022$일시 %>% as.Date(\"%Y-%m-%d\")\n\n\ndata_2022 %>% summary()\n\n      지점          지점명               일시             평균기온..C.   \n Min.   :108.0   Length:2555        Min.   :2022-01-01   Min.   :-11.80  \n 1st Qu.:133.0   Class :character   1st Qu.:2022-04-02   1st Qu.:  8.20  \n Median :185.0   Mode  :character   Median :2022-07-02   Median : 16.40  \n Mean   :175.1                      Mean   :2022-07-02   Mean   : 15.27  \n 3rd Qu.:189.0                      3rd Qu.:2022-10-01   3rd Qu.: 23.00  \n Max.   :239.0                      Max.   :2022-12-31   Max.   : 32.20  \n                                                         NA's   :2       \n  최저기온..C.      최고기온..C.  \n Min.   :-13.800   Min.   :-8.60  \n 1st Qu.:  4.225   1st Qu.:12.30  \n Median : 12.600   Median :20.75  \n Mean   : 11.626   Mean   :19.49  \n 3rd Qu.: 19.800   3rd Qu.:27.20  \n Max.   : 28.900   Max.   :37.50  \n NA's   :1         NA's   :1      \n\n\n\ndate_22s <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\")  #Y는 대문자\ndate_22e <- \"2023-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date_22 <- seq.Date(date_22s, date_22e, by = \"3 month\")\n\n\ndata_2022$지점명 %>% unique()\n\n[1] \"서울\"   \"대전\"   \"제주\"   \"고산\"   \"성산\"   \"서귀포\" \"세종\"  \n\n\nFilter only 서울, 대전, 제주, 세종\n\ndata_2022_2 <- data_2022 %>% filter(data_2022$지점명 %in% c(\"서울\",\"대전\",\"제주\",\"세종\"))\ndata_2022_2$지점명 %>% unique()\n\n[1] \"서울\" \"대전\" \"제주\" \"세종\"\n\n\n\nploting\n\nggplot(data_2022_2,aes(x = 일시,y = 평균기온..C., color = 지점명))+\n  geom_line(linewidth = 1) +\n  scale_x_date(name =\"월\", \n               breaks = break_date_22,\n               labels = c(\"1월\",\"4월\",\"7월\",\"10월\",\"1월\")) +\n  scale_y_continuous(name = \"평균기온\")+\n  theme_light()\n\n\n\n\n\n\nTile plot, mean temperature by month, location\n\nd <- data_2022_2 %>% \n  mutate(month = 일시 %>% format(\"%B\")) %>% \n  group_by(month,지점명) %>% \n  summarize(mean = mean(평균기온..C.)) %>% \n  ungroup()\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\nggplot(d,aes(x = month, y = 지점명, fill = mean))+\n  geom_tile(width = .95,height = 0.95)+\n  scale_fill_viridis_c(option = \"B\",begin = 0.15, end = 0.99,\n                       name = \"temperature\")+\n  coord_fixed(expand = FALSE)+\n  ylab(NULL)\n\n\n\n\n\n\nPie chart, Polar coordinates\n\nggplot(data_2022_2,aes(x = 일시,y = 평균기온..C., color = 지점명))+\n  geom_line(linewidth = 1) +\n  scale_x_date(name =\"월\", \n               breaks = break_date_22,\n               labels = c(\"1월\",\"4월\",\"7월\",\"10월\",\"1월\")) +\n  scale_y_continuous(name = \"평균기온\",\n                     limits = c(-20,35))+\n  coord_polar(theta = \"x\",\n              start = pi,\n              direction = 1)+\n  theme_light()+\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/vis/vis.html#ncdc-normals-houston",
    "href": "posts/vis/vis.html#ncdc-normals-houston",
    "title": "ggplot, ggplot",
    "section": "ncdc normals Houston",
    "text": "ncdc normals Houston\nLoad & filter only Houston temperature\n\ndata_file <- read.csv(\"C:/sanai_sungil/posts/vis/ncdc_normals.csv\")\ndata_file$date <- data_file$date %>% as.Date(\"%Y-%m-%d\")\n\nhouston <- data.frame(station_id = c(\"USW00012918\"), location = c(\"Houston\"))\n\nhouston_temps <- data_file %>% inner_join(houston,by=\"station_id\")\nhouston_temps %>% head()\n\n   station_id month day temperature flag       date location\n1 USW00012918     1   1        53.9    S 0000-01-01  Houston\n2 USW00012918     1   2        53.8    S 0000-01-02  Houston\n3 USW00012918     1   3        53.8    S 0000-01-03  Houston\n4 USW00012918     1   4        53.8    S 0000-01-04  Houston\n5 USW00012918     1   5        53.8    S 0000-01-05  Houston\n6 USW00012918     1   6        53.7    S 0000-01-06  Houston\n\n\n\nSet X scale breaks, Plot\n\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\")  #Y는 대문자\ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\n\nggplot(houston_temps, aes(x=date, y=temperature,color = location))+\n  geom_line(linewidth = 1,color = \"royalblue\")+\n  scale_x_date(name=\"month\", \n               breaks = break_date,\n               labels = c(\"jan\", \"apr\", \"jul\", \"oct\", \"jan\"))+\n  theme_light()+\n  ylab(\"Temperature(℉)\")\n\n\n\n\nUse cowplot package to arrange multiple plots into a grid\n\nlibrary(cowplot)\n\nSpecify plot as a object houston_plot\n\nhouston_plot <- ggplot(houston_temps, aes(x=date, y=temperature,color = location))+\n  geom_line(linewidth = 1,color = \"royalblue\")+\n  scale_x_date(name=\"month\", \n               breaks = break_date,\n               labels = c(\"jan\", \"apr\", \"jul\", \"oct\", \"jan\"))+\n  theme_light()+\n  ylab(\"Temperature(℉)\")\n\n\n\narrange multiple plots into a grid\n\nplot_ab <- plot_grid(houston_plot,houston_plot,\n                    nrow = 1,\n                    rel_widths = c(1,2),\n                    labels = c(\"a\",\"b\"))\n\nplot_abc <- plot_grid(plot_ab, houston_plot,\n                      ncol = 1,\n                      rel_heights = c(1.5,2),\n                      labels = c(\"\",\"c\"))\n\nplot_abc"
  },
  {
    "objectID": "posts/vis/vis.html#texas-counties",
    "href": "posts/vis/vis.html#texas-counties",
    "title": "ggplot, ggplot",
    "section": "Texas counties",
    "text": "Texas counties\nTexas counties by land area, population, and population density\n\ntexas_cnt <- read.csv(\"https://raw.githubusercontent.com/christianmendoza/texas-counties/main/data/texas-counties.csv\")\ntexas_cnt %>% head()\n\n     county fips_code population area_sq_mi pop_per_sq_mi\n1  Anderson     48001      58402       1071         54.53\n2   Andrews     48003      18440       1501         12.29\n3  Angelina     48005      86506        802        107.86\n4   Aransas     48007      24510        252         97.26\n5    Archer     48009       8681        910          9.54\n6 Armstrong     48011       1839        914          2.01\n\n\nSelect county, population columns\nremove “county” word in county columns ex) “sungil county” -> “sungil”\ncalculate popratio = population/ meadian of total population\nlist in order of highest population\nCreate an index column with a number, Add labels to the top 3, bottom 3, and randomly determined values\nrunif(x) Outputs a random number between 0 and 1\n\ntx_counties <- texas_cnt %>% \n  select(county,population) %>% \n  mutate(county = gsub(\"county\",\"\",county),\n         popratio = population/median(population)) %>% \n  arrange(desc(popratio)) %>% \n  mutate(index = 1:n(),\n         label = ifelse(index<=3|index>n()-3|runif(n())<.04, county, \"\"))\n\ntx_counties %>% head()\n\n   county population  popratio index   label\n1  Harris    4728030 256.42857     1  Harris\n2  Dallas    2586050 140.25654     2  Dallas\n3 Tarrant    2126477 115.33122     3 Tarrant\n4   Bexar    2028236 110.00304     4        \n5  Travis    1305154  70.78609     5        \n6  Collin    1109462  60.17258     6        \n\n\nggrepel provides geoms for ggplot2 to repel overlapping text labels\n\nlibrary(ggrepel)\n\n\npoint plot, overlapped text label\n\nggplot(tx_counties,aes(x=index,y=popratio))+\n  geom_hline(yintercept = 0, linetype = 2, color = \"grey40\")+\n  geom_point(size = 1, color = \"royalblue\")+\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100)+\n  theme_light()+\n  theme(panel.border = element_blank())\n\n\n\n\n\n\nPoint plot, y log scale\nThere are two main reasons to use logarithmic scales in charts and graphs. The first is to respond to skewness towards large values; i.e., cases in which one or a few points are much larger than the bulk of the data. The second is to show percent change or multiplicative factors.\n\nlabel_log10 <- sapply(-2:2,function(i) as.expression(bquote(10^ .(i))))\n\nggplot(tx_counties,aes(x=index,y=popratio))+\n  geom_hline(yintercept = 1, linetype = 2, color = \"grey40\")+\n  geom_point(size = 1, color = \"royalblue\")+\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100)+\n  scale_y_log10(name = \"popnumber/median\",\n                breaks = 10^(-2:2),\n                labels = label_log10)+\n  scale_x_continuous(name = \"Texas counties\",\n                     breaks = NULL)+\n  theme_light()+\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/vis/vis.html#시군구-인구수-2023년-2월",
    "href": "posts/vis/vis.html#시군구-인구수-2023년-2월",
    "title": "ggplot, ggplot",
    "section": "시군구 인구수 2023년 2월",
    "text": "시군구 인구수 2023년 2월\n\ndata_pop <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/%ED%96%89%EC%A0%95%EA%B5%AC%EC%97%AD_%EC%8B%9C%EA%B5%B0%EA%B5%AC_%EB%B3%84_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%84%B8%EB%8C%80%EC%88%98_202302.csv\",encoding = \"utf-8\")\n\ndata_pop %>% head()\n\n  행정구역.시군구.별 X2022.11 X2022.12 X2023.01 X2023.02\n1             종로구    72666    72524    72479    72773\n2               중구    63167    63139    63123    63492\n3             용산구   109905   109805   109734   109778\n4             성동구   133435   133305   133293   133517\n5             광진구   169376   169291   169416   169648\n6           동대문구   170154   169873   169716   170766\n\n\nSame process\n\ndata_202302 <- data_pop %>% \n  filter(X2023.02>0) %>%  \n  select(행정구역.시군구.별,X2023.02) %>%\n  mutate(popratio = X2023.02/median(X2023.02)) %>% \n  arrange(desc(popratio)) %>% \n  mutate(index = 1:n(),label = ifelse(index<=3|index>n()-3|runif(n())<.04, 행정구역.시군구.별, \"\"))\n\n\ndata_202302 %>% head()\n\n  행정구역.시군구.별 X2023.02 popratio index  label\n1             수원시   530462 6.043498     1 수원시\n2             고양시   462873 5.273464     2 고양시\n3             창원시   456357 5.199228     3 창원시\n4             용인시   432476 4.927154     4       \n5             성남시   409466 4.665003     5       \n6             청주시   394735 4.497175     6       \n\n\n\nPoint plot\n\nggplot(data_202302,aes(x=index,y=popratio))+\n  geom_point(size = 1, color = \"royalblue\")+\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100)+\n  theme_light()+\n  theme(panel.border = element_blank())\n\n\n\n\n\n\nPoint plot, y log scale\n\nlabel_log10 <- sapply(-2:2,function(i) as.expression(bquote(10^ .(i))))\n\nggplot(data_202302,aes(x=index,y=popratio))+\n  geom_point(size = 1, color = \"royalblue\")+\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100)+\n  scale_y_log10(name = \"인구수/중위수\",\n                breaks = 10^(-2:2),\n                labels = label_log10,\n                limits = c(10^-1.3,10^1.3))+\n  theme_light()+\n  theme(panel.border = element_blank())+\n  scale_x_continuous(name = \"행정구역 (시군구)별 주민등록세대수\",\n                     breaks = NULL)"
  },
  {
    "objectID": "posts/vis/vis.html#rcolorbrewer",
    "href": "posts/vis/vis.html#rcolorbrewer",
    "title": "ggplot, ggplot",
    "section": "RColorBrewer",
    "text": "RColorBrewer\n\nlibrary(colorspace)\nRColorBrewer::display.brewer.all()"
  },
  {
    "objectID": "posts/vis/vis.html#us-census-us-regions",
    "href": "posts/vis/vis.html#us-census-us-regions",
    "title": "ggplot, ggplot",
    "section": "US census , US regions",
    "text": "US census , US regions\nUS_census is dataset of population per region, age, human race\nUS_region is dataset of State informations\nLoad data\n\nUS_census <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/drive-download-20230405T011215Z-001/US_census.csv\")\n\nUS_regions <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/drive-download-20230405T011215Z-001/US_regions.csv\")\n\nJoin & group by State\nsummaise 2000 population, 2010 population, population growth, total area size\n\npopgrowth_df <- US_census %>% left_join(US_regions) %>% \n  group_by(region,division,state) %>% \n  summarize(pop2000 = sum(pop2000, na.rm = TRUE),\n            pop2010 = sum(pop2010, na.rm = TRUE),\n            popgrowth = (pop2010-pop2000)/pop2000,\n            area = sum(area)) %>% \n  arrange(popgrowth) %>% \n  ungroup() %>%\n  mutate(state = factor(state,levels = state),\n         region = factor(region, levels = c(\"West\",\"South\",\"Midwest\",\"Northeast\")))\n\nJoining with `by = join_by(state)`\n`summarise()` has grouped output by 'region', 'division'. You can override\nusing the `.groups` argument.\n\n\n\npopgrowth_df %>% head()\n\n# A tibble: 6 × 7\n  region    division           state          pop2000  pop2010 popgrowth   area\n  <fct>     <chr>              <fct>            <int>    <int>     <dbl>  <dbl>\n1 Midwest   East North Central Michigan       9938444  9883640  -0.00551 56539.\n2 Northeast New England        Rhode Island   1048319  1052567   0.00405  1034.\n3 South     West South Central Louisiana      4468976  4533372   0.0144  43204.\n4 Midwest   East North Central Ohio          11353140 11536504   0.0162  40861.\n5 Northeast Middle Atlantic    New York      18976457 19378102   0.0212  47126.\n6 South     South Atlantic     West Virginia  1808344  1852994   0.0247  24038.\n\n\n\nPlot with color\nSet Colorspace\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\")\nstate_colors <- region_colors[as.numeric(popgrowth_df$region)]\nstate_colors_dark = colorspace::darken(state_colors,0.4)\n\nggplot(popgrowth_df,aes(x = state, y = 100*popgrowth, fill = region))+\n  geom_col()+\n  scale_y_continuous(name = \"population growth, 2000 to 2010\",\n                     labels = scales::percent_format(scale = 1),\n                     expand = c(0,0))+\n  scale_fill_manual(values = region_colors)+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size=10,color = state_colors),\n        legend.position = c(.6,.5),\n        legend.background = element_rect(fill=\"#FFFFFFb0\"))\n\nWarning: Vectorized input to `element_text()` is not officially supported.\nℹ Results may be unexpected or may change in future versions of ggplot2.\n\n\n\n\n\n\n\nPlot lighten\nhighlightinh Texas, Louisiana, NA\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\") %>% \n  lighten(0.4) %>% desaturate(0.8)\n\npopgrowth_df_2 <- popgrowth_df %>% \n  mutate(region_highlight = ifelse(state %in% c(\"Texas\",\"Louisiana\"), NA, region %>% paste()))\npopgrowth_df_2 %>% head()\n\n# A tibble: 6 × 8\n  region    division           state     pop2000 pop2010 popgro…¹   area regio…²\n  <fct>     <chr>              <fct>       <int>   <int>    <dbl>  <dbl> <chr>  \n1 Midwest   East North Central Michigan   9.94e6  9.88e6 -0.00551 56539. Midwest\n2 Northeast New England        Rhode Is…  1.05e6  1.05e6  0.00405  1034. Northe…\n3 South     West South Central Louisiana  4.47e6  4.53e6  0.0144  43204. <NA>   \n4 Midwest   East North Central Ohio       1.14e7  1.15e7  0.0162  40861. Midwest\n5 Northeast Middle Atlantic    New York   1.90e7  1.94e7  0.0212  47126. Northe…\n6 South     South Atlantic     West Vir…  1.81e6  1.85e6  0.0247  24038. South  \n# … with abbreviated variable names ¹​popgrowth, ²​region_highlight\n\nggplot(popgrowth_df_2, aes(x = state, y= 100*popgrowth, fill = region_highlight))+\n  geom_col()+\n  scale_y_continuous(name = \"Population growth, 2000to 2010\",\n                     labels = scales::percent_format(scale=1),\n                     expand = c(0,0))+\n  scale_fill_manual(values = region_colors,\n                    breaks = c(\"West\",\"South\",\"Midwest\",\"Northeast\"),\n                    na.value = \"#56B4E9\")+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size=10),\n        legend.position = c(.58,.68),\n        legend.background = element_rect(fill=\"#FFFFFFb0\"))"
  },
  {
    "objectID": "posts/vis/vis.html#주민등록인구-및-세대-현황",
    "href": "posts/vis/vis.html#주민등록인구-및-세대-현황",
    "title": "ggplot, ggplot",
    "section": "2022 02 주민등록인구 및 세대 현황",
    "text": "2022 02 주민등록인구 및 세대 현황\nLoad & change type\n\nkor_202202 <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/202202_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9.csv\")\n\nkor_202202 %>% head()\n\n           행정구역 행정구역_코드 총인구수  세대수 세대당_인구 남자_인구수\n1       서울특별시     1100000000  9508451 4442586        2.14     4615823\n2 서울특별시 종로구    1111000000   144575   73763        1.96       70092\n3   서울특별시 중구    1114000000   122167   63644        1.92       59446\n4 서울특별시 용산구    1117000000   222413  111134        2.00      106881\n5 서울특별시 성동구    1120000000   285137  134286        2.12      138866\n6 서울특별시 광진구    1121500000   340494  168975        2.02      164226\n  여자_인구수 남여_비율\n1     4892628      0.94\n2       74483      0.94\n3       62721      0.95\n4      115532      0.93\n5      146271      0.95\n6      176268      0.93\n\nkor_202202 %>% sapply(class)\n\n     행정구역 행정구역_코드      총인구수        세대수   세대당_인구 \n  \"character\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\" \n  남자_인구수   여자_인구수     남여_비율 \n    \"numeric\"     \"numeric\"     \"numeric\" \n\nkor_202202$행정구역_코드 <- kor_202202$행정구역_코드 %>% format() #numeric to string\n\nfilter 서울, 대전 ,대구, 부산\n\nkor_202202_use <- kor_202202 %>% \n  filter(substr(행정구역,1,2) %in% c(\"서울\",\"대전\",\"대구\",\"부산\")) %>% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %>% \n  select(행정구역,총인구수) %>% \n  arrange(총인구수)\n\n\nkor_202202_use$시도 = sapply(kor_202202_use$행정구역,\n                           function(x) strsplit(x, \" \")[[1]][1])\nkor_202202_use$시도 = factor(kor_202202_use$시도,\n                           levels = c(\"서울특별시\",\"대전광역시\",\"대구광역시\",\"부산광역시\"))\n\n#kor_202202_use %>% View()\n\n\nkor_202202_use %>% head()\n\n           행정구역 총인구수       시도\n1   부산광역시 중구    40582 부산광역시\n2   대구광역시 중구    74710 대구광역시\n3   부산광역시 동구    88245 부산광역시\n4   부산광역시 서구   104618 부산광역시\n5 부산광역시 영도구   109991 부산광역시\n6   서울특별시 중구   122167 서울특별시\n\nkor_202202_use %>% summary()\n\n   행정구역            총인구수              시도   \n Length:54          Min.   : 40582   서울특별시:25  \n Class :character   1st Qu.:207148   대전광역시: 5  \n Mode  :character   Median :305946   대구광역시: 8  \n                    Mean   :309033   부산광역시:16  \n                    3rd Qu.:407067                  \n                    Max.   :661452                  \n\n\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\")\nregion_colors_2 <- RColorBrewer::brewer.pal(4,\"Set2\")\n\n\nPlot with colors, Korea\n\nggplot(kor_202202_use,aes(x = reorder(행정구역, 총인구수),y= 총인구수, fill = 시도))+\n  geom_col()+\n  scale_y_continuous(name = \"총인구수, 2022년 2월\",\n                     expand = c(0,0),\n                     labels = scales::comma)+\n  scale_fill_manual(values = region_colors_2)+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank())+\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),legend.position = c(.78,.28),legend.background = element_rect(fill = \"#FFFFFFB0\"))\n\n\n\n\n\n\nHighlight Daejeon (1)\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\") %>% \n  lighten(0.4) %>% desaturate(0.8)\nregion_colors[2] <- \"#56b4e9\" %>% darken(0.3)\n\nggplot(kor_202202_use,aes(x = reorder(행정구역, 총인구수),y= 총인구수, fill = 시도))+\n  geom_col()+\n  scale_y_continuous(name = \"총인구수, 2022년 2월\",\n                     expand = c(0,0),\n                     labels = scales::comma)+\n  scale_fill_manual(values = region_colors)+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank())+\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),legend.position = c(.78,.28),legend.background = element_rect(fill = \"#FFFFFFB0\"))\n\n\n\n\n\n\nHighlight Daejeon (2)\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\") %>% \n  lighten(0.4) %>% desaturate(0.8)\n\nkor_202202_use_2 <- kor_202202_use %>% mutate(region_highlight = ifelse(시도 %in% c(\"대전광역시\"),NA,시도 %>% paste()))\n\n\n\nggplot(kor_202202_use_2,aes(x = reorder(행정구역,총인구수),y= 총인구수, fill = region_highlight))+\n  geom_col()+\n  scale_y_continuous(name = \"총인구수, 2022년 2월\",\n                     expand = c(0,0),\n                     labels = scales::comma)+\n  scale_fill_manual(values = region_colors,\n                    breaks = c(\"서울특별시\",\"대전광역시\",\"대구광역시\",\"부산광역시\"),\n                    na.value = \"#56B4E9\")+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank())+\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),legend.position = c(.78,.28),legend.background = element_rect(fill = \"#FFFFFFB0\")\n        )"
  },
  {
    "objectID": "posts/vis/vis.html#map-korea",
    "href": "posts/vis/vis.html#map-korea",
    "title": "ggplot, ggplot",
    "section": "Map Korea",
    "text": "Map Korea\n\nlibrary packages, load .json\n\n#install.packages(\"geojsonsf\")\nlibrary(geojsonsf)\nlibrary(sf)\n\nkor_sido <- geojson_sf(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIDO.json\")\nkor_sigu <- geojson_sf(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json\")\n\nWarning in readLines(con):\n'https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json'에서\n불완전한 마지막 행이 발견되었습니다\n\n\n\n\nMerge data by 행정구역_코드\n\nuse_map <- kor_sigu\nuse_map$행정구역_코드 <- paste(use_map$SIG_CD,\"00000\",sep = \"\")\nuse_map <- use_map %>% merge(kor_202202,by = \"행정구역_코드\", all.x=T)\n\n\n\nPlot\n\nuse_map %>% ggplot(aes(fill=총인구수))+\n  geom_sf(color = \"grey90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"인구수\",\n    palette = \"Blues\", type = \"seq\", na.value = \"grey60\",\n    direction = 1,\n    breaks = seq(0,10,2) * 1e+5,\n    labels = format(seq(0,10,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2)\n  )\n\n\n\n\n\n\nPlot Daejeon\n\ndaejeon_map <-  use_map %>% filter(행정구역 %>% substr(1,5) == \"대전광역시\")\n\ndaejeon_map %>% ggplot(aes(fill=총인구수))+\n  geom_sf(color = \"grey90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"인구수\",\n    palette = \"Blues\", type = \"seq\", na.value = \"grey60\",\n    direction = 1,\n    breaks = seq(0,10,2) * 1e+5,\n    labels = format(seq(0,10,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2)\n  )\n\n\n\n\n\n\nPlot Gender_ratio\n\nuse_map %>% ggplot(aes(fill = 남여_비율))+\n  geom_sf()+\n  scale_fill_continuous_diverging(\n    name = \"남자/여자\",\n    palette = \"BLue-Red\",\n    mid=1,\n  limits = 1 + c(-1,+1)*0.35,\n  rev = T)+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2)\n  )"
  },
  {
    "objectID": "posts/vis/vis.html#년-3월-시군구-인구수",
    "href": "posts/vis/vis.html#년-3월-시군구-인구수",
    "title": "ggplot, ggplot",
    "section": "2023년 3월 시군구 인구수",
    "text": "2023년 3월 시군구 인구수\n시군구 지도 데이터, 행정구역 코드 10자리로 만들기\n\nuse_map <- kor_sigu\nuse_map$행정구역_코드 <- paste(use_map$SIG_CD,\"00000\",sep = \"\")\n\n\n데이터 로드\n2023년 3월 주민등록 인구통계 데이터, 행정안전부\n\ndata_pop <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/202303_202303_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9_%EC%9B%94%EA%B0%84.csv\",encoding = \"utf-8\")\ndata_pop %>% head() \n\n                        행정구역 X2023년03월_총인구수 X2023년03월_세대수\n1       서울특별시  (1100000000)            9,426,404          4,463,385\n2 서울특별시 종로구 (1111000000)              141,060             72,679\n3   서울특별시 중구 (1114000000)              120,963             63,862\n4 서울특별시 용산구 (1117000000)              217,756            109,735\n5 서울특별시 성동구 (1120000000)              280,240            133,513\n6 서울특별시 광진구 (1121500000)              336,801            169,787\n  X2023년03월_세대당.인구 X2023년03월_남자.인구수 X2023년03월_여자.인구수\n1                    2.11               4,566,299               4,860,105\n2                    1.94                  68,170                  72,890\n3                    1.89                  58,699                  62,264\n4                    1.98                 104,640                 113,116\n5                    2.10                 136,233                 144,007\n6                    1.98                 162,209                 174,592\n  X2023년03월_남여.비율\n1                  0.94\n2                  0.94\n3                  0.94\n4                  0.93\n5                  0.95\n6                  0.93\n\n\n\n\n전처리\n\n인구수0명 출장소 제외\n행정구역 코드 10자리 추출\n인구수 숫자형으로 변환\n시 단위 제외, 정렬\n\n\ndata_202303 <- data_pop %>% \n  filter(X2023년03월_총인구수>0) %>%  \n  select(행정구역,X2023년03월_총인구수) %>% \n  mutate(행정구역_코드 = str_sub(행정구역,-11,-2),\n         X2023년03월_총인구수 = gsub(\",\",\"\",X2023년03월_총인구수) %>% as.numeric()) %>% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %>% \n  arrange(desc(X2023년03월_총인구수))\n\ndata_202303 %>% head()\n\n                      행정구역 X2023년03월_총인구수 행정구역_코드\n1   경기도 수원시 (4111000000)              1192225    4111000000\n2   경기도 고양시 (4128000000)              1077909    4128000000\n3   경기도 용인시 (4146000000)              1073513    4146000000\n4 경상남도 창원시 (4812000000)              1017273    4812000000\n5   경기도 성남시 (4113000000)               923416    4113000000\n6   경기도 화성시 (4159000000)               922231    4159000000\n\n\n\n\n지도 데이터와 병합\n\nuse_map <- use_map %>% merge(data_202303,by = \"행정구역_코드\", all.x=T)\n\n\n\nPlot\n\nuse_map %>% ggplot(aes(fill=X2023년03월_총인구수))+\n  geom_sf(color = \"grey90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"2023년 3월 인구수\",\n    palette = \"Blues\", type = \"seq\", na.value = \"grey60\",\n    direction = 1,\n    breaks = seq(0,10,2) * 1e+5,\n    labels = format(seq(0,10,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1.0,\n    legend.position = c(0.85,0.2))+\n  labs(title = \"2023년 3월\")"
  },
  {
    "objectID": "posts/vis/vis.html#인구수-증가율",
    "href": "posts/vis/vis.html#인구수-증가율",
    "title": "ggplot, ggplot",
    "section": "인구수 증가율",
    "text": "인구수 증가율\n\nfile_2023 <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/202303_202303_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9_%EC%9B%94%EA%B0%84.csv\",encoding = \"utf-8\")\n\nfile_2013 <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/201303_201303_%EC%A3%BC%EB%AF%BC%EB%93%B1%EB%A1%9D%EC%9D%B8%EA%B5%AC%EB%B0%8F%EC%84%B8%EB%8C%80%ED%98%84%ED%99%A9_%EC%9B%94%EA%B0%84.csv\",encoding = \"UTF-8\")\n\n\n전처리\n\n인구수 0명 이상 필터\n행정구역(지역코드), 총인구수 열만 선택\n행정구역(지역코드)에서 지역코드와 행정구역 분리\n시 단위 제외, 인구수 기준 정렬\n\n\ndata_2023 <- file_2023 %>%\n  filter(X2023년03월_총인구수>0) %>%  \n  select(행정구역,X2023년03월_총인구수) %>% \n  mutate(행정구역_코드 = str_sub(행정구역,-11,-2),\n         X2023년03월_총인구수 = gsub(\",\",\"\",X2023년03월_총인구수) %>% as.numeric(),\n         행정구역 =  sapply(행정구역, function(x) strsplit(x, \"(\", fixed=T)[[1]][1]),\n         행정구역 = sapply(행정구역, function(x) gsub(\"( *)$\", \"\", x) %>% paste())) %>% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %>% \n  arrange(desc(X2023년03월_총인구수))\n\ndata_2013 <- file_2013 %>% \n  filter(X2013년03월_총인구수>0) %>%  \n  select(행정구역,X2013년03월_총인구수) %>% \n  mutate(행정구역_코드 = str_sub(행정구역,-11,-2),\n         X2013년03월_총인구수 = gsub(\",\",\"\",X2013년03월_총인구수) %>% as.numeric(),\n         행정구역 =  sapply(행정구역, function(x) strsplit(x, \"(\", fixed=T)[[1]][1]),\n         행정구역 = sapply(행정구역, function(x) gsub(\"( *)$\", \"\", x) %>% paste())) %>% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %>% \n  arrange(desc(X2013년03월_총인구수))\n\n\n\n병합\n\n지역코드 기준 병합\n인구성장률 열 추가\n중복 열 제거, 인구성장률 기준 정렬\n서울, 대전, 대구, 부산지역만 필터\n시도 추출, factor 변환\n\n\nkor_census <- data_2013 %>% \n  merge(data_2023,by = \"행정구역_코드\", all.x=T) %>%  \n  mutate(성장률 = (X2023년03월_총인구수 - X2013년03월_총인구수) / X2013년03월_총인구수) %>% \n  select(행정구역.x,X2013년03월_총인구수,X2023년03월_총인구수, 성장률, 행정구역_코드) %>% \n  filter(substr(행정구역.x,1,2) %in% c(\"서울\",\"대전\",\"대구\",\"부산\")) %>%\n  arrange(desc(성장률))\n\nnames(kor_census) <- c(\"행정구역\", \"X2013인구수\",\"X2023인구수\",\"성장률\",\"행정구역_코드\")\n\nkor_census$시도 = sapply(kor_census$행정구역,\n                           function(x) strsplit(x, \" \")[[1]][1])\nkor_census$시도 = factor(kor_census$시도,\n                           levels = c(\"서울특별시\",\"대전광역시\",\"대구광역시\",\"부산광역시\"))\n\n\n\nPlot\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\")\n\n\nggplot(kor_census,aes(x = reorder(행정구역,성장률),y= 성장률, fill = 시도))+\n  geom_col()+\n  scale_y_continuous(name = \"인구성장률\",\n                     expand = c(0,0),\n                     labels = scales::percent_format(scale = 100))+\n  scale_fill_manual(values = region_colors)+\n  coord_flip()+\n  theme_light()+\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank())+\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),legend.position = c(.78,.28),legend.background = element_rect(fill = \"#FFFFFFB0\"))\n\n\n\n\n\n\nMap plot\n\nkor_sigu <- geojson_sf(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json\")\n\nWarning in readLines(con):\n'https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json'에서\n불완전한 마지막 행이 발견되었습니다\n\nkor_map <- kor_sigu\nkor_map$행정구역_코드 <- paste(kor_map$SIG_CD,\"00000\",sep=\"\")\n\n\nkor_census_2 <- data_2013 %>% \n  merge(data_2023,by = \"행정구역_코드\", all.x=T) %>%  \n  mutate(성장률 = (X2023년03월_총인구수 - X2013년03월_총인구수) / X2013년03월_총인구수) %>% \n  select(행정구역.x,X2013년03월_총인구수,X2023년03월_총인구수, 성장률, 행정구역_코드) %>% \n  arrange(desc(성장률))\n\nnames(kor_census_2) <- c(\"행정구역\", \"X2013인구수\",\"X2023인구수\",\"성장률\",\"행정구역_코드\")\n\n\nkor_map <- kor_map %>% left_join(kor_census_2, by=\"행정구역_코드\")\n\n\nkor_map %>% ggplot(aes(fill=성장률))+\n  geom_sf()+\n  scale_fill_continuous_diverging(\n    name = \"인구성장률\",\n    palette = \"BLue-Red\",\n    limits = c(-0.4,2.4))+\n  theme_minimal()+\n  theme(legend.title.align = 0.5,\n        legend.text.align = 1.0,\n        legend.position = c(0.85,0.2))"
  },
  {
    "objectID": "posts/vis/vis.html#인구증감",
    "href": "posts/vis/vis.html#인구증감",
    "title": "ggplot, ggplot",
    "section": "인구증감",
    "text": "인구증감\n\nkor_sigu <- geojson_sf(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json\")\n\nWarning in readLines(con):\n'https://raw.githubusercontent.com/Sungileo/trainsets/main/kor/KOR_SIGU.json'에서\n불완전한 마지막 행이 발견되었습니다\n\ndata <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/kor_census_2013_2023.csv\",encoding = \"utf-8\")\n\ndata <- data %>% mutate(인구증감 = 총인구수_2023-총인구수_2013)\n\nuse_map <- kor_sigu\nuse_map$행정구역_코드 <- paste(use_map$SIG_CD,\"00000\",sep = \"\") %>% as.numeric()\nuse_map <- use_map %>% merge(data,by = \"행정구역_코드\")\n\nuse_map %>% ggplot(aes(fill = 인구증감))+\n  geom_sf()+\n  coord_sf(datum = NA)+\n  scale_fill_continuous_diverging(\n    name = \"인구증감\",\n    palette = \"BLue-Red\",\n    na.value = \"grey40\",\n    mid=0,\n    rev = T,\n    limits = c(-4,4)*100000,\n    labels = format(seq(-4,4,2) * 1e+5, big.mark = \",\",scientific = FALSE))+\n  theme_minimal()+\n  theme(legend.position = c(0.85,0.2))"
  },
  {
    "objectID": "posts/vis/vis.html#random-p-value",
    "href": "posts/vis/vis.html#random-p-value",
    "title": "ggplot, ggplot",
    "section": "Random p-value",
    "text": "Random p-value\n\nset.seed(100)\ndf <- data.frame(p = runif(1000)) %>% \n  mutate(idx = 1:n(),\n         p_log10 = -log10(p))\n\ndf <- df %>% \n  arrange(p) %>%\n  mutate(idx_2 = 1:n(),\n         label = ifelse(idx_2<=3,p,\"\"))\n\n\nggplot(df, aes(x=idx, y=p_log10)) +\n  geom_point(color = \"darkblue\") +\n  geom_hline(yintercept = -log10(0.05),\n             linetype = 2,\n             linewidth = 0.8,\n             color = \"darkred\") +\n  scale_y_continuous(name = expression(-log[10](italic(p)))) +\n  scale_x_continuous(name = \"\",\n                     breaks = NULL) +\n  theme_minimal()+\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100)"
  },
  {
    "objectID": "posts/vis/vis.html#visualizing-amounts-수량-데이터의-시각화",
    "href": "posts/vis/vis.html#visualizing-amounts-수량-데이터의-시각화",
    "title": "ggplot, ggplot",
    "section": "Visualizing amounts: 수량 데이터의 시각화",
    "text": "Visualizing amounts: 수량 데이터의 시각화\n\nboxoffice <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/boxoffice.csv\")\n\nboxoffice %>% ggplot(aes(x=fct_reorder(title,rank),y=amount))+\n  geom_col(fill=\"#56b4e9\",width = 0.6,alpha = 0.9)+\n  scale_y_continuous(expand = c(0,0),\n                     breaks = c(0,2e7,4e7,6e7),\n                     labels = c(0,20,40,60),\n                     name = \"Weekend gross(million USD)\")+\n  xlab(\"\")+\n  theme_minimal()+\n  theme(\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.x = element_text(angle=45,hjust = 1)\n  )+\n  coord_flip()\n\n\n\n\n\nkobis <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/KOBIS_%EC%8B%A4%EC%8B%9C%EA%B0%84_%EC%98%88%EB%A7%A4%EC%9C%A8_2023-04-26.csv\")\n\nkobis %>% filter(누적매출액>0) %>% head(5) %>% \n  ggplot(aes(x=reorder(영화명,-예매매출액),y=예매매출액))+\n  geom_col(fill=\"#56b4e9\",width = 0.6,alpha = 0.9)+\n  scale_y_continuous(expand = c(0,0),\n                     breaks = c(0,5e8,1e9,1.5e9),\n                     labels = c(0,5,10,15),\n                     name = \"에매매출액(억원)\")+\n  xlab(\"\")+\n  theme_minimal()+\n  theme(\n    axis.ticks.x = element_blank(),\n    panel.grid.major.x = element_blank())\n\n\n\n\n\nincome_by_age <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets/main/income_by_age.csv\")\n\nincome_by_age <- income_by_age %>% \n  mutate(age = age %>% factor(levels = c(\"15 to 24\",\n                                         \"25 to 34\",\n                                         \"35 to 44\",\n                                         \"45 to 54\",\n                                         \"55 to 64\",\n                                         \"65 to 74\",\n                                         \"> 74\")))\nincome_df <- income_by_age %>% \n  filter(race %in% c(\"white\",\"asian\",\"black\",\"hispanic\")) %>% \n  mutate(race = fct_relevel(race,c(\"asian\",\"white\",\"hispanic\",\"black\")),\n         race = fct_recode(race,\"Asian\"=\"asian\",\"Hispanic\"=\"hispanic\"),\n         age = fct_recode(age,\">= 75\" = \"> 74\"))\n\ncolors_four <- RColorBrewer::brewer.pal(5,\"PuBu\")[5:2]\n\nincome_df %>% ggplot(aes(x=age,y=median_income,fill=race))+\n  geom_col(position = \"dodge\",alpha = 0.9)+\n  scale_fill_manual(values = colors_four,name=NULL)+\n  xlab(\"age (years)\")+\n  scale_y_continuous(expand = c(0,0),\n                     name = \"Median income (USD)\",\n                     breaks = c(seq(0,100,20)*1000),\n                     labels = c(seq(0,100,20)*1000) %>% paste())+\n  theme_minimal()\n\n\n\n\n\ncolors_seven <- RColorBrewer::brewer.pal(8,\"PuBu\")[2:8]\n\nincome_df %>% ggplot(aes(x=race,y=median_income,fill=age))+\n  geom_col(position = \"dodge\",alpha=0.9)+\n  scale_fill_manual(values = colors_seven,name=NULL)+\n  xlab(NULL)+\n  scale_y_continuous(expand = c(0,0),\n                     name = \"Median income (USD)\",\n                     breaks = c(seq(0,100,20)*1000),\n                     labels = c(seq(0,100,20)*1000) %>% paste())+\n  theme_minimal()\n\n\n\n\n\nincome_df %>% ggplot(aes(x=age,y=median_income))+\n  geom_col(fill = \"#56b4e9\",alpha = 0.9)+\n  xlab(\"age (years)\")+\n  scale_y_continuous(expand = c(0,0),\n                     name = \"Median income (USD)\",\n                     breaks = c(seq(0,100,20)*1000),\n                     labels = c(seq(0,100,20)*1000) %>% paste())+\n  theme_minimal()+\n  theme(panel.grid.major.x = element_blank(),\n        axis.ticks.x = element_blank())+\n  facet_wrap(~race, scales =\"free_x\")"
  },
  {
    "objectID": "posts/vis/vis.html#dot-plot-heatmap",
    "href": "posts/vis/vis.html#dot-plot-heatmap",
    "title": "ggplot, ggplot",
    "section": "Dot plot & Heatmap",
    "text": "Dot plot & Heatmap\nimportant limitation of bars is that they need to start at zero, so that the bar length is proportional to the amount shown.\n막대는 금액을 시각화하는 유일한 옵션이 아닙니다. 막대의 한 가지 중요한 제한 사항은 막대 길이가 표시된 양에 비례하도록 0에서 시작해야 한다는 것입니다. 일부 데이터 세트의 경우 이는 비실용적이거나 주요 기능을 모호하게 할 수 있습니다. 이 경우 x 축 또는 y 축을 따라 적절한 위치에 점을 배치하여 금액을 표시할 수 있습니다.\n\nDot plot(gapminder)\n\nlibrary(gapminder)\n\ndf_america <- gapminder %>% filter(year == 2007,continent == \"Americas\")\n\ndf_america %>% ggplot(aes(x=lifeExp,y=reorder(country,lifeExp)))+\n  geom_point(color = \"#0073b2\",size = 3)+\n  scale_x_continuous(\n    name = \"Life expectancy(years)\",\n    limits = c(59,82),\n    expand = c(0,0))+\n  scale_y_discrete(name = NULL,expand = c(0,0.5))+\n  theme_minimal()+\n  theme(plot.margin = margin(18,6,3,1.5))\n\n\n\n\n\n\nDot plot(SW기술자 평균연봉)\n\ndata_sw_raw <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/SW%EA%B8%B0%EC%88%A0%EC%9E%90_%ED%8F%89%EA%B7%A0%EC%9E%84%EA%B8%88_20220412145301.csv\")\n\ndata_sw_21 <- data_sw_raw %>% \n  select(직무별,X2021) %>% \n  drop_na()\n\ndata_sw_21 %>% ggplot(aes(x=X2021,y=reorder(직무별,X2021)))+\n  geom_point(color = \"#0073b2\",size = 3)+\n  scale_x_continuous(\n    name = \"평균임금\",\n    limits = c(14.6e4,65e4),\n    expand = c(0,0),\n    labels = format(seq(2,6,1)*1e5,big.mark = \",\",scientific = FALSE))+\n  scale_y_discrete(name = NULL,expand = c(0,0.5))+\n  theme_minimal()+\n  theme(plot.margin = margin(18,6,3,1.5))\n\n\n\n\n\n\nTile plot(internet prevalce)\n\ninternet <- read.csv('https://raw.githubusercontent.com/Sungileo/trainsets_2/main/internet.csv')\n\ncountry_list <- c(\"United States\",\"China\",\"India\",\"Japan\",\"Algeria\",\"Brazil\",\"Germany\",\"France\",\"United Kingdom\",\"Italy\",\"New Zealand\",\"Canada\",\"Mexico\",\"Chile\",\"Argentina\",\"Norway\",\"South Africa\",\"Kenya\",\"Israel\",\"Iceland\")\n\ninternet_short <- internet %>% \n  filter(country %in% country_list) %>% \n  mutate(users = ifelse(is.na(users),0,users))\n  \ninternet_summary <- internet_short %>% \n  filter(year == 2016) %>% \n  arrange(users)\n\ninternet_short <- internet_short %>% \n  filter(year > 1993) %>% \n  mutate(country = factor(country,levels = internet_summary$country))\n\ninternet_short %>% ggplot(aes(x=year,y=country,fill=users))+\n  geom_tile(color = \"white\",linewidth = 0.25)+\n  scale_fill_viridis_c(\n    option = \"A\", begin = 0.05, end = 0.98,\n    limits = c(0,100), name = \"Internet users / 100 people\",\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      label.position = \"bottom\",\n      title.position = \"top\",\n      ticks = FALSE,\n      barwidth = grid::unit(3.5,\"in\"),\n      barheight = grid::unit(0.2,\"in\")))+\n  scale_x_continuous(expand = c(0,0),name = NULL)+\n  scale_y_discrete(name = NULL,position = \"right\")+\n  theme_minimal()+\n  theme(\n    axis.line = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.title.align = 0.5,\n    legend.title = element_text(size = 10))\n\n\n\n\n\n\nTile plot(연령집단별 자살률)\n\ndata_rate_raw <- read.csv(\"https://raw.githubusercontent.com/Sungileo/trainsets_2/main/%EC%97%B0%EB%A0%B9%EC%A7%91%EB%8B%A8%EB%B3%84_%EC%9E%90%EC%82%B4%EB%A5%A0.csv\")\n\ndata_rate_melt <- data_rate_raw %>% \n  reshape2::melt(id.vars = c(\"연령집단\"),\n                 variable.name = \"년도\",\n                 value.name = \"자살률\") %>% \n  mutate(년도 = gsub(\"\\\\D\",\"\",년도) %>% as.integer())\n\ndata_rate_melt %>% ggplot(aes(x=년도,y=연령집단,fill=자살률))+\n  geom_tile(color = \"white\",linewidth = 0.25)+\n  scale_fill_viridis_c(\n    option = \"A\", begin = 0.05, end = 0.98,\n    limits = c(0,130), name = \"연령집단(인구 십만 명당)\",\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      label.position = \"bottom\",\n      title.position = \"top\",\n      ticks = FALSE,\n      barwidth = grid::unit(3.5,\"in\"),\n      barheight = grid::unit(0.2,\"in\")))+\n  scale_x_continuous(expand = c(0,0),name = NULL)+\n  scale_y_discrete(name = NULL,position = \"right\")+\n  theme_minimal()+\n  theme(\n    axis.line = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.title.align = 0.5,\n    legend.title = element_text(size = 10))"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.zeros",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.zeros",
    "title": "week_1a_numpy",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.zeros((3,4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#용어",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#용어",
    "title": "week_1a_numpy",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)와 같습니다\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#n-차원-배열",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#n-차원-배열",
    "title": "week_1a_numpy",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\n\nnp.zeros((2,3,4))\n\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#배열-타입",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#배열-타입",
    "title": "week_1a_numpy",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\n\ntype(np.zeros((3,4)))\n\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.ones",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.ones",
    "title": "week_1a_numpy",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.ones((3,4))\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.full",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.full",
    "title": "week_1a_numpy",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\nnp.full((3,4), np.pi)\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.empty",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.empty",
    "title": "week_1a_numpy",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\n\nnp.empty((2,3))\n\narray([[1.01243582e-311, 1.01243582e-311, 1.01243965e-311],\n       [1.22528280e-321, 0.00000000e+000, 0.00000000e+000]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.array",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.array",
    "title": "week_1a_numpy",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.arange",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.arange",
    "title": "week_1a_numpy",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\n\nnp.arange(1, 5)\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\n\nnp.arange(1, 5, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\n\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n\nprint(np.arange(0, 5/3, 0.333333333))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333334]\n\n\nfor loops를 사용하지 않고 전체 array에 대한 연산 수행이 가능합니다.\n평균적으로 Numpy-based 알고리즘은 10~100배정도 속도가 더 빠르고 적은 메모리를 사용합니다.\n\nmy_arr = np.arange(1000000)\nmy_list = list(range(1000000))\n\n#%time for _ in range(10): my_arr2 = my_arr * 2\n#%time for _ in range(10): my_list2 = [x * 2 for x in my_list]\n\nFor loop를 돌릴 때의 속도 비교\n\nsize = 10\nfor x in range(size): x ** 2\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n\n\n\nimport sys\n\nsize = 10\n\n#%timeit for x in range(size): x ** 2\n# out: 10 loops, best of 3: 136 ms per loop\n\n# avoid this\n#%timeit for x in np.arange(size): x ** 2\n#out: 1 loops, best of 3: 1.16 s per loop\n\n# use this\n#%timeit np.arange(size) ** 2\n#out: 100 loops, best of 3: 19.5 ms per loop"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.linspace",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.linspace",
    "title": "week_1a_numpy",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\n\nprint(np.linspace(0, 5/3, 6))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.rand와-np.randn",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.rand와-np.randn",
    "title": "week_1a_numpy",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\n\nnp.random.rand(3,4)\n\narray([[0.11692257, 0.08969128, 0.33133623, 0.54561177],\n       [0.36285982, 0.70040254, 0.31700339, 0.05228367],\n       [0.37903464, 0.62776877, 0.81708241, 0.01826426]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\n\nnp.random.randn(3,4)\n\narray([[-0.51137816, -1.02023304, -0.08412165,  0.30591357],\n       [-0.54789866, -0.50029364, -1.57251316,  2.01096654],\n       [-0.1347091 , -0.30671405,  0.29583043,  0.66797862]])\n\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n\n#%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\n\n(array([0.96101146, 0.98801178, 0.96401149, 0.96301148, 0.98001168,\n       1.02501222, 1.05501258, 0.97901167, 1.0230122 , 0.9900118 ,\n       0.99201183, 1.02901227, 1.02801226, 1.00301196, 0.98701177,\n       1.04701248, 1.07301279, 1.02701224, 0.99501186, 1.04301243,\n       0.95001133, 1.07901286, 0.97901167, 1.01901215, 0.97701165,\n       0.9980119 , 1.01001204, 0.98401173, 1.01201206, 1.01901215,\n       0.98301172, 0.94101122, 0.94301124, 0.99101181, 1.01001204,\n       0.9480113 , 0.98201171, 1.05501258, 1.02201218, 0.9730116 ,\n       1.03101229, 0.9650115 , 0.98901179, 0.97501162, 0.99701189,\n       1.03801237, 1.02501222, 1.04201242, 1.00101193, 1.01301208,\n       0.95101134, 1.00601199, 0.99701189, 1.02201218, 1.03801237,\n       1.03901239, 0.97101158, 1.02801226, 0.96401149, 0.97101158,\n       1.01401209, 0.99701189, 0.96801154, 0.97601164, 0.96701153,\n       0.97401161, 1.03101229, 0.93201111, 1.00101193, 1.03801237,\n       0.96201147, 1.03901239, 0.99501186, 1.01601211, 0.9900118 ,\n       1.00401197, 1.00101193, 0.99101181, 0.97901167, 0.99301184,\n       1.04701248, 0.9310111 , 1.00701201, 0.97001156, 0.98401173,\n       1.00201195, 0.99401185, 1.03301232, 1.0150121 , 0.98401173,\n       1.00201195, 1.03701236, 1.00701201, 1.04601247, 1.0400124 ,\n       0.99101181, 0.92601104, 0.95701141, 1.0320123 , 1.03701236]), array([6.24378325e-06, 1.00061246e-02, 2.00060054e-02, 3.00058861e-02,\n       4.00057669e-02, 5.00056477e-02, 6.00055285e-02, 7.00054093e-02,\n       8.00052901e-02, 9.00051708e-02, 1.00005052e-01, 1.10004932e-01,\n       1.20004813e-01, 1.30004694e-01, 1.40004575e-01, 1.50004456e-01,\n       1.60004336e-01, 1.70004217e-01, 1.80004098e-01, 1.90003979e-01,\n       2.00003859e-01, 2.10003740e-01, 2.20003621e-01, 2.30003502e-01,\n       2.40003383e-01, 2.50003263e-01, 2.60003144e-01, 2.70003025e-01,\n       2.80002906e-01, 2.90002787e-01, 3.00002667e-01, 3.10002548e-01,\n       3.20002429e-01, 3.30002310e-01, 3.40002190e-01, 3.50002071e-01,\n       3.60001952e-01, 3.70001833e-01, 3.80001714e-01, 3.90001594e-01,\n       4.00001475e-01, 4.10001356e-01, 4.20001237e-01, 4.30001118e-01,\n       4.40000998e-01, 4.50000879e-01, 4.60000760e-01, 4.70000641e-01,\n       4.80000521e-01, 4.90000402e-01, 5.00000283e-01, 5.10000164e-01,\n       5.20000045e-01, 5.29999925e-01, 5.39999806e-01, 5.49999687e-01,\n       5.59999568e-01, 5.69999449e-01, 5.79999329e-01, 5.89999210e-01,\n       5.99999091e-01, 6.09998972e-01, 6.19998852e-01, 6.29998733e-01,\n       6.39998614e-01, 6.49998495e-01, 6.59998376e-01, 6.69998256e-01,\n       6.79998137e-01, 6.89998018e-01, 6.99997899e-01, 7.09997780e-01,\n       7.19997660e-01, 7.29997541e-01, 7.39997422e-01, 7.49997303e-01,\n       7.59997183e-01, 7.69997064e-01, 7.79996945e-01, 7.89996826e-01,\n       7.99996707e-01, 8.09996587e-01, 8.19996468e-01, 8.29996349e-01,\n       8.39996230e-01, 8.49996110e-01, 8.59995991e-01, 8.69995872e-01,\n       8.79995753e-01, 8.89995634e-01, 8.99995514e-01, 9.09995395e-01,\n       9.19995276e-01, 9.29995157e-01, 9.39995038e-01, 9.49994918e-01,\n       9.59994799e-01, 9.69994680e-01, 9.79994561e-01, 9.89994441e-01,\n       9.99994322e-01]), [<matplotlib.patches.Polygon object at 0x000001DD386DD4D0>])\n\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\n\n(array([1.09525908e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       2.19051816e-04, 3.28577725e-04, 2.19051816e-04, 7.66681358e-04,\n       2.19051816e-04, 7.66681358e-04, 1.75241453e-03, 1.97146635e-03,\n       1.86194044e-03, 2.95719952e-03, 2.51909589e-03, 4.05245860e-03,\n       5.36676950e-03, 8.98112447e-03, 1.17192722e-02, 1.41288422e-02,\n       1.41288422e-02, 2.12480262e-02, 2.46433293e-02, 3.38435056e-02,\n       4.12912674e-02, 4.65485110e-02, 5.76106277e-02, 6.82346408e-02,\n       8.01729648e-02, 9.45208588e-02, 1.12483108e-01, 1.30773934e-01,\n       1.53336272e-01, 1.62536448e-01, 1.89698873e-01, 2.12261210e-01,\n       2.33180659e-01, 2.57057307e-01, 2.83781628e-01, 2.90900812e-01,\n       3.21896644e-01, 3.30768243e-01, 3.51906743e-01, 3.64721274e-01,\n       3.74469080e-01, 3.79069168e-01, 3.92869433e-01, 3.94840899e-01,\n       3.93307536e-01, 3.94293270e-01, 3.89036026e-01, 3.78631065e-01,\n       3.57492564e-01, 3.51906743e-01, 3.31973028e-01, 3.04153447e-01,\n       2.93419908e-01, 2.77319600e-01, 2.52895322e-01, 2.29018674e-01,\n       2.06675389e-01, 1.84770207e-01, 1.68560373e-01, 1.51145753e-01,\n       1.40302688e-01, 1.16316515e-01, 9.70399547e-02, 7.76538689e-02,\n       6.80155890e-02, 5.60772650e-02, 5.04914437e-02, 4.35913115e-02,\n       3.39530315e-02, 2.82576843e-02, 2.30004407e-02, 1.62098344e-02,\n       1.35812126e-02, 1.07335390e-02, 9.85733174e-03, 6.24297677e-03,\n       5.69534723e-03, 3.50482906e-03, 3.17625134e-03, 2.84767361e-03,\n       1.20478499e-03, 9.85733174e-04, 8.76207266e-04, 5.47629541e-04,\n       3.28577725e-04, 2.19051816e-04, 1.09525908e-04, 1.09525908e-04,\n       1.09525908e-04, 1.09525908e-04, 0.00000000e+00, 1.09525908e-04]), array([-4.74915503e+00, -4.65785243e+00, -4.56654983e+00, -4.47524723e+00,\n       -4.38394464e+00, -4.29264204e+00, -4.20133944e+00, -4.11003684e+00,\n       -4.01873424e+00, -3.92743164e+00, -3.83612905e+00, -3.74482645e+00,\n       -3.65352385e+00, -3.56222125e+00, -3.47091865e+00, -3.37961606e+00,\n       -3.28831346e+00, -3.19701086e+00, -3.10570826e+00, -3.01440566e+00,\n       -2.92310306e+00, -2.83180047e+00, -2.74049787e+00, -2.64919527e+00,\n       -2.55789267e+00, -2.46659007e+00, -2.37528747e+00, -2.28398488e+00,\n       -2.19268228e+00, -2.10137968e+00, -2.01007708e+00, -1.91877448e+00,\n       -1.82747188e+00, -1.73616929e+00, -1.64486669e+00, -1.55356409e+00,\n       -1.46226149e+00, -1.37095889e+00, -1.27965629e+00, -1.18835370e+00,\n       -1.09705110e+00, -1.00574850e+00, -9.14445901e-01, -8.23143303e-01,\n       -7.31840705e-01, -6.40538107e-01, -5.49235508e-01, -4.57932910e-01,\n       -3.66630312e-01, -2.75327713e-01, -1.84025115e-01, -9.27225169e-02,\n       -1.41991856e-03,  8.98826797e-02,  1.81185278e-01,  2.72487876e-01,\n        3.63790475e-01,  4.55093073e-01,  5.46395671e-01,  6.37698269e-01,\n        7.29000868e-01,  8.20303466e-01,  9.11606064e-01,  1.00290866e+00,\n        1.09421126e+00,  1.18551386e+00,  1.27681646e+00,  1.36811906e+00,\n        1.45942165e+00,  1.55072425e+00,  1.64202685e+00,  1.73332945e+00,\n        1.82463205e+00,  1.91593465e+00,  2.00723724e+00,  2.09853984e+00,\n        2.18984244e+00,  2.28114504e+00,  2.37244764e+00,  2.46375024e+00,\n        2.55505283e+00,  2.64635543e+00,  2.73765803e+00,  2.82896063e+00,\n        2.92026323e+00,  3.01156582e+00,  3.10286842e+00,  3.19417102e+00,\n        3.28547362e+00,  3.37677622e+00,  3.46807882e+00,  3.55938141e+00,\n        3.65068401e+00,  3.74198661e+00,  3.83328921e+00,  3.92459181e+00,\n        4.01589441e+00,  4.10719700e+00,  4.19849960e+00,  4.28980220e+00,\n        4.38110480e+00]), [<matplotlib.patches.Polygon object at 0x000001DD3870CAD0>])\n\nplt.axis([-2.5, 2.5, 0, 1.1])\n\n(-2.5, 2.5, 0.0, 1.1)\n\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.fromfunction",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.fromfunction",
    "title": "week_1a_numpy",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#dtype",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#dtype",
    "title": "week_1a_numpy",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint32 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#itemsize",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#itemsize",
    "title": "week_1a_numpy",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#data-버퍼",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#data-버퍼",
    "title": "week_1a_numpy",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n<memory at 0x000001DD3873EDC0>\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#자신을-변경",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#자신을-변경",
    "title": "week_1a_numpy",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\ng = np.arange(24)\nprint(g)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n\nprint(\"랭크:\", g.ndim)\n\n랭크: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n\nprint(\"랭크:\", g.ndim)\n\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n\nprint(\"랭크:\", g.ndim)\n\n랭크: 3\n\n\n\ng[1,1,1]\n\n17"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#reshape",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#reshape",
    "title": "week_1a_numpy",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\ng2 = g.reshape(4,6)\nprint(g2)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n\nprint(\"랭크:\", g2.ndim)\n\n랭크: 2\n\n\n\ng[0,0,0] = 10\ng2\n\narray([[10,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\n\ng2[1, 2] = 999\ng2\n\narray([[ 10,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\ng\n\narray([[[ 10,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])\n\n\n완전히 다른 공간에 값만 같게 복사를 하고 싶다면 copy를 사용.\n이렇게 할 경우 두 객체는 독립적인 객체로 존재함\n\ng2 = g.copy()"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#ravel",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#ravel",
    "title": "week_1a_numpy",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\n\ng.ravel()\n\narray([ 10,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#규칙-1",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#규칙-1",
    "title": "week_1a_numpy",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\nh = np.arange(5).reshape(1, 1, 5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#규칙-2",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#규칙-2",
    "title": "week_1a_numpy",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\n[[100], [200]]\n\n[[100], [200]]\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n(2,3) 크기의 ndarray에 (3,) 크기의 ndarray 더하기\n\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n\ntest = np.array([100, 200, 300])\ntest.shape\n\n(3,)\n\ntest\n\narray([100, 200, 300])\n\n\n\n# step 1\ntest = test.reshape(1,3)\ntest\n\narray([[100, 200, 300]])\n\n\n\n# step 2\nnp.vstack((test,test))\n\narray([[100, 200, 300],\n       [100, 200, 300]])\n\n\n\n# step 2\nnp.concatenate((test,test),axis=0)\n\narray([[100, 200, 300],\n       [100, 200, 300]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#규칙-3",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#규칙-3",
    "title": "week_1a_numpy",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요.\n\na = np.array([[0.0],[10.0],[20.0],[30.0]])\n\n\na\n\narray([[ 0.],\n       [10.],\n       [20.],\n       [30.]])\n\n\n\na = np.array([0.0, 10.0, 20.0, 30.0])\nb = np.array([1.0, 2.0, 3.0])\na[:, np.newaxis] + b\n\narray([[ 1.,  2.,  3.],\n       [11., 12., 13.],\n       [21., 22., 23.],\n       [31., 32., 33.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#업캐스팅",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#업캐스팅",
    "title": "week_1a_numpy",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#ndarray-메서드",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#ndarray-메서드",
    "title": "week_1a_numpy",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\nprint(\"평균 =\", a.mean())\n\n평균 = 6.766666666666667\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\n\nc=np.arange(24).reshape(2,3,4)\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n\nc.sum(axis=2) \n\narray([[ 6, 22, 38],\n       [54, 70, 86]])\n\n\n여러 축에 대해서 더할 수도 있습니다:\n\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#일반-함수",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#일반-함수",
    "title": "week_1a_numpy",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다:\n\nprint(\"원본 ndarray\")\n\n원본 ndarray\n\nprint(a)\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n<string>:3: RuntimeWarning: invalid value encountered in sqrt\n<string>:3: RuntimeWarning: invalid value encountered in log"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#이항-일반-함수",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#이항-일반-함수",
    "title": "week_1a_numpy",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a > b 와 동일\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#차원-배열",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#차원-배열",
    "title": "week_1a_numpy",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소를 수정할 수 있죠:\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])\n\n\nQuiz. 아래의 array를 사용해서 다음 퀴즈를 풀어봅시다.\n\nnp.arange(5,50,5).reshape(3,3)\n\narray([[ 5, 10, 15],\n       [20, 25, 30],\n       [35, 40, 45]])\n\n\n\nimport numpy as np\n\narray_2d = np.array([[5, 10, 15],\n                     [20, 25, 30],\n                     [35, 40, 45]])\n\n\n2차원 배열 ’array_2d’에서 첫 번째 행(row)의 모든 요소를 선택해 보세요.\n\n힌트: 인덱싱을 사용하여 첫 번째 행을 선택할 수 있습니다.\n\n2차원 배열 ’array_2d’에서 두 번째 열(column)의 모든 요소를 선택해 보세요.\n\n힌트: 인덱싱과 슬라이싱을 사용하여 두 번째 열을 선택할 수 있습니다.\n\n2차원 배열 ’array_2d’에서 다음 요소들을 선택해 보세요: 25, 30, 40, 45\n\n힌트: 팬시 인덱싱(fancy indexing)을 사용하여 여러 요소를 한 번에 선택할 수 있습니다."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#보통의-파이썬-배열과-차이점",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#보통의-파이썬-배열과-차이점",
    "title": "week_1a_numpy",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\na\n\narray([  1,   5, 997, 998, 999,   7,   3])\n\n\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\nList는 브로드캐스팅으로 할당이 안됨\n\nb = [1, 5, 3, 19, 13, 7, 3]\n#b[2:5] = -1\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\ncould not broadcast input array from shape (6,) into shape (3,)\n\n\n원소를 삭제할 수도 없습니다:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\nList에서는 삭제가 가능\n\nb = [1, 5, 3, 19, 13, 7, 3] \ndel b[2:5]\nb\n\n[1, 5, 7, 3]\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#다차원-배열",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#다차원-배열",
    "title": "week_1a_numpy",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[1, 2]  # 행 1, 열 2\n\n14\n\n\n\nb[1, :]  # 행 1, 모든 열\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # 모든 행, 열 1\n\narray([ 1, 13, 25, 37])\n\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\n\nb[1, :]\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n\nb[1, :].shape\n\n(12,)\n\n\n\nb[1:2, :].shape\n\n(1, 12)\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#팬시-인덱싱fancy-indexing",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#팬시-인덱싱fancy-indexing",
    "title": "week_1a_numpy",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[(2,3),0:2]\n\narray([[24, 25],\n       [36, 37]])\n\n\n\nb[2:,0:2]\n\narray([[24, 25],\n       [36, 37]])\n\n\n\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#고차원",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#고차원",
    "title": "week_1a_numpy",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n34\n\n\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#생략-부호-...",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#생략-부호-...",
    "title": "week_1a_numpy",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\narray([27, 33])\n\n\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#불리언-인덱싱",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#불리언-인덱싱",
    "title": "week_1a_numpy",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#np.ix_",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#np.ix_",
    "title": "week_1a_numpy",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\n\nb[np.ix_((0,2),(1,4,7,10))]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n       [2]], dtype=int64), array([[ 1,  4,  7, 10]], dtype=int64))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\n\nb.shape\n\n(4, 12)\n\n\n\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb % 3 == 1\n\narray([[False,  True, False, False,  True, False, False,  True, False,\n        False,  True, False],\n       [False,  True, False, False,  True, False, False,  True, False,\n        False,  True, False],\n       [False,  True, False, False,  True, False, False,  True, False,\n        False,  True, False],\n       [False,  True, False, False,  True, False, False,  True, False,\n        False,  True, False]])\n\n\n\nb[b % 3 == 1]\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#vstack",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#vstack",
    "title": "week_1a_numpy",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#hstack",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#hstack",
    "title": "week_1a_numpy",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#concatenate",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#concatenate",
    "title": "week_1a_numpy",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\nq1.shape, q2.shape,q3.shape\n\n((3, 4), (4, 4), (3, 4))\n\n\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다.\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nnp.concatenate((q1,q3),axis=1)\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#stack",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#stack",
    "title": "week_1a_numpy",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\nq1.shape\n\n(3, 4)\n\n\n\nq3.shape\n\n(3, 4)\n\n\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\nq8.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#행렬-전치",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#행렬-전치",
    "title": "week_1a_numpy",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#행렬-곱셈",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#행렬-곱셈",
    "title": "week_1a_numpy",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#역행렬과-유사-역행렬",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#역행렬과-유사-역행렬",
    "title": "week_1a_numpy",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#단위-행렬",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#단위-행렬",
    "title": "week_1a_numpy",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#qr-분해",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#qr-분해",
    "title": "week_1a_numpy",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.r는 m3와 같습니다\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#행렬식",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#행렬식",
    "title": "week_1a_numpy",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\n\nlinalg.det(m3)  # 행렬식 계산\n\n43.99999999999997"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#고윳값과-고유벡터",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#고윳값과-고유벡터",
    "title": "week_1a_numpy",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#특잇값-분해",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#특잇값-분해",
    "title": "week_1a_numpy",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Σ.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#대각원소와-대각합",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#대각원소와-대각합",
    "title": "week_1a_numpy",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n39"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#선형-방정식-풀기",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#선형-방정식-풀기",
    "title": "week_1a_numpy",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolution을 확인해 보죠:\n\ncoeffs.dot(solution), depvars  # 네 같네요\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#바이너리-.npy-포맷",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#바이너리-.npy-포맷",
    "title": "week_1a_numpy",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\n\na = np.random.rand(2,3)\na\n\narray([[0.66290832, 0.16280721, 0.94895338],\n       [0.06189101, 0.10678874, 0.67879585]])\n\n\n\n# np.save(\"my_array\", a)\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\n\n#with open(\"my_array.npy\", \"rb\") as f:\n#    content = f.read()\n\n#content\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\n\n#a_loaded = np.load(\"my_array.npy\")\n#a_loaded"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#텍스트-포맷",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#텍스트-포맷",
    "title": "week_1a_numpy",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\n\n#np.savetxt(\"my_array.csv\", a)\n\n파일 내용을 확인해 보겠습니다:\n\n#with open(\"my_array.csv\", \"rt\") as f:\n#    print(f.read())\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\n\n#np.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\n\n#a_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\n#a_loaded"
  },
  {
    "objectID": "posts/week_1a_numpy/week_1a_numpy.html#압축된-.npz-포맷",
    "href": "posts/week_1a_numpy/week_1a_numpy.html#압축된-.npz-포맷",
    "title": "week_1a_numpy",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\n#np.savez(\"my_arrays\", my_a=a, my_b=b)\n\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\n\n#with open(\"my_arrays.npz\", \"rb\") as f:\n#    content = f.read()\n\n#repr(content)[:180] + \"[...]\"\n\n다음과 같이 이 파일을 로드할 수 있습니다:\n\n#my_arrays = np.load(\"my_arrays.npz\")\n#my_arrays\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\n#list(my_arrays.keys())\n\n\n#my_arrays[\"my_a\"]"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#series-만들기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#series-만들기",
    "title": "week_1b_pandas",
    "section": "Series 만들기",
    "text": "Series 만들기\n첫 번째 Series 객체를 만들어 보죠!\n\nimport numpy as np\nnp.array([2,-1,3,5])\n\narray([ 2, -1,  3,  5])\n\n\n\ns = pd.Series([2,-1,3,5])\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#d-ndarray와-비슷합니다",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#d-ndarray와-비슷합니다",
    "title": "week_1b_pandas",
    "section": "1D ndarray와 비슷합니다",
    "text": "1D ndarray와 비슷합니다\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다:\n\nimport numpy as np\nnp.exp(s)\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다:\n\ns + [1000,2000,3000,4000]\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다:\n\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\n\ns + 1000\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다:\n\ns < 0\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#인덱스-레이블",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#인덱스-레이블",
    "title": "week_1b_pandas",
    "section": "인덱스 레이블",
    "text": "인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다:\n\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n그다음 dict처럼 Series를 사용할 수 있습니다:\n\ns2[\"bob\"]\n\n83\n\n\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다:\n\ns2[1]\n\n83\n\n\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다:\n\ns2.loc[\"bob\"]\n\n83\n\n\n\ns2.iloc[1]\n\n83\n\n\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다:\n\ns2.iloc[1:3]\n\nbob         83\ncharles    112\ndtype: int64\n\n\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다:\n\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n2    1002\n3    1003\ndtype: int64\n\n\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다:\n\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 0\n\n\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다:\n\nsurprise_slice.iloc[0]\n\n1002\n\n\n\nsurprise_slice.loc[2]\n\n1002"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#dict에서-초기화",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#dict에서-초기화",
    "title": "week_1b_pandas",
    "section": "dict에서 초기화",
    "text": "dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다:\n\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다:\n\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#자동-정렬",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#자동-정렬",
    "title": "week_1b_pandas",
    "section": "자동 정렬",
    "text": "자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\n\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\n\nprint(s2.keys())\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\n\nprint(s3.keys())\n\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\ns2 + s3\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 \"colin\"이 없고 s3에 \"charles\"가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다르고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다:\n\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\n\ns2 = [ 68  83 112  68]\n\nprint(\"s5 =\", s5.values)\n\ns5 = [1000 1000 1000 1000]\n\ns2 + s5\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#스칼라로-초기화",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#스칼라로-초기화",
    "title": "week_1b_pandas",
    "section": "스칼라로 초기화",
    "text": "스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다.\n\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#series-이름",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#series-이름",
    "title": "week_1b_pandas",
    "section": "Series 이름",
    "text": "Series 이름\nSeries는 name을 가질 수 있습니다:\n\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\nbob      83\nalice    68\nName: weights, dtype: int64\n\n\n\ns6.name\n\n'weights'"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#series-그래프-출력",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#series-그래프-출력",
    "title": "week_1b_pandas",
    "section": "Series 그래프 출력",
    "text": "Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다:\n\n#%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\n\ns7.plot()\nplt.show()\n\n\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#시간-범위",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#시간-범위",
    "title": "week_1b_pandas",
    "section": "시간 범위",
    "text": "시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\n\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\n\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\n\n\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다:\n\ntemperatures\n\n[4.4, 5.1, 6.1, 6.2, 6.1, 6.1, 5.7, 5.2, 4.7, 4.1, 3.9, 3.5]\n\n\n\ntemp_series = pd.Series(data = temperatures, index = dates)\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n이 시리즈를 그래프로 출력해 보죠:\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#리샘플링",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#리샘플링",
    "title": "week_1b_pandas",
    "section": "리샘플링",
    "text": "리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다:\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x00000187112F5590>\n\n\n리샘플링 연산은 사실 지연된 연산입니다. (https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95) 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다:\n\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n결과를 그래프로 출력해 보죠:\n\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").mean()\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다:\n\nimport numpy as np\n\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#업샘플링과-보간",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#업샘플링과-보간",
    "title": "week_1b_pandas",
    "section": "업샘플링과 보간",
    "text": "업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다:\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: https://bskyvision.com/789\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#시간대",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#시간대",
    "title": "week_1b_pandas",
    "section": "시간대",
    "text": "시간대\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: https://www.timeanddate.com/time/map/\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다:\n\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다):\n\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다:\n\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n<class 'pytz.exceptions.AmbiguousTimeError'>\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tori-tours&logNo=221221361831\n\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#기간",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#기간",
    "title": "week_1b_pandas",
    "section": "기간",
    "text": "기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠:\n\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다:\n\nquarters + 3\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠:\n\nquarters.asfreq(\"M\")\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\n\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다:\n\nquarters.asfreq(\"M\", how=\"start\")\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\n간격을 늘릴 수도 있습니다: pandas 공식 메뉴얼 참조: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n\nquarters.asfreq(\"A\")\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\n물론 PeriodIndex로 Series를 만들 수 있습니다:\n\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다:\n\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nto_peroid를 호출하면 다시 기간으로 돌아갑니다:\n\nlast_hours.to_period()\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다:\n\nmonths_2022 = pd.period_range(\"2022\", periods=12, freq=\"M\")\none_day_after_last_days = months_2022.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)\nlast_bdays.to_period(\"H\") + 9\n\nPeriodIndex(['2022-01-31 09:00', '2022-02-28 09:00', '2022-03-31 09:00',\n             '2022-04-29 09:00', '2022-05-31 09:00', '2022-06-30 09:00',\n             '2022-07-29 09:00', '2022-08-31 09:00', '2022-09-30 09:00',\n             '2022-10-31 09:00', '2022-11-30 09:00', '2022-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-만들기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-만들기",
    "title": "week_1b_pandas",
    "section": "DataFrame 만들기",
    "text": "DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다:\n\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n         weight  birthyear  children    hobby\nalice        68       1985       NaN   Biking\nbob          83       1984       3.0  Dancing\ncharles     112       1992       0.0      NaN\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다(\"year\"란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Series 객체가 반환됩니다:\n\npeople[\"birthyear\"]\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n동시에 여러 개의 열을 선택할 수 있습니다:\n\npeople[[\"birthyear\", \"hobby\"]]\n\n         birthyear    hobby\nalice         1985   Biking\nbob           1984  Dancing\ncharles       1992      NaN\n\n\n\npeople.loc[:,[\"birthyear\", \"hobby\"]]\n\n         birthyear    hobby\nalice         1985   Biking\nbob           1984  Dancing\ncharles       1992      NaN\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면:\n\npeople_dict\n\n{'weight': alice       68\nbob         83\ncharles    112\ndtype: int64, 'birthyear': bob        1984\nalice      1985\ncharles    1992\nName: year, dtype: int64, 'children': charles    0\nbob        3\ndtype: int64, 'hobby': alice     Biking\nbob      Dancing\ndtype: object}\n\n\n\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\n\n\nd2\n\n        birthyear  weight height\nbob        1984.0    83.0    NaN\nalice      1985.0    68.0    NaN\neugene        NaN     NaN    NaN\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다:\n\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n         birthyear  children    hobby  weight\nalice         1985       NaN   Biking      68\nbob           1984       3.0  Dancing      83\ncharles       1992       0.0      NaN     112\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다:\ndtype = object는 문자열 데이터를 의미\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array\n\nmasked_array(\n  data=[[1985, nan, 'Biking', 68],\n        [1984, 3, 'Dancing', 83],\n        [1992, 0, nan, 112]],\n  mask=False,\n  fill_value='?',\n  dtype=object)\n\n\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n        birthyear children    hobby weight\nalice        1985      NaN   Biking     68\nbob          1984        3  Dancing     83\ncharles      1992        0      NaN    112\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다:\n\nd3\n\n        birthyear children    hobby weight\nalice        1985      NaN   Biking     68\nbob          1984        3  Dancing     83\ncharles      1992        0      NaN    112\n\n\n\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n         hobby children\nalice   Biking      NaN\nbob    Dancing        3\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다:\n\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\n\npeople\n\n         birthyear    hobby  weight  children\nalice         1985   Biking      68       NaN\nbob           1984  Dancing      83       3.0\ncharles       1992      NaN     112       0.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#멀티-인덱싱",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#멀티-인덱싱",
    "title": "week_1b_pandas",
    "section": "멀티 인덱싱",
    "text": "멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면:\n\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n                  public          private         \n               birthyear    hobby  weight children\nParis  alice        1985   Biking      68      NaN\n       bob          1984  Dancing      83      3.0\nLondon charles      1992      NaN     112      0.0\n\n\n이제 \"public\" 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다:\n\nd5[\"public\"]\n\n                birthyear    hobby\nParis  alice         1985   Biking\n       bob           1984  Dancing\nLondon charles       1992      NaN\n\n\n\nd5[\"public\", \"hobby\"]  # d5[\"public\"][\"hobby\"]와 같습니다.\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object\n\n\n\nd5[\"public\"]['hobby']\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: hobby, dtype: object"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#레벨-낮추기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#레벨-낮추기",
    "title": "week_1b_pandas",
    "section": "레벨 낮추기",
    "text": "레벨 낮추기\nd5를 다시 확인해 보죠:\n\nd5\n\n                  public          private         \n               birthyear    hobby  weight children\nParis  alice        1985   Biking      68      NaN\n       bob          1984  Dancing      83      3.0\nLondon charles      1992      NaN     112      0.0\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다):\n\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n                birthyear    hobby  weight  children\nParis  alice         1985   Biking      68       NaN\n       bob           1984  Dancing      83       3.0\nLondon charles       1992      NaN     112       0.0\n\n\n\nd6 = d5.copy()\nd6.index = d6.index.droplevel(level = 0)\nd6\n\n         birthyear    hobby  weight  children\nalice         1985   Biking      68       NaN\nbob           1984  Dancing      83       3.0\ncharles       1992      NaN     112       0.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#전치",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#전치",
    "title": "week_1b_pandas",
    "section": "전치",
    "text": "전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다:\n\nd5\n\n                birthyear    hobby  weight  children\nParis  alice         1985   Biking      68       NaN\n       bob           1984  Dancing      83       3.0\nLondon charles       1992      NaN     112       0.0\n\n\n\nd6 = d5.T\nd6\n\n            Paris           London\n            alice      bob charles\nbirthyear    1985     1984    1992\nhobby      Biking  Dancing     NaN\nweight         68       83     112\nchildren      NaN      3.0     0.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#레벨-스택과-언스택",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#레벨-스택과-언스택",
    "title": "week_1b_pandas",
    "section": "레벨 스택과 언스택",
    "text": "레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다:\n\nd6\n\n            Paris           London\n            alice      bob charles\nbirthyear    1985     1984    1992\nhobby      Biking  Dancing     NaN\nweight         68       83     112\nchildren      NaN      3.0     0.0\n\n\n\nd7 = d6.stack()\nd7\n\n                  London    Paris\nbirthyear alice      NaN     1985\n          bob        NaN     1984\n          charles   1992      NaN\nhobby     alice      NaN   Biking\n          bob        NaN  Dancing\nweight    alice      NaN       68\n          bob        NaN       83\n          charles    112      NaN\nchildren  bob        NaN      3.0\n          charles    0.0      NaN\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\n\nd8 = d7.unstack()\nd8\n\n          London                Paris                 \n           alice  bob charles   alice      bob charles\nbirthyear    NaN  NaN    1992    1985     1984     NaN\nchildren     NaN  NaN     0.0     NaN      3.0     NaN\nhobby        NaN  NaN     NaN  Biking  Dancing     NaN\nweight       NaN  NaN     112      68       83     NaN\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다:\n\nd9 = d8.unstack()\nd9\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다:\n\nd10 = d9.unstack(level = (0,1))\nd10\n\n          London                Paris                 \n           alice  bob charles   alice      bob charles\nbirthyear    NaN  NaN    1992    1985     1984     NaN\nchildren     NaN  NaN     0.0     NaN      3.0     NaN\nhobby        NaN  NaN     NaN  Biking  Dancing     NaN\nweight       NaN  NaN     112      68       83     NaN"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "week_1b_pandas",
    "section": "대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다.\nStack & Unstack + Pivot에 대한 설명 참고 https://pandas.pydata.org/docs/user_guide/reshaping.html\nData Reshaping!\n\nPivot\n\nimport pandas._testing as tm\n\ndef unpivot(frame):\n    N, K = frame.shape\n    data = {\n        \"value\": frame.to_numpy().ravel(\"F\"),\n        \"variable\": np.asarray(frame.columns).repeat(N),\n        \"date\": np.tile(np.asarray(frame.index), K),\n    }\n    return pd.DataFrame(data, columns=[\"date\", \"variable\", \"value\"])\n\ndf = unpivot(tm.makeTimeDataFrame(3))\n\n\ndf\n\n         date variable     value\n0  2000-01-03        A -0.108664\n1  2000-01-04        A -0.913200\n2  2000-01-05        A  0.131372\n3  2000-01-03        B  1.858622\n4  2000-01-04        B  0.085382\n5  2000-01-05        B  0.092182\n6  2000-01-03        C  1.989986\n7  2000-01-04        C -0.077859\n8  2000-01-05        C -0.635210\n9  2000-01-03        D  0.893048\n10 2000-01-04        D -1.251949\n11 2000-01-05        D -1.178071\n\n\nTo select out everything for variable A we could do:\n\nfiltered = df[df[\"variable\"] == \"A\"]\nfiltered\n\n        date variable     value\n0 2000-01-03        A -0.108664\n1 2000-01-04        A -0.913200\n2 2000-01-05        A  0.131372\n\n\nBut suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):\n\npivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\n\npivoted\n\nvariable           A         B         C         D\ndate                                              \n2000-01-03 -0.108664  1.858622  1.989986  0.893048\n2000-01-04 -0.913200  0.085382 -0.077859 -1.251949\n2000-01-05  0.131372  0.092182 -0.635210 -1.178071\n\n\n\npivoted.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object', name='variable')\n\n\n\npivoted.index\n\nDatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', name='date', freq=None)\n\n\nIf the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot(), then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column:\n\ndf[\"value2\"] = df[\"value\"] * 2\n\n\ndf\n\n         date variable     value    value2\n0  2000-01-03        A -0.108664 -0.217328\n1  2000-01-04        A -0.913200 -1.826400\n2  2000-01-05        A  0.131372  0.262744\n3  2000-01-03        B  1.858622  3.717245\n4  2000-01-04        B  0.085382  0.170765\n5  2000-01-05        B  0.092182  0.184364\n6  2000-01-03        C  1.989986  3.979972\n7  2000-01-04        C -0.077859 -0.155719\n8  2000-01-05        C -0.635210 -1.270421\n9  2000-01-03        D  0.893048  1.786096\n10 2000-01-04        D -1.251949 -2.503898\n11 2000-01-05        D -1.178071 -2.356141\n\n\n\npivoted = df.pivot(index=\"date\", columns=\"variable\")\n\npivoted\n\n               value                      ...    value2                    \nvariable           A         B         C  ...         B         C         D\ndate                                      ...                              \n2000-01-03 -0.108664  1.858622  1.989986  ...  3.717245  3.979972  1.786096\n2000-01-04 -0.913200  0.085382 -0.077859  ...  0.170765 -0.155719 -2.503898\n2000-01-05  0.131372  0.092182 -0.635210  ...  0.184364 -1.270421 -2.356141\n\n[3 rows x 8 columns]\n\n\n\npivoted.columns\n\nMultiIndex([( 'value', 'A'),\n            ( 'value', 'B'),\n            ( 'value', 'C'),\n            ( 'value', 'D'),\n            ('value2', 'A'),\n            ('value2', 'B'),\n            ('value2', 'C'),\n            ('value2', 'D')],\n           names=[None, 'variable'])\n\n\n\npivoted['value']\n\nvariable           A         B         C         D\ndate                                              \n2000-01-03 -0.108664  1.858622  1.989986  0.893048\n2000-01-04 -0.913200  0.085382 -0.077859 -1.251949\n2000-01-05  0.131372  0.092182 -0.635210 -1.178071"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#행-참조하기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#행-참조하기",
    "title": "week_1b_pandas",
    "section": "행 참조하기",
    "text": "행 참조하기\npeople DataFrame으로 돌아가 보죠:\n\npeople\n\n         birthyear    hobby  weight  children\nalice         1985   Biking      68       NaN\nbob           1984  Dancing      83       3.0\ncharles       1992      NaN     112       0.0\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다:\n\npeople['birthyear']\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople.loc[\"charles\"]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다:\n\npeople.iloc[2]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다:\n\npeople\n\n         birthyear    hobby  weight  children\nalice         1985   Biking      68       NaN\nbob           1984  Dancing      83       3.0\ncharles       1992      NaN     112       0.0\n\n\n\npeople.iloc[1:3]\n\n         birthyear    hobby  weight  children\nbob           1984  Dancing      83       3.0\ncharles       1992      NaN     112       0.0\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다:\n\npeople[np.array([True, False, True])]\n\n         birthyear   hobby  weight  children\nalice         1985  Biking      68       NaN\ncharles       1992     NaN     112       0.0\n\n\n불리언 표현식을 사용할 때 아주 유용합니다:\n\npeople[\"birthyear\"] < 1990\n\nalice       True\nbob         True\ncharles    False\nName: birthyear, dtype: bool\n\n\n\npeople[people[\"birthyear\"] < 1990]\n\n       birthyear    hobby  weight  children\nalice       1985   Biking      68       NaN\nbob         1984  Dancing      83       3.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#열-추가-삭제",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#열-추가-삭제",
    "title": "week_1b_pandas",
    "section": "열 추가, 삭제",
    "text": "열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다:\n\npeople\n\n         birthyear    hobby  weight  children\nalice         1985   Biking      68       NaN\nbob           1984  Dancing      83       3.0\ncharles       1992      NaN     112       0.0\n\n\n\npeople[\"age\"] = 2022 - people[\"birthyear\"]  # \"age\" 열을 추가합니다\npeople[\"over 30\"] = people[\"age\"] > 30      # \"over 30\" 열을 추가합니다\n\npeople\n\n         birthyear    hobby  weight  children  age  over 30\nalice         1985   Biking      68       NaN   37     True\nbob           1984  Dancing      83       3.0   38     True\ncharles       1992      NaN     112       0.0   30    False\n\n\n\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\n\nbirthyears\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople\n\n           hobby  weight  age  over 30\nalice     Biking      68   37     True\nbob      Dancing      83   38     True\ncharles      NaN     112   30    False\n\n\n\n# 딕셔너리도 유사함\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\n\n\nweights.pop(\"alice\")\n\n68\n\n\n\nweights\n\n{'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\ndel weights[\"bob\"]\n\n\nweights\n\n{'colin': 86, 'darwin': 68}\n\n\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다:\n\npeople.index\n\nIndex(['alice', 'bob', 'charles'], dtype='object')\n\n\n\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice 누락됨, eugene은 무시됨\npeople\n\n           hobby  weight  age  over 30  pets\nalice     Biking      68   37     True   NaN\nbob      Dancing      83   38     True   0.0\ncharles      NaN     112   30    False   5.0\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다:\n\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n           hobby  height  weight  age  over 30  pets\nalice     Biking     172      68   37     True   NaN\nbob      Dancing     181      83   38     True   0.0\ncharles      NaN     185     112   30    False   5.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#새로운-열-할당하기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#새로운-열-할당하기",
    "title": "week_1b_pandas",
    "section": "새로운 열 할당하기",
    "text": "새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다:\n\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] > 0\n)\n\n           hobby  height  weight  age  over 30  pets  body_mass_index  has_pets\nalice     Biking     172      68   37     True   NaN        22.985398     False\nbob      Dancing     181      83   38     True   0.0        25.335002     False\ncharles      NaN     185     112   30    False   5.0        32.724617      True\n\n\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n\npeople\n\n           hobby  height  weight  age  over 30  pets  body_mass_index\nalice     Biking     172      68   37     True   NaN        22.985398\nbob      Dancing     181      83   38     True   0.0        25.335002\ncharles      NaN     185     112   30    False   5.0        32.724617\n\n\n\ndel people[\"body_mass_index\"]\n\n\npeople\n\n           hobby  height  weight  age  over 30  pets\nalice     Biking     172      68   37     True   NaN\nbob      Dancing     181      83   38     True   0.0\ncharles      NaN     185     112   30    False   5.0\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\n\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] > 25\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\n\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] > 25)\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002        True\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다:\n\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] > 25)\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다:\n\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] > 25)\n)\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002        True\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]\n\n\n문제가 해결되었군요!\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\npeople[\"overweight\"] = people[\"body_mass_index\"]>25\n\n\npeople\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002        True\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#표현식-평가",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#표현식-평가",
    "title": "week_1b_pandas",
    "section": "표현식 평가",
    "text": "표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\n\npeople\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002        True\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]\n\n\n\n\"weight / (height/100) ** 2 > 25\"\n\n'weight / (height/100) ** 2 > 25'\n\n\n\npeople.eval(\"weight / (height/100) ** 2 > 25\")\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다:\n\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002        True\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다:\n\npeople\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002        True\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]\n\n\n\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True)\npeople\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002       False\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-쿼리하기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-쿼리하기",
    "title": "week_1b_pandas",
    "section": "DataFrame 쿼리하기",
    "text": "DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다:\n\npeople\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\nalice     Biking     172      68  ...   NaN        22.985398       False\nbob      Dancing     181      83  ...   0.0        25.335002       False\ncharles      NaN     185     112  ...   5.0        32.724617        True\n\n[3 rows x 8 columns]\n\n\n\npeople.query(\"age > 30 and pets == 0\")\n\n       hobby  height  weight  age  over 30  pets  body_mass_index  overweight\nbob  Dancing     181      83   38     True   0.0        25.335002       False\n\n\n아래의 방법이 더 자주 사용됨\n\npeople[(people[\"age\"]>30) & (people[\"pets\"] == 0)]\n\n       hobby  height  weight  age  over 30  pets  body_mass_index  overweight\nbob  Dancing     181      83   38     True   0.0        25.335002       False\n\n\n\nmask = (people[\"age\"]>30) & (people[\"pets\"] == 0)\n\n\npeople[mask]\n\n       hobby  height  weight  age  over 30  pets  body_mass_index  overweight\nbob  Dancing     181      83   38     True   0.0        25.335002       False"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-정렬",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-정렬",
    "title": "week_1b_pandas",
    "section": "DataFrame 정렬",
    "text": "DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠:\n\npeople.sort_index(ascending=False)\n\n           hobby  height  weight  ...  pets  body_mass_index  overweight\ncharles      NaN     185     112  ...   5.0        32.724617        True\nbob      Dancing     181      83  ...   0.0        25.335002       False\nalice     Biking     172      68  ...   NaN        22.985398       False\n\n[3 rows x 8 columns]\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다:\n\npeople.sort_index(axis=1, inplace=True)\npeople\n\n         age  body_mass_index  height  ... overweight  pets  weight\nalice     37        22.985398     172  ...      False   NaN      68\nbob       38        25.335002     181  ...      False   0.0      83\ncharles   30        32.724617     185  ...       True   5.0     112\n\n[3 rows x 8 columns]\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다:\n\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n         age  body_mass_index  height  ... overweight  pets  weight\ncharles   30        32.724617     185  ...       True   5.0     112\nalice     37        22.985398     172  ...      False   NaN      68\nbob       38        25.335002     181  ...      False   0.0      83\n\n[3 rows x 8 columns]"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-그래프-그리기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-그래프-그리기",
    "title": "week_1b_pandas",
    "section": "DataFrame 그래프 그리기",
    "text": "DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다:\n\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\n\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요.\n\nHistogram\n\n\ndf4 = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000) + 1,\n        \"b\": np.random.randn(1000),\n        \"c\": np.random.randn(1000) - 1,\n    },\n    columns=[\"a\", \"b\", \"c\"],\n)\n\nplt.figure();\n\ndf4.plot.hist(alpha=0.5);\n\n\ndf4\n\n            a         b         c\n0    1.506963  0.490901 -1.900364\n1    0.814504  1.500996  0.781771\n2    1.251316  1.414835 -1.748052\n3    1.919667  0.316833 -0.080313\n4    2.798829 -0.559509  0.431584\n..        ...       ...       ...\n995  1.855788 -0.190072 -2.593906\n996  2.576432  0.998805 -0.205988\n997  1.185178 -1.142957 -3.008392\n998  1.000368 -0.471479 -0.865947\n999  1.031370 -0.283910 -0.958816\n\n[1000 rows x 3 columns]\n\n\n\ndf4.plot(kind=\"hist\",alpha=0.5, x=\"a\")\nplt.show()\n\n\n\n\n\ndf4['a'].plot.hist()\nplt.show()\n\n\n\n\n\nBoxplot\n\n\ndf\n\n         date variable     value    value2\n0  2000-01-03        A -0.108664 -0.217328\n1  2000-01-04        A -0.913200 -1.826400\n2  2000-01-05        A  0.131372  0.262744\n3  2000-01-03        B  1.858622  3.717245\n4  2000-01-04        B  0.085382  0.170765\n5  2000-01-05        B  0.092182  0.184364\n6  2000-01-03        C  1.989986  3.979972\n7  2000-01-04        C -0.077859 -0.155719\n8  2000-01-05        C -0.635210 -1.270421\n9  2000-01-03        D  0.893048  1.786096\n10 2000-01-04        D -1.251949 -2.503898\n11 2000-01-05        D -1.178071 -2.356141\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\ndf.plot.box();\n\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\ndf\n\n       Col1      Col2  X\n0  0.499669  0.779945  A\n1  0.037291  0.644845  A\n2  0.063303  0.252593  A\n3  0.538277  0.083526  A\n4  0.927436  0.747575  A\n5  0.171397  0.056565  B\n6  0.379479  0.231624  B\n7  0.293040  0.916849  B\n8  0.737468  0.659382  B\n9  0.040580  0.094164  B\n\n\n\nplt.figure();\n\nbp = df.boxplot(by=\"X\")"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-연산",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#dataframe-연산",
    "title": "week_1b_pandas",
    "section": "DataFrame 연산",
    "text": "DataFrame 연산\nDataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n         sep  oct  nov\nalice      8    8    9\nbob       10    9    9\ncharles    4    8    2\ndarwin     9   10   10\n\n\nDataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다:\n\nnp.sqrt(grades)\n\n              sep       oct       nov\nalice    2.828427  2.828427  3.000000\nbob      3.162278  3.000000  3.000000\ncharles  2.000000  2.828427  1.414214\ndarwin   3.000000  3.162278  3.162278\n\n\n비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다:\n\ngrades + 1\n\n         sep  oct  nov\nalice      9    9   10\nbob       11   10   10\ncharles    5    9    3\ndarwin    10   11   11\n\n\n물론 산술 연산(*,/,**…)과 조건 연산(>, ==…)을 포함해 모든 이항 연산에도 마찬가지 입니다:\n\ngrades >= 5\n\n           sep   oct    nov\nalice     True  True   True\nbob       True  True   True\ncharles  False  True  False\ndarwin    True  True   True\n\n\nDataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다:\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nall 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠:\n\n(grades > 5).all()\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\ngrades\n\n         sep  oct  nov\nalice      8    8    9\nbob       10    9    9\ncharles    4    8    2\ndarwin     9   10   10\n\n\n\n(grades > 5).all(axis = 1)\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nany 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠:\n\n(grades == 10).any(axis = 1)\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nDataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠:\n\ngrades\n\n         sep  oct  nov\nalice      8    8    9\nbob       10    9    9\ncharles    4    8    2\ndarwin     9   10   10\n\n\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\n\ngrades - grades.mean()  # grades - [7.75, 8.75, 7.50] 와 동일\n\n          sep   oct  nov\nalice    0.25 -0.75  1.5\nbob      2.25  0.25  1.5\ncharles -3.75 -0.75 -5.5\ndarwin   1.25  1.25  2.5\n\n\n모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다:\n\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n          sep   oct  nov\nalice    7.75  8.75  7.5\nbob      7.75  8.75  7.5\ncharles  7.75  8.75  7.5\ndarwin   7.75  8.75  7.5\n\n\n모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다:\n\ngrades.values.mean()\n\n8.0\n\n\n\ngrades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다\n\n         sep  oct  nov\nalice    0.0  0.0  1.0\nbob      2.0  1.0  1.0\ncharles -4.0  0.0 -6.0\ndarwin   1.0  2.0  2.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#자동-정렬-1",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#자동-정렬-1",
    "title": "week_1b_pandas",
    "section": "자동 정렬",
    "text": "자동 정렬\nSeries와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n         sep  oct  nov\nalice      8    8    9\nbob       10    9    9\ncharles    4    8    2\ndarwin     9   10   10\n\n\n\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n         oct  nov  dec\nbob      0.0  NaN  2.0\ncolin    NaN  1.0  0.0\ndarwin   0.0  1.0  0.0\ncharles  3.0  3.0  0.0\n\n\n\ngrades + bonus_points\n\n         dec   nov   oct  sep\nalice    NaN   NaN   NaN  NaN\nbob      NaN   NaN   9.0  NaN\ncharles  NaN   5.0  11.0  NaN\ncolin    NaN   NaN   NaN  NaN\ndarwin   NaN  11.0  10.0  NaN\n\n\n덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다."
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#누락된-데이터-다루기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#누락된-데이터-다루기",
    "title": "week_1b_pandas",
    "section": "누락된 데이터 다루기",
    "text": "누락된 데이터 다루기\n실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다.\n위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다:\n\n(grades + bonus_points).fillna(0)\n\n         dec   nov   oct  sep\nalice    0.0   0.0   0.0  0.0\nbob      0.0   0.0   9.0  0.0\ncharles  0.0   5.0  11.0  0.0\ncolin    0.0   0.0   0.0  0.0\ndarwin   0.0  11.0  10.0  0.0\n\n\n9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다:\n\nbonus_points\n\n         oct  nov  dec\nbob      0.0  NaN  2.0\ncolin    NaN  1.0  0.0\ndarwin   0.0  1.0  0.0\ncharles  3.0  3.0  0.0\n\n\n\nfixed_bonus_points = bonus_points.fillna(0) # NA 값 0으로 바꾸기\nfixed_bonus_points.insert(loc=0, column=\"sep\", value=0) # 누락된 컬럼 만들기\nfixed_bonus_points.loc[\"alice\"] = 0 # 누락된 행 만들기\nfixed_bonus_points\n\n         sep  oct  nov  dec\nbob        0  0.0  0.0  2.0\ncolin      0  0.0  1.0  0.0\ndarwin     0  0.0  1.0  0.0\ncharles    0  3.0  3.0  0.0\nalice      0  0.0  0.0  0.0\n\n\n\ngrades + fixed_bonus_points\n\n         dec   nov   oct   sep\nalice    NaN   9.0   8.0   8.0\nbob      NaN   9.0   9.0  10.0\ncharles  NaN   5.0  11.0   4.0\ncolin    NaN   NaN   NaN   NaN\ndarwin   NaN  11.0  10.0   9.0\n\n\n훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다.\n누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠:\n\nbonus_points\n\n         oct  nov  dec\nbob      0.0  NaN  2.0\ncolin    NaN  1.0  0.0\ndarwin   0.0  1.0  0.0\ncharles  3.0  3.0  0.0\n\n\ninterpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다.\n\nbonus_points.interpolate(axis=1)\n\n         oct  nov  dec\nbob      0.0  1.0  2.0\ncolin    NaN  1.0  0.0\ndarwin   0.0  1.0  0.0\ncharles  3.0  3.0  0.0\n\n\nbob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다.\n\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n         sep  oct  nov  dec\nbob      0.0  0.0  1.0  2.0\ncolin    0.0  0.5  1.0  0.0\ndarwin   0.0  0.0  1.0  0.0\ncharles  0.0  3.0  3.0  0.0\nalice    0.0  0.0  0.0  0.0\n\n\n좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠:\n\ngrades + better_bonus_points\n\n         dec   nov   oct   sep\nalice    NaN   9.0   8.0   8.0\nbob      NaN  10.0   9.0  10.0\ncharles  NaN   5.0  11.0   4.0\ncolin    NaN   NaN   NaN   NaN\ndarwin   NaN  11.0  10.0   9.0\n\n\n9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 \"dec\" 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다:\n\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n          sep   oct   nov  dec\nalice     8.0   8.0   9.0  NaN\nbob      10.0   9.0  10.0  NaN\ncharles   4.0  11.0   5.0  NaN\ncolin     NaN   NaN   NaN  NaN\ndarwin    9.0  10.0  11.0  NaN\n\n\n12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다:\n\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n          sep   oct   nov  dec\nalice     8.0   8.0   9.0  NaN\nbob      10.0   9.0  10.0  NaN\ncharles   4.0  11.0   5.0  NaN\ndarwin    9.0  10.0  11.0  NaN\n\n\n그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다:\n\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n          sep   oct   nov\nalice     8.0   8.0   9.0\nbob      10.0   9.0  10.0\ncharles   4.0  11.0   5.0\ndarwin    9.0  10.0  11.0"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#groupby로-집계하기",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#groupby로-집계하기",
    "title": "week_1b_pandas",
    "section": "groupby로 집계하기",
    "text": "groupby로 집계하기\nSQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다.\n먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다:\n\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n          sep   oct   nov  dec    hobby\nalice     8.0   8.0   9.0  NaN   Biking\nbob      10.0   9.0  10.0  NaN  Dancing\ncharles   4.0  11.0   5.0  NaN      NaN\ncolin     NaN   NaN   NaN  NaN  Dancing\ndarwin    9.0  10.0  11.0  NaN   Biking\n\n\nhobby로 이 DataFrame을 그룹핑해 보죠:\n\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000187269D6510>\n\n\n이제 hobby마다 평균 점수를 계산할 수 있습니다:\n\ngrouped_grades.mean()\n\n          sep  oct   nov  dec\nhobby                        \nBiking    8.5  9.0  10.0  NaN\nDancing  10.0  9.0  10.0  NaN\n\n\n\nfinal_grades.groupby(\"hobby\").mean()\n\n          sep  oct   nov  dec\nhobby                        \nBiking    8.5  9.0  10.0  NaN\nDancing  10.0  9.0  10.0  NaN\n\n\n아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다."
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#피봇-테이블",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#피봇-테이블",
    "title": "week_1b_pandas",
    "section": "피봇 테이블",
    "text": "피봇 테이블\n판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\nbonus_points.stack().reset_index()\n\n   level_0 level_1    0\n0      bob     oct  0.0\n1      bob     dec  2.0\n2    colin     nov  1.0\n3    colin     dec  0.0\n4   darwin     oct  0.0\n5   darwin     nov  1.0\n6   darwin     dec  0.0\n7  charles     oct  3.0\n8  charles     nov  3.0\n9  charles     dec  0.0\n\n\n\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n       name month  grade  bonus\n0     alice   sep    8.0    NaN\n1     alice   oct    8.0    NaN\n2     alice   nov    9.0    NaN\n3       bob   sep   10.0    0.0\n4       bob   oct    9.0    NaN\n5       bob   nov   10.0    2.0\n6   charles   sep    4.0    3.0\n7   charles   oct   11.0    3.0\n8   charles   nov    5.0    0.0\n9    darwin   sep    9.0    0.0\n10   darwin   oct   10.0    1.0\n11   darwin   nov   11.0    0.0\n\n\n이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다:\n\npd.pivot_table(more_grades, index=\"name\")\n\n            bonus      grade\nname                        \nalice         NaN   8.333333\nbob      1.000000   9.666667\ncharles  2.000000   6.666667\ndarwin   0.333333  10.000000\n\n<string>:1: FutureWarning: pivot_table dropped a column because it failed to aggregate. This behavior is deprecated and will raise in a future version of pandas. Select only the columns that can be aggregated.\n\n\n집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n         bonus  grade\nname                 \nalice      NaN    9.0\nbob        2.0   10.0\ncharles    3.0   11.0\ndarwin     1.0   11.0\n\n\ncolumns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\nmonth      nov   oct    sep        All\nname                                  \nalice     9.00   8.0   8.00   8.333333\nbob      10.00   9.0  10.00   9.666667\ncharles   5.00  11.0   4.00   6.666667\ndarwin   11.00  10.0   9.00  10.000000\nAll       8.75   9.5   7.75   8.666667\n\n\n마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다:\n\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n               bonus  grade\nname    month              \nalice   nov      NaN   9.00\n        oct      NaN   8.00\n        sep      NaN   8.00\nbob     nov    2.000  10.00\n        oct      NaN   9.00\n        sep    0.000  10.00\ncharles nov    0.000   5.00\n        oct    3.000  11.00\n        sep    3.000   4.00\ndarwin  nov    0.000  11.00\n        oct    1.000  10.00\n        sep    0.000   9.00\nAll            1.125   8.75"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#함수",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#함수",
    "title": "week_1b_pandas",
    "section": "함수",
    "text": "함수\n큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다:\n\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n         A     B     C some_text      D  ...      V      W     X      Y      Z\n0      NaN  11.0  44.0    Blabla   99.0  ...    NaN   88.0  22.0  165.0  143.0\n1     11.0  22.0  55.0    Blabla  110.0  ...    NaN   99.0  33.0    NaN  154.0\n2     22.0  33.0  66.0    Blabla  121.0  ...   11.0  110.0  44.0    NaN  165.0\n3     33.0  44.0  77.0    Blabla  132.0  ...   22.0  121.0  55.0   11.0    NaN\n4     44.0  55.0  88.0    Blabla  143.0  ...   33.0  132.0  66.0   22.0    NaN\n...    ...   ...   ...       ...    ...  ...    ...    ...   ...    ...    ...\n9995   NaN   NaN  33.0    Blabla   88.0  ...  165.0   77.0  11.0  154.0  132.0\n9996   NaN  11.0  44.0    Blabla   99.0  ...    NaN   88.0  22.0  165.0  143.0\n9997  11.0  22.0  55.0    Blabla  110.0  ...    NaN   99.0  33.0    NaN  154.0\n9998  22.0  33.0  66.0    Blabla  121.0  ...   11.0  110.0  44.0    NaN  165.0\n9999  33.0  44.0  77.0    Blabla  132.0  ...   22.0  121.0  55.0   11.0    NaN\n\n[10000 rows x 27 columns]\n\n\nhead() 메서드는 처음 5개 행을 반환합니다:\n\nlarge_df.head(n=10)\n\n      A      B      C some_text      D  ...     V      W      X      Y      Z\n0   NaN   11.0   44.0    Blabla   99.0  ...   NaN   88.0   22.0  165.0  143.0\n1  11.0   22.0   55.0    Blabla  110.0  ...   NaN   99.0   33.0    NaN  154.0\n2  22.0   33.0   66.0    Blabla  121.0  ...  11.0  110.0   44.0    NaN  165.0\n3  33.0   44.0   77.0    Blabla  132.0  ...  22.0  121.0   55.0   11.0    NaN\n4  44.0   55.0   88.0    Blabla  143.0  ...  33.0  132.0   66.0   22.0    NaN\n5  55.0   66.0   99.0    Blabla  154.0  ...  44.0  143.0   77.0   33.0   11.0\n6  66.0   77.0  110.0    Blabla  165.0  ...  55.0  154.0   88.0   44.0   22.0\n7  77.0   88.0  121.0    Blabla    NaN  ...  66.0  165.0   99.0   55.0   33.0\n8  88.0   99.0  132.0    Blabla    NaN  ...  77.0    NaN  110.0   66.0   44.0\n9  99.0  110.0  143.0    Blabla   11.0  ...  88.0    NaN  121.0   77.0   55.0\n\n[10 rows x 27 columns]\n\n\n마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다:\n\nlarge_df.tail(n=2)\n\n         A     B     C some_text      D  ...     V      W     X     Y      Z\n9998  22.0  33.0  66.0    Blabla  121.0  ...  11.0  110.0  44.0   NaN  165.0\n9999  33.0  44.0  77.0    Blabla  132.0  ...  22.0  121.0  55.0  11.0    NaN\n\n[2 rows x 27 columns]\n\n\ninfo() 메서드는 각 열의 내용을 요약하여 출력합니다:\n\nlarge_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\n마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다:\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: null(NaN)이 아닌 값의 개수 * mean: null이 아닌 값의 평균 * std: null이 아닌 값의 표준 편차 * min: null이 아닌 값의 최솟값 * 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 * max: null이 아닌 값의 최댓값\n\nlarge_df.describe()\n\n                 A            B  ...            Y            Z\ncount  8823.000000  8824.000000  ...  8822.000000  8823.000000\nmean     87.977559    87.972575  ...    88.000000    88.022441\nstd      47.535911    47.535523  ...    47.536879    47.535911\nmin      11.000000    11.000000  ...    11.000000    11.000000\n25%      44.000000    44.000000  ...    44.000000    44.000000\n50%      88.000000    88.000000  ...    88.000000    88.000000\n75%     132.000000   132.000000  ...   132.000000   132.000000\nmax     165.000000   165.000000  ...   165.000000   165.000000\n\n[8 rows x 26 columns]"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#저장",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#저장",
    "title": "week_1b_pandas",
    "section": "저장",
    "text": "저장\nCSV, HTML, JSON로 저장해 보죠:\n\nmy_df.to_csv(\"my_df.csv\")\n#my_df.to_html(\"my_df.html\")\n#my_df.to_json(\"my_df.json\")\n\n저장된 내용을 확인해 보죠:\n\n#for filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n#    print(\"#\", filename)\n#    with open(filename, \"rt\") as f:\n#        print(f.read())\n#\n\n인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 <th> 태그와 JSON에서는 키로 저장되었습니다.\n다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다:\n\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\nNo module named 'openpyxl'"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#로딩",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#로딩",
    "title": "week_1b_pandas",
    "section": "로딩",
    "text": "로딩\nCSV 파일을 DataFrame으로 로드해 보죠:\n\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n         hobby  weight  birthyear  children\nalice   Biking    68.5       1985       NaN\nbob    Dancing    83.1       1984       3.0\n\n\n예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠:\n\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n                     State  Population        lat         lon\nCity                                                         \nMarysville      Washington       63269  48.051764 -122.177082\nPerris          California       72326  33.782519 -117.228648\nCleveland             Ohio      390113  41.499320  -81.694361\nWorcester    Massachusetts      182544  42.262593  -71.802293\nColumbia    South Carolina      133358  34.000710  -81.034814\n\n\n이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#sql-조인",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#sql-조인",
    "title": "week_1b_pandas",
    "section": "SQL 조인",
    "text": "SQL 조인\n판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n  state            city        lat         lng\n0    CA   San Francisco  37.781334 -122.416728\n1    NY        New York  40.705649  -74.008344\n2    FL           Miami  25.791100  -80.320733\n3    OH       Cleveland  41.473508  -81.739791\n4    UT  Salt Lake City  40.755851 -111.896657\n\n\n\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n   population           city       state\n3      808976  San Francisco  California\n4     8363710       New York    New-York\n5      413201          Miami     Florida\n6     2242193        Houston       Texas\n\n\n이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n  state_x           city        lat         lng  population     state_y\n0      CA  San Francisco  37.781334 -122.416728      808976  California\n1      NY       New York  40.705649  -74.008344     8363710    New-York\n2      FL          Miami  25.791100  -80.320733      413201     Florida\n\n\n두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다.\n또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=\"outer\"로 지정합니다:\n\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n  state_x            city        lat         lng  population     state_y\n0      CA   San Francisco  37.781334 -122.416728    808976.0  California\n1      NY        New York  40.705649  -74.008344   8363710.0    New-York\n2      FL           Miami  25.791100  -80.320733    413201.0     Florida\n3      OH       Cleveland  41.473508  -81.739791         NaN         NaN\n4      UT  Salt Lake City  40.755851 -111.896657         NaN         NaN\n5     NaN         Houston        NaN         NaN   2242193.0       Texas\n\n\n물론 LEFT OUTER JOIN은 how=\"left\"로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=\"right\"는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n  state_x           city        lat         lng  population     state_y\n0      CA  San Francisco  37.781334 -122.416728      808976  California\n1      NY       New York  40.705649  -74.008344     8363710    New-York\n2      FL          Miami  25.791100  -80.320733      413201     Florida\n3     NaN        Houston        NaN         NaN     2242193       Texas\n\n\n조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어:\n\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\ncity_pop2\n\n   population           name       state\n3      808976  San Francisco  California\n4     8363710       New York    New-York\n5      413201          Miami     Florida\n6     2242193        Houston       Texas\n\n\n\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n  state_x           city        lat  ...  population           name     state_y\n0      CA  San Francisco  37.781334  ...      808976  San Francisco  California\n1      NY       New York  40.705649  ...     8363710       New York    New-York\n2      FL          Miami  25.791100  ...      413201          Miami     Florida\n\n[3 rows x 7 columns]"
  },
  {
    "objectID": "posts/week_1b_pandas/week_1b_pandas.html#연결",
    "href": "posts/week_1b_pandas/week_1b_pandas.html#연결",
    "title": "week_1b_pandas",
    "section": "연결",
    "text": "연결\nDataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다:\n\ncity_loc\n\n  state            city        lat         lng\n0    CA   San Francisco  37.781334 -122.416728\n1    NY        New York  40.705649  -74.008344\n2    FL           Miami  25.791100  -80.320733\n3    OH       Cleveland  41.473508  -81.739791\n4    UT  Salt Lake City  40.755851 -111.896657\n\n\n\ncity_pop\n\n   population           city       state\n3      808976  San Francisco  California\n4     8363710       New York    New-York\n5      413201          Miami     Florida\n6     2242193        Houston       Texas\n\n\n\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n        state            city        lat         lng  population\n0          CA   San Francisco  37.781334 -122.416728         NaN\n1          NY        New York  40.705649  -74.008344         NaN\n2          FL           Miami  25.791100  -80.320733         NaN\n3          OH       Cleveland  41.473508  -81.739791         NaN\n4          UT  Salt Lake City  40.755851 -111.896657         NaN\n3  California   San Francisco        NaN         NaN    808976.0\n4    New-York        New York        NaN         NaN   8363710.0\n5     Florida           Miami        NaN         NaN    413201.0\n6       Texas         Houston        NaN         NaN   2242193.0\n\n\n이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다:\n\nresult_concat.loc[3]\n\n        state           city        lat        lng  population\n3          OH      Cleveland  41.473508 -81.739791         NaN\n3  California  San Francisco        NaN        NaN    808976.0\n\n\n또는 인덱스를 무시하도록 설정할 수 있습니다:\n\npd.concat([city_loc, city_pop], ignore_index=True)\n\n        state            city        lat         lng  population\n0          CA   San Francisco  37.781334 -122.416728         NaN\n1          NY        New York  40.705649  -74.008344         NaN\n2          FL           Miami  25.791100  -80.320733         NaN\n3          OH       Cleveland  41.473508  -81.739791         NaN\n4          UT  Salt Lake City  40.755851 -111.896657         NaN\n5  California   San Francisco        NaN         NaN    808976.0\n6    New-York        New York        NaN         NaN   8363710.0\n7     Florida           Miami        NaN         NaN    413201.0\n8       Texas         Houston        NaN         NaN   2242193.0\n\n\n한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=\"inner\"로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다:\n\npd.concat([city_loc, city_pop], join=\"inner\")\n\n        state            city\n0          CA   San Francisco\n1          NY        New York\n2          FL           Miami\n3          OH       Cleveland\n4          UT  Salt Lake City\n3  California   San Francisco\n4    New-York        New York\n5     Florida           Miami\n6       Texas         Houston\n\n\naxis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다:\n\npd.concat([city_loc, city_pop], axis=1)\n\n  state            city        lat  ...  population           city       state\n0    CA   San Francisco  37.781334  ...         NaN            NaN         NaN\n1    NY        New York  40.705649  ...         NaN            NaN         NaN\n2    FL           Miami  25.791100  ...         NaN            NaN         NaN\n3    OH       Cleveland  41.473508  ...    808976.0  San Francisco  California\n4    UT  Salt Lake City  40.755851  ...   8363710.0       New York    New-York\n5   NaN             NaN        NaN  ...    413201.0          Miami     Florida\n6   NaN             NaN        NaN  ...   2242193.0        Houston       Texas\n\n[7 rows x 7 columns]\n\n\n이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠:\n\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n               state        lat         lng  population       state\ncity                                                               \nSan Francisco     CA  37.781334 -122.416728    808976.0  California\nNew York          NY  40.705649  -74.008344   8363710.0    New-York\nMiami             FL  25.791100  -80.320733    413201.0     Florida\nCleveland         OH  41.473508  -81.739791         NaN         NaN\nSalt Lake City    UT  40.755851 -111.896657         NaN         NaN\nHouston          NaN        NaN         NaN   2242193.0       Texas\n\n\nFULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다.\nappend() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다:\n\ncity_loc.append(city_pop)\n\n        state            city        lat         lng  population\n0          CA   San Francisco  37.781334 -122.416728         NaN\n1          NY        New York  40.705649  -74.008344         NaN\n2          FL           Miami  25.791100  -80.320733         NaN\n3          OH       Cleveland  41.473508  -81.739791         NaN\n4          UT  Salt Lake City  40.755851 -111.896657         NaN\n3  California   San Francisco        NaN         NaN    808976.0\n4    New-York        New York        NaN         NaN   8363710.0\n5     Florida           Miami        NaN         NaN    413201.0\n6       Texas         Houston        NaN         NaN   2242193.0\n\n<string>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\npd.concat([city_loc,city_pop])\n\n        state            city        lat         lng  population\n0          CA   San Francisco  37.781334 -122.416728         NaN\n1          NY        New York  40.705649  -74.008344         NaN\n2          FL           Miami  25.791100  -80.320733         NaN\n3          OH       Cleveland  41.473508  -81.739791         NaN\n4          UT  Salt Lake City  40.755851 -111.896657         NaN\n3  California   San Francisco        NaN         NaN    808976.0\n4    New-York        New York        NaN         NaN   8363710.0\n5     Florida           Miami        NaN         NaN    413201.0\n6       Texas         Houston        NaN         NaN   2242193.0\n\n\n판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Geocomputation Apply\n\n\n\n\n\n\n\nR\n\n\n\n\nGeocomputation\n\n\n\n\n\n\nApr 29, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nLinear model\n\n\n\n\n\n\n\nR\n\n\n\n\nStatistics basics\n\n\n\n\n\n\nApr 27, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nPlotly\n\n\n\n\n\n\n\nR\n\n\n\n\nInteractive Plot Basic\n\n\n\n\n\n\nApr 27, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nR Statistics\n\n\n\n\n\n\n\nR\n\n\n\n\nR basics\n\n\n\n\n\n\nApr 14, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nR basics\n\n\n\n\n\n\n\nR\n\n\n\n\nR basics\n\n\n\n\n\n\nApr 11, 2023\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nSQL\n\n\n\n\n\n\n\nSQL\n\n\n\n\nSQL Basics\n\n\n\n\n\n\nApr 10, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nweek_1a_numpy\n\n\n\n\n\n\n\nPython\n\n\nLecture\n\n\n\n\nPython Basics\n\n\n\n\n\n\nApr 6, 2023\n\n\nJiho Yeo\n\n\n\n\n\n\n  \n\n\n\n\nweek_1b_pandas\n\n\n\n\n\n\n\nPython\n\n\nLecture\n\n\n\n\nPython Basics\n\n\n\n\n\n\nApr 6, 2023\n\n\nJiho Yeo\n\n\n\n\n\n\n  \n\n\n\n\nPython playground\n\n\n\n\n\n\n\nPython\n\n\n\n\nDatamining Basics\n\n\n\n\n\n\nMar 28, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nGeocomputation with R\n\n\n\n\n\n\n\nR\n\n\n\n\nCreative & Experimental geography\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTensorflow ex\n\n\n\n\n\n\n\nPython\n\n\n\n\nDatamining Basics\n\n\n\n\n\n\nMar 21, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nggplot, ggplot\n\n\n\n\n\n\n\nR\n\n\n\n\nData Visualization\n\n\n\n\n\n\nMar 15, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\npizza\n\n\n\n\n\nPost\n\n\n\n\n\n\nMar 14, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nIssue Report\n\n\n\n\n\n\n\nIssue Report\n\n\n\n\n에잇!\n\n\n\n\n\n\nMar 3, 2023\n\n\nsungil_park\n\n\n\n\n\n\n  \n\n\n\n\nTraffic line detection using CV2\n\n\n\n\n\n\n\nPython\n\n\n\n\nPython, cv2\n\n\n\n\n\n\nOct 18, 2022\n\n\nsungil_park\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recordings.html",
    "href": "recordings.html",
    "title": "Recordings",
    "section": "",
    "text": "Running\n\n\n\nAthletics\n\n\n\nRunning\n\n\n\nsungil_park\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStress is water soluble\n\n\n\nAthletics\n\n\n\nSwimming\n\n\n\nsungil_park\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSungil Gallery\n\n\n\nPhoto\n\n\n\nSome photo\n\n\n\nsungil_park\n\n\nJul 26, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "records/gallery.html",
    "href": "records/gallery.html",
    "title": "Sungil Gallery",
    "section": "",
    "text": "대한민국 해병대\n달리는 제 1 상륙사단\n최강킹콩 3여단\n제 1576 공정대대\n흑곰 화기중대\n스파르타 직사화기소대\n상승불패 K-4반\n해병공수 243차\n兵1255期 박성일"
  },
  {
    "objectID": "records/gallery.html#전역",
    "href": "records/gallery.html#전역",
    "title": "Sungil Gallery",
    "section": "전역",
    "text": "전역\n\n\nVideo"
  },
  {
    "objectID": "records/running.html",
    "href": "records/running.html",
    "title": "Running",
    "section": "",
    "text": "Updates at weekend\n\n\nRecord with Galaxy Watch 4\n\n\nGears\n\nNIKE Air Zoom Tempo NEXT% Flyknit\nNIKE Epic React Flyknit 1\n\n\n\n\nDate\nDistance(km)\nTime\nPace(/1km)\n\n\n\n\n2023/04/28\n5.22\n27:33\n5’16\n\n\n2023/04/23\n3.56\n18:30\n5’11\n\n\n2023/04/20\n10.06\n52:39\n5’13\n\n\n2023/04/16\n5.88\n35:36\n6’03\n\n\n2023/03/25\n4.2\n30:16\nTreadmil\n\n\n2023/03/24\n3.54\n23:25\n6’36\n\n\n2023/03/19\n3.01\n15:36\n5’10\n\n\n2023/03/17\n3.03\n17:55\n5’54\n\n\n2023/03/15\n3.38\n23:21\n6’54\n\n\n2023/03/05\n4.14\n30:30\n7’20"
  },
  {
    "objectID": "records/Swimming.html",
    "href": "records/Swimming.html",
    "title": "Stress is water soluble",
    "section": "",
    "text": "Updates at weekend\n\n\nRecord with Galaxy Watch 4\n\n\nGears\n\nSPEEDO Allover V-cut Jammer\nNIKE have a Nike day Swimming cap\nNIKE Vapor Mirrored Performance Goggle\n\n\n\n\nDate\nDistance(m)\nTime\nPace(/100m)\nReview\n\n\n\n\n2023/04/30\n1500\n30~\n2’~\n-\n\n\n2023/04/29\n650\n13:06\n1’59\nRecovery day\n\n\n2023/04/26\n2900\n60:40\n1’13\nWetsuit day 4\n\n\n2023/04/24\n2000\n39:43\n1’58\n\n\n\n2023/04/21\n1000\n20:37\n1’58\nRecovery day\n\n\n2023/04/19\n3000\n60:59\n1’09\nWetsuit day 3\n\n\n2023/04/18\n450\n09:02\n1’40\nRecovery day\n\n\n2023/04/17\n1700\n33:59\n1’59\n1500m 29:56\n\n\n2023/04/14\n1300\n25:59\n1’57\n\n\n\n2023/04/12\n3050\n60:22\n1’18\nWetsuit day 2\n\n\n2023/04/11\n1200\n23:54\n1’59\n\n\n\n2023/04/10\n1250\n36:05\n1’58\n\n\n\n2023/04/07\n1000\n22:56\n2’57\n25m\n\n\n2023/04/05\n3150\n67:44\n1’22\nWetsuit day 1\n\n\n2023/04/03\n2000\n41:55\n2’05\n\n\n\n2023/04/01\n1000\n19:10\n1’53\n\n\n\n2023/03/31\n1000\n19:08\n1’54\n\n\n\n2023/03/30\n700\n15:52\n2’07\nRecovery day\n\n\n2023/03/29\n2800\n51:31\n1’07\nOpenwater week6\n\n\n2023/03/23\n600\n28:59\n1’52\nRecovery day\n\n\n2023/03/22\n3050\n56:50\n1’38\nOpenwater Week5\n\n\n2023/03/21\n900\n18:53\n1’59\nRecovery day\n\n\n2023/03/20\n2100\n42:57\n2’02\n1800m\n\n\n2023/03/18\n1600\n29:31\n1’50\n50m Dash PR 42sec\n\n\n2023/03/16\n1800\n35:42\n1’58\ngood\n\n\n2023/03/14\n700\n13:52\n1’58\nDNF 40m Success\n\n\n2023/03/13\n1300\n26:25\n2’01\n\n\n\n2023/03/12\n1050\n27:28\n1’44\n\n\n\n2023/03/10\n1500\n30:00\n1’59\n1500m/30min Success\n\n\n2023/03/07\n2000\n41:52\n2’05\n\n\n\n2023/03/06\n1350\n27:28\n2’00\n\n\n\n2023/03/04\n450\n9:42\n2’06\nRecovery day\n\n\n2023/03/03\n1950\n50:13\n2’16\n\n\n\n2023/03/01\n3100\n65:00\n2’05\n3.1 to 3.1km"
  }
]